{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Monstache Sync MongoDB to Elasticsearch in realtime Monstache is a sync daemon written in Go that continously indexes your MongoDB collections into Elasticsearch. Monstache gives you the ability to use Elasticsearch to do complex searches and aggregations of your MongoDB data and easily build realtime Kibana visualizations and dashboards. Features Supports up to and including the latest versions of Elasticsearch and MongoDB Single binary with a light footprint Support for MongoDB change streams and aggregation pipelines Pre built Docker containers Optionally filter the set of collections to sync Advanced support for sharded MongoDB clusters including auto-detection of new shards Direct read mode to do a full sync of collections in addition to tailing the oplog Transform and filter documents before indexing using Golang plugins or JavaScript Index the content of GridFS files Support for propogating hard/soft deletes Support for propogating database and collection drops Optional custom document routing in Elasticsearch Stateful resume feature Time machine feature to track document changes over time Worker and Clustering modes for High Availability Support for rfc7396 JSON merge patches Systemd support Optional http server to get access to liveness, stats, profiling, etc Next Steps See Getting Started for instructions how to get it up and running. See Release Notes for updates.","title":"Home"},{"location":"#monstache","text":"Sync MongoDB to Elasticsearch in realtime Monstache is a sync daemon written in Go that continously indexes your MongoDB collections into Elasticsearch. Monstache gives you the ability to use Elasticsearch to do complex searches and aggregations of your MongoDB data and easily build realtime Kibana visualizations and dashboards.","title":"Monstache"},{"location":"#features","text":"Supports up to and including the latest versions of Elasticsearch and MongoDB Single binary with a light footprint Support for MongoDB change streams and aggregation pipelines Pre built Docker containers Optionally filter the set of collections to sync Advanced support for sharded MongoDB clusters including auto-detection of new shards Direct read mode to do a full sync of collections in addition to tailing the oplog Transform and filter documents before indexing using Golang plugins or JavaScript Index the content of GridFS files Support for propogating hard/soft deletes Support for propogating database and collection drops Optional custom document routing in Elasticsearch Stateful resume feature Time machine feature to track document changes over time Worker and Clustering modes for High Availability Support for rfc7396 JSON merge patches Systemd support Optional http server to get access to liveness, stats, profiling, etc","title":"Features"},{"location":"#next-steps","text":"See Getting Started for instructions how to get it up and running. See Release Notes for updates.","title":"Next Steps"},{"location":"about/","text":"About License The MIT License (MIT) Copyright (c) 2016-2018 Ryan Wynn Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. Contributing The Monstache project welcomes, and depends, on contributions from developers and users in the open source community. Contributions can be made in a number of ways, a few examples are: Code patches via pull requests Documentation improvements Bug reports and patch reviews Reporting an Issue Please include as much detail as you can. Let us know your platform, Monstache version, MongoDB version, and Elasticsearch version. Testing the Development Version If you want to just install and try out the latest development version of Monstache you can do so with the following command. This can be useful if you want to provide feedback for a new feature or want to confirm if a bug you have encountered is fixed in the git master. go get -u github.com/rwynn/monstache Running the tests To run the tests without Docker, you will need to have local mongod and elasticsearch servers running. Then you will need to start a monstache process in one terminal or in the background. monstache -verbose Finally in another terminal you can run the tests by issuing the following commands cd $GOPATH/src/github.com/rwynn/monstache go test -v Warning Running the Monstache tests will perform modifications to the test.test namespace in MongoDB and will index documents in the test.test index in Elasticsearch. If you have data that you need to keep on your local servers, make a back up before running the tests. Note If you don't want to setup MongoDB and Elasticsearch on your machine another option for running the tests is via Docker. After cloning the monstache repo you can cd into the docker/test directory and run run-tests.sh . You will need docker and docker-compose to run the tests this way. Services for MongoDB and Elasticsearch will be started and the tests run on any changes you have made to the source code. Submitting Pull Requests Once you are happy with your changes or you are ready for some feedback, push it to your fork and send a pull request. For a change to be accepted it will most likely need to have tests and documentation if it is a new feature. Release Notes monstache v4.11.4 Use less CPU resources Fix for race conditions monstache v3.18.4 Use less CPU resources Fix for race conditions monstache v4.11.3 Fix an issue where a paused monstache process would not resume correctly in cluster mode monstache v3.18.3 Fix an issue where a paused monstache process would not resume correctly in cluster mode monstache v4.11.2 Fix an issue with workers where only one worker would be used for change documents Redact sensitive connection information when logging connection failures monstache v3.18.2 Fix an issue with workers where only one worker would be used for change documents Redact sensitive connection information when logging connection failures monstache v4.11.1 Fix for connection checker thread exiting early in cluster mode Better handling of JSON serialization errors monstache v3.18.1 Fix for connection checker thread exiting early in cluster mode Better handling of JSON serialization errors monstache v4.11.0 Reliability improvements Fix allowing one to use a MongoDB view as a direct-read-namespace Addition of the relate config to declare dependencies between collections Experimental support for AWS Signing Version 4 monstache v3.18.0 Reliability improvements Fix allowing one to use a MongoDB view as a direct-read-namespace Addition of the relate config to declare dependencies between collections Experimental support for AWS Signing Version 4 monstache v4.10.2 Fixes related to i/o timeout errors Default timeout configurations changed to no timeout (0) with the exception of the initial connection to MongoDB which times out after 15 seconds. Values of 0 disable timeouts. All other positive values are in seconds. New defaults shown below. You probably do not need to specify any of these values unless you encounter problems. [mongo-dial-settings] timeout=15 read-timeout=0 write-timeout=0 [mongo-session-settings] socket-timeout=0 sync-timeout=0 monstache v3.17.2 Fixes related to i/o timeout errors Default timeout configurations changed to no timeout (0) with the exception of the initial connection to MongoDB which times out after 15 seconds. Values of 0 disable timeouts. All other positive values are in seconds. New defaults shown below. You probably do not need to specify any of these values unless you encounter problems. [mongo-dial-settings] timeout=15 read-timeout=0 write-timeout=0 [mongo-session-settings] socket-timeout=0 sync-timeout=0 monstache v4.10.1 Clean up timeout configurations and increase default timeout values New timeout configurations surfaced - read and write timeout. Configure as follows (default values shown): [mongo-dial-settings] timeout=10 read-timeout=600 write-timeout=30 [mongo-session-settings] socket-timeout=600 sync-timeout=600 monstache v3.17.1 Clean up timeout configurations and increase default timeout values New timeout configurations surfaced - read and write timeout. Configure as follows (default values shown): [mongo-dial-settings] timeout=10 read-timeout=600 write-timeout=30 [mongo-session-settings] socket-timeout=600 sync-timeout=600 monstache v4.10.0 Fix for issue #97 where monstache would exit before syncing all documents with -exit-after-direct-reads enabled Support added for MongoDB change streams via the change-stream-namespaces option New golang plugin functions Process and Pipeline added to the existing Map and Filter functions. The Process function allows one to code complex processing after an event. The Process function has access to the MongoDB session, the Elasticsearch client, the Elasticsearch bulk processor, and information about the change that occurred (insert, update, delete). The Pipeline function allows one to assign MongoDB pipeline stages to both direct reads and change streams. Since the pipeline stages may differ between direct reads and change streams the function is passed a boolean indicating the source of the data. For example, a $match clause on the change stream may need to reference the fullDocument field since the root will be the change event. For direct reads the root will simply be the full document. New config option pipeline allows one to create aggregation pipelines in javascript for direct reads and change streams. This can be used instead of the Pipeline function in a golang plugin. The exported function in javascript takes a namespace and a boolean indicating whether or not the source was a change stream. The function should return an array of pipeline stages to apply. New config option pipe-allow-disk which when enabled allows large pipelines to use the disk to save intermediate results. New global function available in javascript script functions named pipe . The pipe function is simliar to existing find function but takes an array of aggregation pipeline stages as the first argument. direct-read-namespaces = [test.test] change-stream-namespaces = [test.test] [[pipeline]] script = module.exports = function(ns, changeStream) { if (changeStream) { return [ { $match: { fullDocument.foo : 1} } ]; } else { return [ { $match: { foo : 1} } ]; } } [[script]] namespace = test.test script = module.exports = function(doc, ns) { doc.extra = pipe([ { $match: {foo: 1} }, { $limit: 1 }, { $project: { _id: 0, foo: 1}} ]); return doc; } monstache v3.17.0 Fix for issue #97 where monstache would exit before syncing all documents with -exit-after-direct-reads enabled Support added for MongoDB change streams via the change-stream-namespaces option New golang plugin functions Process and Pipeline . The Process function allows one to code complex processing after an event. The Process function has access to the MongoDB session, the Elasticsearch client, the Elasticsearch bulk processor, and information about the change that occurred (insert, update, delete). The Pipeline function allows one to assign MongoDB pipeline stages to both direct reads and change streams. Since the pipeline stages may differ between direct reads and change streams the function is passed a boolean indicating the source of the data. For example, a $match clause on the change stream may need to reference the fullDocument field since the root will be the change event. For direct reads the root will simply be the full document. New config option pipeline allows one to create aggregation pipelines in javascript for direct reads and change streams. This can be used instead of the Pipeline function in a golang plugin. The exported function in javascript takes a namespace and a boolean indicating whether or not the source was a change stream. The function should return an array of pipeline stages to apply. New config option pipe-allow-disk which when enabled allows large pipelines to use the disk to save intermediate results. New global function available in javascript script functions named pipe . The pipe function is simliar to existing find function but takes an array of aggregation pipeline stages as the first argument. monstache v4.9.0 Fix to omit version information on deletes when the index-as-update setting is ON (to match the omitted version information at indexing time) Fix issue #89 by making the indexed oplog field names and date format configurable monstache v3.16.0 Fix to omit version information on deletes when the index-as-update setting is ON (to match the omitted version information at indexing time) Fix issue #89 by making the indexed oplog field names and date format configurable monstache v4.8.0 Seperate namespace regexes for drop operations vs create/update/delete operations Better handling of panics A new index-as-update boolean config option that allow merge instead of replace Fixes to the find and findOne functions available in scripts monstache v3.15.0 Seperate namespace regexes for drop operations vs create/update/delete operations Better handling of panics A new index-as-update boolean config option that allow merge instead of replace Fixes to the find and findOne functions available in scripts monstache v4.7.0 add -pprof setting. When enabled and combined with -enable-http-server you can read profiling information. See Profiling for Go for more information. add -enable-easy-json setting. When enabled easy-json will be used for serialization to Elasticsearch. tweaks to the tailing code in gtm monstache v3.14.0 add -pprof setting. When enabled and combined with -enable-http-server you can read profiling information. See Profiling for Go for more information. add -enable-easy-json setting. When enabled easy-json will be used for serialization to Elasticsearch. tweaks to the tailing code in gtm monstache v4.6.5 decrease the fetch channel flush timeout clarify version conflicts and invalid json messages as warnings remove the /config endpoint for better security (use -print-config instead) fix LoadPlugins method (contributed by @YouthLab) performance tweaks in gtm monstache v3.13.5 decrease the fetch channel flush timeout clarify version conflicts and invalid json messages as warnings remove the /config endpoint for better security (use -print-config instead) fix LoadPlugins method (contributed by @YouthLab) performance tweaks in gtm monstache v4.6.4 Expose new setting direct-read-split-max which limits the number of times a collection is split for reading during direct-reads and thus the number of go routines and MongoDB connections spawned. The default is 9. Tune this setting to increase/decrease the amount of memory the monstache process will consume. Better Docker support due to the contributions of @a-magdy. monstache v3.13.4 Expose new setting direct-read-split-max which limits the number of times a collection is split for reading during direct-reads and thus the number of go routines and MongoDB connections spawned. The default is 9. Tune this setting to increase/decrease the amount of memory the monstache process will consume. Better Docker support due to the contributions of @a-magdy. monstache v4.6.3 Fix for issue #65, year outside of [0,9999]. Invalid time will be removed now with prune-invalid-json turned on Fix for issue #62, the number of connections to MongoDB is now limited to a max of 32 per namespace Fix for issue #59, unsupported values of +/- Infinity and NaN. These values can now be removed with the prune-invalid-json setting Fix for issue $46 and #66, having to do with filtering. Filters now use locks to ensure the javascript environment is used by one at a time. monstache v3.13.3 Fix for issue #65, year outside of [0,9999]. Invalid time will be removed now with prune-invalid-json turned on Fix for issue #62, the number of connections to MongoDB is now limited to a max of 32 per namespace Fix for issue #59, unsupported values of +/- Infinity and NaN. These values can now be removed with the prune-invalid-json setting Fix for issue $46 and #66, having to do with filtering. Filters now use locks to ensure the javascript environment is used by one at a time. monstache v4.6.2 Fix regression in 3.13 series where collections under 50K documents were not synching Performing Tuning. The following defaults have changed so please update your config files accordingly. elasticsearch-max-conns went from 10 - 4 elasticsearch-max-docs went from 1000 - do not flush based on count (I suggest not overriding this since document sizes can vary greatly - instead use max-bytes) elasticsearch-max-bytes went from 5MB - 8MB Note when you specify elasticseach-max-bytes the value must be in bytes not MB monstache v3.13.2 Fix regression in 3.13 series where collections under 50K documents were not synching Performing Tuning. The following defaults have changed so please update your config files accordingly. elasticsearch-max-conns went from 10 - 4 elasticsearch-max-docs went from 1000 - do not flush based on count (I suggest not overriding this since document sizes can vary greatly - instead use max-bytes) elasticsearch-max-bytes went from 5MB - 8MB Note when you specify elasticseach-max-bytes the value must be in bytes not MB monstache v4.6.1 Performance and bug fixes in the gtm library monstache v3.13.1 Performance and bug fixes in the gtm library monstache v4.6.0 Performance improvements. Much of the performance gains come from an upgrade of the gtm library. This library now uses split vector failing back to a paginated range queries. Also, some buffering has been removed at the gtm level for certain operations. monstache v3.13.0 Performance improvements. Much of the performance gains come from an upgrade of the gtm library. This library now uses split vector failing back to a paginated range queries. Also, some buffering has been removed at the gtm level for certain operations. monstache v4.5.0 Adds an option delete-index-pattern to specify an Elasticsearch index pattern to scope stateless deletes. Indexes outside of this pattern will not be considered when propogating deletes from MongoDB to Elasticsearch. By default all Elasticsearch indexes are queried. Adds the ability to specify a global filter function in Javascript. Previously, a filter function needed to be tied to a MongoDB namespace. Now you can leave off the namespace and the filter function will be applied to all namespaces. The filter function will receive the document as the first argument and the MongoDB namespace as the second argument. Breaking change: direct-read-cursors and direct-read-batch-size have been removed as options. The underlying gtm library of monstache has been upgraded and no longer supports parallelCollectionScan which is being removed in future versions of MongoDB. Now gtm will use splitVector to divy up collections to read documents concurrently. Also, the batch size will be managed by MongoDB and not set explicitly. See the gtm library docs for more information. Adds a boolean configuration option, prune-invalid-json , which defaults to false. Set this to true if your MongoDB data has values such as +Inf, -Inf, or NaN which are not supported by the golang JSON parser and cause infinite error loops to occur. With prune-invalid-json set to true Monstache will remove these values before indexing into Elasticsearch to avoid these errors. monstache v3.12.0 Adds an option delete-index-pattern to specify an Elasticsearch index pattern to scope stateless deletes. Indexes outside of this pattern will not be considered when propogating deletes from MongoDB to Elasticsearch. By default all Elasticsearch indexes are queried. Adds the ability to specify a global filter function in Javascript. Previously, a filter function needed to be tied to a MongoDB namespace. Now you can leave off the namespace and the filter function will be applied to all namespaces. The filter function will receive the document as the first argument and the MongoDB namespace as the second argument. Breaking change: direct-read-cursors and direct-read-batch-size have been removed as options. The underlying gtm library of monstache has been upgraded and no longer supports parallelCollectionScan which is being removed in future versions of MongoDB. Now gtm will use splitVector to divy up collections to read documents concurrently. Also, the batch size will be managed by MongoDB and not set explicitly. See the gtm library docs for more information. Adds a boolean configuration option, prune-invalid-json , which defaults to false. Set this to true if your MongoDB data has values such as +Inf, -Inf, or NaN which are not supported by the golang JSON parser and cause infinite error loops to occur. With prune-invalid-json set to true Monstache will remove these values before indexing into Elasticsearch to avoid these errors. monstache v4.4.0 Updated the default delete strategy Breaking change: check delete-strategy monstache v3.11.0 Updated the default delete strategy Breaking change: check delete-strategy monstache v4.3.2 Allow specifying a script without a namespace. In this case documents from all collections will be run through the script. The document object will continue to be the 1st argument to the function and a new 2nd argument will be the namespace of the source document. Fixes #55. Fix an issue where a Date object created in Javascript would not be formatted correctly for indexing. Build with go 1.10.1 monstache v3.10.2 Allow specifying a script without a namespace. In this case documents from all collections will be run through the script. The document object will continue to be the 1st argument to the function and a new 2nd argument will be the namespace of the source document. Fixes #55. Fix an issue where a Date object created in Javascript would not be formatted correctly for indexing. Build with go 1.10.1 monstache v4.3.1 Upgrade gtm to pick up fix for parallel collection scans on direct reads. Each cursor now gets its own connection. monstache v3.10.1 Upgrade gtm to pick up fix for parallel collection scans on direct reads. Each cursor now gets its own connection. monstache v4.3.0 Upgrade gtm to pick up support for parallel collection scan on direct reads if your mongodb storage engine supports it Add config option to specify the number of cursors to request for parallel collection scans Allow mappings to specify overrides for 1 of index and type instead of requiring both Fix an issue where filters were not being applied to document updates monstache v3.10.0 Upgrade gtm to pick up support for parallel collection scan on direct reads if your mongodb storage engine supports it Add config option to specify the number of cursors to request for parallel collection scans Allow mappings to specify overrides for 1 of index and type instead of requiring both Fix an issue where filters were not being applied to document updates monstache v4.2.1 Ensure index names are lowercase monstache v3.9.1 Ensure index names are lowercase monstache v4.2.0 Add filtering to Javascript and Golang plugins. Filtered documents are completely ignored while dropped documents result in a delete request. monstache v3.9.0 Add filtering to Javascript and Golang plugins. Filtered documents are completely ignored while dropped documents result in a delete request. monstache v4.1.2 Fix custom routing for golang plugins Configuration now supports paths to Javascript files in addition to inline scripts monstache v3.8.2 Fix custom routing for golang plugins Configuration now supports paths to Javascript files in addition to inline scripts monstache v4.1.1 Route time machine docs by MongoDB source id monstache v3.8.1 Route time machine docs by MongoDB source id monstache v4.1.0 Add a nifty time machine feature monstache v3.8.0 Add a nifty time machine feature monstache v4.0.1 Fixed a bug where monstache would think direct reads were done when they had not even started Performance improvements for direct reads on large collections monstache v3.7.0 Fixed a bug where monstache would think direct reads were done when they had not even started Performance improvements for direct reads on large collections monstache v4.0.0 Monstache v4+ should be used for ES6+. There will still be bug fixes and maintenance done to the Monstache v3 releases to support ES2-5. You can still download v3.x releases from the downloads page or by directing go get to gopkg.in/rwynn/monstache.v3 Fixes deprecation warnings during bulk indexing against ES6 because of renamed fields version and version_type Monstache will now default to using the ES type _doc (as opposed to the MongoDB collection name) when it detects ES 6.2+. This is the new recommended type name going forward. See issue #42. monstache v3.6.5 Remove brittle normalization of index names, type names, and ids Start differentiating between releases supporting ES6+ and pre-ES6 by releasing from rel3 branch Soon a 4.0.0 release will be cut from master that will be ES6 forward. pre-ES6 will still be supported by downloading 3.x releases from the releases page or directing go get to gopkg.in/rwynn/monstache.v3 Technically this release will still work with ES+ but that won't last forever. There are some deprecation warnings. In summary, if you need pre-ES6 use v3.x releases of monstache and v4.x releases of monstache for ES6+ going forward. monstache v3.6.4 Trying to set the record for github releases in one night Fix a regression whereby monstache would exit after direct reads were complete when it should have kept tailing the oplog monstache v3.6.3 Fix for a benign race condition in shutdown, introduced in 3.6.2, that caused a panic monstache v3.6.2 Resume usage of upstream elastic client library now that fix for Elasticsearch going down has been merged When Elasticsearch goes down the elastic client will now put back pressure on Add and Flush calls. When Elasticsearch comes back up it will resume Adding and Flushing were it left off. Do to the blocking nature of Add and Flush the shutdown function of monstache has been refactored to take this into account. Shutdown will not hang if Elasticsearch is down. It will try to Flush pending documents but if this blocks due to a down server it will still exit after a 5 second deadline. monstache v3.6.1 Added more detailed error logging. Each bulk request line that failed will be logged separately with details. This is much more lightweight than having to turn on verbose to get error details. Verbose is not a recommended setting for production. monstache v3.6.0 This release focuses on improvements with regards to handling dropped connections to either Elasticsearch or MongoDB and resuming gracefully when they come back online monstache v3.5.2 The previous release safeguards the integrity of inserts and updates with a version number, but neglected deletes. This release adds versions to deletes such that an [insert, delete] sequence that gets sent to Elasticsearch in 2 different requests (due to elasticsearch-max-conns 1) cannot actually perform a [delete, insert] instead. In this case the insert would now carry a version number the delete version number and be rejected. monstache v3.5.1 Fix for issue #37 - out of order indexing due to concurrent bulk indexing requests. With elasticsearch-max-conns set to greater than 1 you may get out of order index requests; however after this fix each document is versioned such that Elasticsearch will not replace a newer version with an older one. The version of the document is the timestamp from the MongoDB oplog of when the change (insert, update) occurred. Out of order indexing typically happens when both an insert and an update are queued for a bulk request at around the same time. In this case, do to the way the bulk processor multiplexes requests onto multiple connections, the document may be received out of order. monstache v3.5.0 Support for sharded MongoDB cluster. See docs for details Performance optimizations Turn off bulk retries if configured to do so monstache v3.4.2 Allow the stats index name format to be configurable. Continues to default to index per day. monstache v3.4.1 Fix for the javascript mapping functions. An Otto Export does not appear to recurse into arrays. Need to do a recursive Export for this scenario. monstache v3.4.0 Add ability to embed documents during the mapping phase. Javascript plugins get 3 new global functions: findId, findOne, and find. Golang plugins get access to the mgo.Session. See the docs for details. monstache v3.3.1 Improve support for additional indexing metadata. Fix issue where indexing metadata was not honored monstache v3.3.0 Added optional http server. Enable with --enable-http-server flag. Listens on :8080 by default. Configure address with --http-server-addr :8000. The server responds to the following endpoints (/started, /healthz, /config, and /stats). The stats endpoint is only enabled if stats are enabled. The /started and /healthz endpoints can be used to check for liveness. Upgraded the gtm library with performance improvements monstache v3.2.0 Add systemd support monstache v3.1.2 Built with go1.9 Fix golint warnings monstache v3.1.1 timestamp stats indexes by day for easier cleanup using e.g. curator monstache v3.1.0 add print-config argument to display the configuration and exit add index-stats option to write indexing statistics into Elasticsearch for analysis monstache v3.0.7 fix elasticsearch client http scheme for secure connections monstache v3.0.6 fix invalid struct field tag monstache v3.0.5 add direct-read-batch-size option upgrade gtm to accept batch size and to ensure all direct read errors are logged monstache v3.0.4 fix slowdown on direct reads for large mongodb collections monstache v3.0.3 small changes to the settings for the exponential back off on retry. see the docs for details. only record timestamps originating from the oplog and not from direct reads apply the worker routing filter to direct reads in worker mode monstache v3.0.2 add option to configure elasticsearch client http timout. up the default timeout to 60 seconds monstache v3.0.1 upgrade gtm to fix an issue where a mongodb query error (such as CappedPositionLost) causes the tail go routine to exit (after which no more events will be processed) monstache v3.0.0 new major release configuration changes with regards to Elasticsearch. see docs for details adds ability to write rolling logs to files adds ability to log indexing statistics changed go Elasticsearch client from elastigo to elastic which provides more API coverage upgrade gtm monstache v2.14.0 add support for golang plugins. you can now do in golang what you previously could do in javascript add more detail to bulk indexing errors upgrade gtm monstache v2.13.0 add direct-read-ns option. allows one to sync documents directly from a set of collections in addition to going through the oplog add exit-after-direct-reads option. tells monstache to exit after performing direct reads. useful for running monstache as a cron job. fix issue around custom routing where db name was being stored as an array upgrade gtm monstache v2.12.0 Fix order of operations surrounding db or collection drops in the oplog. Required the removal of some gtm-options introduced in 2.11. Built with latest version of gtm which includes some performance gains Add ssl option under mongo-dial-settings. Previously, in order to enable connections with TLS one had to provide a PEM file. Now, one can enable TLS without a PEM file by setting this new option to true. This was tested with MongoDB Atlas which requires SSL but does not provide a PEM file monstache v2.11.2 Built with Go 1.8 Added option fail-fast Added option index-oplog-time monstache v2.11.1 Built with Go 1.8 Performance improvements Support for rfc7386 JSON merge patches Support for overriding Elasticsearch index and type in JavaScript More configuration options surfaced monstache v2.10.0 add shard routing capability add Makefile monstache v2.9.3 extend ttl for active in cluster to reduce process switching monstache v2.9.2 fix potential collision on floating point _id closes #16 monstache v2.9.1 fix an edge case #18 where a process resuming for the cluster would remain paused monstache v2.9 fix an issue with formatting of integer ids enable option for new clustering feature for high availability add TLS skip verify options for mongodb and elasticsearch add an option to specify a specific timestamp to start syncing from monstache v2.8.1 fix an index out of bounds panic during error reporting surface gtm options for setting the oplog database and collection name as well as the cursor timeout report an error if unable to unzip a response when verbose is true monstache v2.8 add a version flag -v document the elasticsearch-pem-file option add the elasticsearch-hosts option to configure pool of available nodes within a cluster monstache v2.7 add a gzip configuration option to increase performance default resume-name to the worker name if defined decrease binary size by building with -ldflags \"-w\" monstache v2.6 reuse allocations made for gridfs files add workers feature to distribute synching between multiple processes monstache v2.5 add option to speed up writes when saving resume state remove extra buffering when adding file content monstache v2.4 Fixed issue #10 Fixed issue #11 monstache v2.3 Added configuration option for max file size Added code to normalize index and type names based on restrictions in Elasticsearch Performance improvements for GridFs files monstache v2.2 Added configuration option for dropped databases and dropped collections. See the README for more information. monstache v2.1 Added support for dropped databases and collections. Now when you drop a database or collection from mongodb the corresponding indexes are deleted in elasticsearch. monstache v2.0 Fixes an issue with the default mapping between mongodb and elasticsearch. Previously, each database in mongodb was mapped to an index of the same name in elasticsearch. This creates a problem because mongodb document ids are only guaranteed unique at the collection level. If there are 2 or more documents in a mongodb database with the same id those documents were previously written to the same elasticsearch index. This fix changes the default mapping such that the entire mongodb document namespace (database + collection) is mapped to the destination index in elasticsearch. This prevents the possibility of collisions within an index. Since this change requires reindexing of previously indexed data using monstache, the version number of monstache was bumped to 2. This change also means that by default you will have an index in elasticsearch for each mongodb collection instead of each mongod database. So more indexes by default. You still have control to override the default mapping. See the docs for how to explicitly control the index and type used for a particular mongodb namespace. Bumps the go version to 1.7.3 monstache v1.3.1 Version 1.3 rebuilt with go1.7.1 monstache v1.3 Improve log messages Add support for the ingest-attachment plugin in elasticsearch 5 monstache v1.2 Improve Error Reporting and Add Config Options monstache v1.1 Fixes crash during replay (issue #2) Adds supports for indexing GridFS content (issue #3) monstache v1.0 64-bit Linux binary built with go1.6.2 monstache v0.8-beta.2 64-bit Linux binary built with go1.6.2 monstache v0.8-beta.1 64-bit Linux binary built with go1.6.2 monstache v0.8-beta 64-bit Linux binary built with go1.6.2 monstache v0.8-alpha 64-bit Linux binary built with go1.6.2","title":"About"},{"location":"about/#about","text":"","title":"About"},{"location":"about/#license","text":"The MIT License (MIT) Copyright (c) 2016-2018 Ryan Wynn Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"about/#contributing","text":"The Monstache project welcomes, and depends, on contributions from developers and users in the open source community. Contributions can be made in a number of ways, a few examples are: Code patches via pull requests Documentation improvements Bug reports and patch reviews","title":"Contributing"},{"location":"about/#reporting-an-issue","text":"Please include as much detail as you can. Let us know your platform, Monstache version, MongoDB version, and Elasticsearch version.","title":"Reporting an Issue"},{"location":"about/#testing-the-development-version","text":"If you want to just install and try out the latest development version of Monstache you can do so with the following command. This can be useful if you want to provide feedback for a new feature or want to confirm if a bug you have encountered is fixed in the git master. go get -u github.com/rwynn/monstache","title":"Testing the Development Version"},{"location":"about/#running-the-tests","text":"To run the tests without Docker, you will need to have local mongod and elasticsearch servers running. Then you will need to start a monstache process in one terminal or in the background. monstache -verbose Finally in another terminal you can run the tests by issuing the following commands cd $GOPATH/src/github.com/rwynn/monstache go test -v Warning Running the Monstache tests will perform modifications to the test.test namespace in MongoDB and will index documents in the test.test index in Elasticsearch. If you have data that you need to keep on your local servers, make a back up before running the tests. Note If you don't want to setup MongoDB and Elasticsearch on your machine another option for running the tests is via Docker. After cloning the monstache repo you can cd into the docker/test directory and run run-tests.sh . You will need docker and docker-compose to run the tests this way. Services for MongoDB and Elasticsearch will be started and the tests run on any changes you have made to the source code.","title":"Running the tests"},{"location":"about/#submitting-pull-requests","text":"Once you are happy with your changes or you are ready for some feedback, push it to your fork and send a pull request. For a change to be accepted it will most likely need to have tests and documentation if it is a new feature.","title":"Submitting Pull Requests"},{"location":"about/#release-notes","text":"","title":"Release Notes"},{"location":"about/#monstache-v4114","text":"Use less CPU resources Fix for race conditions","title":"monstache v4.11.4"},{"location":"about/#monstache-v3184","text":"Use less CPU resources Fix for race conditions","title":"monstache v3.18.4"},{"location":"about/#monstache-v4113","text":"Fix an issue where a paused monstache process would not resume correctly in cluster mode","title":"monstache v4.11.3"},{"location":"about/#monstache-v3183","text":"Fix an issue where a paused monstache process would not resume correctly in cluster mode","title":"monstache v3.18.3"},{"location":"about/#monstache-v4112","text":"Fix an issue with workers where only one worker would be used for change documents Redact sensitive connection information when logging connection failures","title":"monstache v4.11.2"},{"location":"about/#monstache-v3182","text":"Fix an issue with workers where only one worker would be used for change documents Redact sensitive connection information when logging connection failures","title":"monstache v3.18.2"},{"location":"about/#monstache-v4111","text":"Fix for connection checker thread exiting early in cluster mode Better handling of JSON serialization errors","title":"monstache v4.11.1"},{"location":"about/#monstache-v3181","text":"Fix for connection checker thread exiting early in cluster mode Better handling of JSON serialization errors","title":"monstache v3.18.1"},{"location":"about/#monstache-v4110","text":"Reliability improvements Fix allowing one to use a MongoDB view as a direct-read-namespace Addition of the relate config to declare dependencies between collections Experimental support for AWS Signing Version 4","title":"monstache v4.11.0"},{"location":"about/#monstache-v3180","text":"Reliability improvements Fix allowing one to use a MongoDB view as a direct-read-namespace Addition of the relate config to declare dependencies between collections Experimental support for AWS Signing Version 4","title":"monstache v3.18.0"},{"location":"about/#monstache-v4102","text":"Fixes related to i/o timeout errors Default timeout configurations changed to no timeout (0) with the exception of the initial connection to MongoDB which times out after 15 seconds. Values of 0 disable timeouts. All other positive values are in seconds. New defaults shown below. You probably do not need to specify any of these values unless you encounter problems. [mongo-dial-settings] timeout=15 read-timeout=0 write-timeout=0 [mongo-session-settings] socket-timeout=0 sync-timeout=0","title":"monstache v4.10.2"},{"location":"about/#monstache-v3172","text":"Fixes related to i/o timeout errors Default timeout configurations changed to no timeout (0) with the exception of the initial connection to MongoDB which times out after 15 seconds. Values of 0 disable timeouts. All other positive values are in seconds. New defaults shown below. You probably do not need to specify any of these values unless you encounter problems. [mongo-dial-settings] timeout=15 read-timeout=0 write-timeout=0 [mongo-session-settings] socket-timeout=0 sync-timeout=0","title":"monstache v3.17.2"},{"location":"about/#monstache-v4101","text":"Clean up timeout configurations and increase default timeout values New timeout configurations surfaced - read and write timeout. Configure as follows (default values shown): [mongo-dial-settings] timeout=10 read-timeout=600 write-timeout=30 [mongo-session-settings] socket-timeout=600 sync-timeout=600","title":"monstache v4.10.1"},{"location":"about/#monstache-v3171","text":"Clean up timeout configurations and increase default timeout values New timeout configurations surfaced - read and write timeout. Configure as follows (default values shown): [mongo-dial-settings] timeout=10 read-timeout=600 write-timeout=30 [mongo-session-settings] socket-timeout=600 sync-timeout=600","title":"monstache v3.17.1"},{"location":"about/#monstache-v4100","text":"Fix for issue #97 where monstache would exit before syncing all documents with -exit-after-direct-reads enabled Support added for MongoDB change streams via the change-stream-namespaces option New golang plugin functions Process and Pipeline added to the existing Map and Filter functions. The Process function allows one to code complex processing after an event. The Process function has access to the MongoDB session, the Elasticsearch client, the Elasticsearch bulk processor, and information about the change that occurred (insert, update, delete). The Pipeline function allows one to assign MongoDB pipeline stages to both direct reads and change streams. Since the pipeline stages may differ between direct reads and change streams the function is passed a boolean indicating the source of the data. For example, a $match clause on the change stream may need to reference the fullDocument field since the root will be the change event. For direct reads the root will simply be the full document. New config option pipeline allows one to create aggregation pipelines in javascript for direct reads and change streams. This can be used instead of the Pipeline function in a golang plugin. The exported function in javascript takes a namespace and a boolean indicating whether or not the source was a change stream. The function should return an array of pipeline stages to apply. New config option pipe-allow-disk which when enabled allows large pipelines to use the disk to save intermediate results. New global function available in javascript script functions named pipe . The pipe function is simliar to existing find function but takes an array of aggregation pipeline stages as the first argument. direct-read-namespaces = [test.test] change-stream-namespaces = [test.test] [[pipeline]] script = module.exports = function(ns, changeStream) { if (changeStream) { return [ { $match: { fullDocument.foo : 1} } ]; } else { return [ { $match: { foo : 1} } ]; } } [[script]] namespace = test.test script = module.exports = function(doc, ns) { doc.extra = pipe([ { $match: {foo: 1} }, { $limit: 1 }, { $project: { _id: 0, foo: 1}} ]); return doc; }","title":"monstache v4.10.0"},{"location":"about/#monstache-v3170","text":"Fix for issue #97 where monstache would exit before syncing all documents with -exit-after-direct-reads enabled Support added for MongoDB change streams via the change-stream-namespaces option New golang plugin functions Process and Pipeline . The Process function allows one to code complex processing after an event. The Process function has access to the MongoDB session, the Elasticsearch client, the Elasticsearch bulk processor, and information about the change that occurred (insert, update, delete). The Pipeline function allows one to assign MongoDB pipeline stages to both direct reads and change streams. Since the pipeline stages may differ between direct reads and change streams the function is passed a boolean indicating the source of the data. For example, a $match clause on the change stream may need to reference the fullDocument field since the root will be the change event. For direct reads the root will simply be the full document. New config option pipeline allows one to create aggregation pipelines in javascript for direct reads and change streams. This can be used instead of the Pipeline function in a golang plugin. The exported function in javascript takes a namespace and a boolean indicating whether or not the source was a change stream. The function should return an array of pipeline stages to apply. New config option pipe-allow-disk which when enabled allows large pipelines to use the disk to save intermediate results. New global function available in javascript script functions named pipe . The pipe function is simliar to existing find function but takes an array of aggregation pipeline stages as the first argument.","title":"monstache v3.17.0"},{"location":"about/#monstache-v490","text":"Fix to omit version information on deletes when the index-as-update setting is ON (to match the omitted version information at indexing time) Fix issue #89 by making the indexed oplog field names and date format configurable","title":"monstache v4.9.0"},{"location":"about/#monstache-v3160","text":"Fix to omit version information on deletes when the index-as-update setting is ON (to match the omitted version information at indexing time) Fix issue #89 by making the indexed oplog field names and date format configurable","title":"monstache v3.16.0"},{"location":"about/#monstache-v480","text":"Seperate namespace regexes for drop operations vs create/update/delete operations Better handling of panics A new index-as-update boolean config option that allow merge instead of replace Fixes to the find and findOne functions available in scripts","title":"monstache v4.8.0"},{"location":"about/#monstache-v3150","text":"Seperate namespace regexes for drop operations vs create/update/delete operations Better handling of panics A new index-as-update boolean config option that allow merge instead of replace Fixes to the find and findOne functions available in scripts","title":"monstache v3.15.0"},{"location":"about/#monstache-v470","text":"add -pprof setting. When enabled and combined with -enable-http-server you can read profiling information. See Profiling for Go for more information. add -enable-easy-json setting. When enabled easy-json will be used for serialization to Elasticsearch. tweaks to the tailing code in gtm","title":"monstache v4.7.0"},{"location":"about/#monstache-v3140","text":"add -pprof setting. When enabled and combined with -enable-http-server you can read profiling information. See Profiling for Go for more information. add -enable-easy-json setting. When enabled easy-json will be used for serialization to Elasticsearch. tweaks to the tailing code in gtm","title":"monstache v3.14.0"},{"location":"about/#monstache-v465","text":"decrease the fetch channel flush timeout clarify version conflicts and invalid json messages as warnings remove the /config endpoint for better security (use -print-config instead) fix LoadPlugins method (contributed by @YouthLab) performance tweaks in gtm","title":"monstache v4.6.5"},{"location":"about/#monstache-v3135","text":"decrease the fetch channel flush timeout clarify version conflicts and invalid json messages as warnings remove the /config endpoint for better security (use -print-config instead) fix LoadPlugins method (contributed by @YouthLab) performance tweaks in gtm","title":"monstache v3.13.5"},{"location":"about/#monstache-v464","text":"Expose new setting direct-read-split-max which limits the number of times a collection is split for reading during direct-reads and thus the number of go routines and MongoDB connections spawned. The default is 9. Tune this setting to increase/decrease the amount of memory the monstache process will consume. Better Docker support due to the contributions of @a-magdy.","title":"monstache v4.6.4"},{"location":"about/#monstache-v3134","text":"Expose new setting direct-read-split-max which limits the number of times a collection is split for reading during direct-reads and thus the number of go routines and MongoDB connections spawned. The default is 9. Tune this setting to increase/decrease the amount of memory the monstache process will consume. Better Docker support due to the contributions of @a-magdy.","title":"monstache v3.13.4"},{"location":"about/#monstache-v463","text":"Fix for issue #65, year outside of [0,9999]. Invalid time will be removed now with prune-invalid-json turned on Fix for issue #62, the number of connections to MongoDB is now limited to a max of 32 per namespace Fix for issue #59, unsupported values of +/- Infinity and NaN. These values can now be removed with the prune-invalid-json setting Fix for issue $46 and #66, having to do with filtering. Filters now use locks to ensure the javascript environment is used by one at a time.","title":"monstache v4.6.3"},{"location":"about/#monstache-v3133","text":"Fix for issue #65, year outside of [0,9999]. Invalid time will be removed now with prune-invalid-json turned on Fix for issue #62, the number of connections to MongoDB is now limited to a max of 32 per namespace Fix for issue #59, unsupported values of +/- Infinity and NaN. These values can now be removed with the prune-invalid-json setting Fix for issue $46 and #66, having to do with filtering. Filters now use locks to ensure the javascript environment is used by one at a time.","title":"monstache v3.13.3"},{"location":"about/#monstache-v462","text":"Fix regression in 3.13 series where collections under 50K documents were not synching Performing Tuning. The following defaults have changed so please update your config files accordingly. elasticsearch-max-conns went from 10 - 4 elasticsearch-max-docs went from 1000 - do not flush based on count (I suggest not overriding this since document sizes can vary greatly - instead use max-bytes) elasticsearch-max-bytes went from 5MB - 8MB Note when you specify elasticseach-max-bytes the value must be in bytes not MB","title":"monstache v4.6.2"},{"location":"about/#monstache-v3132","text":"Fix regression in 3.13 series where collections under 50K documents were not synching Performing Tuning. The following defaults have changed so please update your config files accordingly. elasticsearch-max-conns went from 10 - 4 elasticsearch-max-docs went from 1000 - do not flush based on count (I suggest not overriding this since document sizes can vary greatly - instead use max-bytes) elasticsearch-max-bytes went from 5MB - 8MB Note when you specify elasticseach-max-bytes the value must be in bytes not MB","title":"monstache v3.13.2"},{"location":"about/#monstache-v461","text":"Performance and bug fixes in the gtm library","title":"monstache v4.6.1"},{"location":"about/#monstache-v3131","text":"Performance and bug fixes in the gtm library","title":"monstache v3.13.1"},{"location":"about/#monstache-v460","text":"Performance improvements. Much of the performance gains come from an upgrade of the gtm library. This library now uses split vector failing back to a paginated range queries. Also, some buffering has been removed at the gtm level for certain operations.","title":"monstache v4.6.0"},{"location":"about/#monstache-v3130","text":"Performance improvements. Much of the performance gains come from an upgrade of the gtm library. This library now uses split vector failing back to a paginated range queries. Also, some buffering has been removed at the gtm level for certain operations.","title":"monstache v3.13.0"},{"location":"about/#monstache-v450","text":"Adds an option delete-index-pattern to specify an Elasticsearch index pattern to scope stateless deletes. Indexes outside of this pattern will not be considered when propogating deletes from MongoDB to Elasticsearch. By default all Elasticsearch indexes are queried. Adds the ability to specify a global filter function in Javascript. Previously, a filter function needed to be tied to a MongoDB namespace. Now you can leave off the namespace and the filter function will be applied to all namespaces. The filter function will receive the document as the first argument and the MongoDB namespace as the second argument. Breaking change: direct-read-cursors and direct-read-batch-size have been removed as options. The underlying gtm library of monstache has been upgraded and no longer supports parallelCollectionScan which is being removed in future versions of MongoDB. Now gtm will use splitVector to divy up collections to read documents concurrently. Also, the batch size will be managed by MongoDB and not set explicitly. See the gtm library docs for more information. Adds a boolean configuration option, prune-invalid-json , which defaults to false. Set this to true if your MongoDB data has values such as +Inf, -Inf, or NaN which are not supported by the golang JSON parser and cause infinite error loops to occur. With prune-invalid-json set to true Monstache will remove these values before indexing into Elasticsearch to avoid these errors.","title":"monstache v4.5.0"},{"location":"about/#monstache-v3120","text":"Adds an option delete-index-pattern to specify an Elasticsearch index pattern to scope stateless deletes. Indexes outside of this pattern will not be considered when propogating deletes from MongoDB to Elasticsearch. By default all Elasticsearch indexes are queried. Adds the ability to specify a global filter function in Javascript. Previously, a filter function needed to be tied to a MongoDB namespace. Now you can leave off the namespace and the filter function will be applied to all namespaces. The filter function will receive the document as the first argument and the MongoDB namespace as the second argument. Breaking change: direct-read-cursors and direct-read-batch-size have been removed as options. The underlying gtm library of monstache has been upgraded and no longer supports parallelCollectionScan which is being removed in future versions of MongoDB. Now gtm will use splitVector to divy up collections to read documents concurrently. Also, the batch size will be managed by MongoDB and not set explicitly. See the gtm library docs for more information. Adds a boolean configuration option, prune-invalid-json , which defaults to false. Set this to true if your MongoDB data has values such as +Inf, -Inf, or NaN which are not supported by the golang JSON parser and cause infinite error loops to occur. With prune-invalid-json set to true Monstache will remove these values before indexing into Elasticsearch to avoid these errors.","title":"monstache v3.12.0"},{"location":"about/#monstache-v440","text":"Updated the default delete strategy Breaking change: check delete-strategy","title":"monstache v4.4.0"},{"location":"about/#monstache-v3110","text":"Updated the default delete strategy Breaking change: check delete-strategy","title":"monstache v3.11.0"},{"location":"about/#monstache-v432","text":"Allow specifying a script without a namespace. In this case documents from all collections will be run through the script. The document object will continue to be the 1st argument to the function and a new 2nd argument will be the namespace of the source document. Fixes #55. Fix an issue where a Date object created in Javascript would not be formatted correctly for indexing. Build with go 1.10.1","title":"monstache v4.3.2"},{"location":"about/#monstache-v3102","text":"Allow specifying a script without a namespace. In this case documents from all collections will be run through the script. The document object will continue to be the 1st argument to the function and a new 2nd argument will be the namespace of the source document. Fixes #55. Fix an issue where a Date object created in Javascript would not be formatted correctly for indexing. Build with go 1.10.1","title":"monstache v3.10.2"},{"location":"about/#monstache-v431","text":"Upgrade gtm to pick up fix for parallel collection scans on direct reads. Each cursor now gets its own connection.","title":"monstache v4.3.1"},{"location":"about/#monstache-v3101","text":"Upgrade gtm to pick up fix for parallel collection scans on direct reads. Each cursor now gets its own connection.","title":"monstache v3.10.1"},{"location":"about/#monstache-v430","text":"Upgrade gtm to pick up support for parallel collection scan on direct reads if your mongodb storage engine supports it Add config option to specify the number of cursors to request for parallel collection scans Allow mappings to specify overrides for 1 of index and type instead of requiring both Fix an issue where filters were not being applied to document updates","title":"monstache v4.3.0"},{"location":"about/#monstache-v3100","text":"Upgrade gtm to pick up support for parallel collection scan on direct reads if your mongodb storage engine supports it Add config option to specify the number of cursors to request for parallel collection scans Allow mappings to specify overrides for 1 of index and type instead of requiring both Fix an issue where filters were not being applied to document updates","title":"monstache v3.10.0"},{"location":"about/#monstache-v421","text":"Ensure index names are lowercase","title":"monstache v4.2.1"},{"location":"about/#monstache-v391","text":"Ensure index names are lowercase","title":"monstache v3.9.1"},{"location":"about/#monstache-v420","text":"Add filtering to Javascript and Golang plugins. Filtered documents are completely ignored while dropped documents result in a delete request.","title":"monstache v4.2.0"},{"location":"about/#monstache-v390","text":"Add filtering to Javascript and Golang plugins. Filtered documents are completely ignored while dropped documents result in a delete request.","title":"monstache v3.9.0"},{"location":"about/#monstache-v412","text":"Fix custom routing for golang plugins Configuration now supports paths to Javascript files in addition to inline scripts","title":"monstache v4.1.2"},{"location":"about/#monstache-v382","text":"Fix custom routing for golang plugins Configuration now supports paths to Javascript files in addition to inline scripts","title":"monstache v3.8.2"},{"location":"about/#monstache-v411","text":"Route time machine docs by MongoDB source id","title":"monstache v4.1.1"},{"location":"about/#monstache-v381","text":"Route time machine docs by MongoDB source id","title":"monstache v3.8.1"},{"location":"about/#monstache-v410","text":"Add a nifty time machine feature","title":"monstache v4.1.0"},{"location":"about/#monstache-v380","text":"Add a nifty time machine feature","title":"monstache v3.8.0"},{"location":"about/#monstache-v401","text":"Fixed a bug where monstache would think direct reads were done when they had not even started Performance improvements for direct reads on large collections","title":"monstache v4.0.1"},{"location":"about/#monstache-v370","text":"Fixed a bug where monstache would think direct reads were done when they had not even started Performance improvements for direct reads on large collections","title":"monstache v3.7.0"},{"location":"about/#monstache-v400","text":"Monstache v4+ should be used for ES6+. There will still be bug fixes and maintenance done to the Monstache v3 releases to support ES2-5. You can still download v3.x releases from the downloads page or by directing go get to gopkg.in/rwynn/monstache.v3 Fixes deprecation warnings during bulk indexing against ES6 because of renamed fields version and version_type Monstache will now default to using the ES type _doc (as opposed to the MongoDB collection name) when it detects ES 6.2+. This is the new recommended type name going forward. See issue #42.","title":"monstache v4.0.0"},{"location":"about/#monstache-v365","text":"Remove brittle normalization of index names, type names, and ids Start differentiating between releases supporting ES6+ and pre-ES6 by releasing from rel3 branch Soon a 4.0.0 release will be cut from master that will be ES6 forward. pre-ES6 will still be supported by downloading 3.x releases from the releases page or directing go get to gopkg.in/rwynn/monstache.v3 Technically this release will still work with ES+ but that won't last forever. There are some deprecation warnings. In summary, if you need pre-ES6 use v3.x releases of monstache and v4.x releases of monstache for ES6+ going forward.","title":"monstache v3.6.5"},{"location":"about/#monstache-v364","text":"Trying to set the record for github releases in one night Fix a regression whereby monstache would exit after direct reads were complete when it should have kept tailing the oplog","title":"monstache v3.6.4"},{"location":"about/#monstache-v363","text":"Fix for a benign race condition in shutdown, introduced in 3.6.2, that caused a panic","title":"monstache v3.6.3"},{"location":"about/#monstache-v362","text":"Resume usage of upstream elastic client library now that fix for Elasticsearch going down has been merged When Elasticsearch goes down the elastic client will now put back pressure on Add and Flush calls. When Elasticsearch comes back up it will resume Adding and Flushing were it left off. Do to the blocking nature of Add and Flush the shutdown function of monstache has been refactored to take this into account. Shutdown will not hang if Elasticsearch is down. It will try to Flush pending documents but if this blocks due to a down server it will still exit after a 5 second deadline.","title":"monstache v3.6.2"},{"location":"about/#monstache-v361","text":"Added more detailed error logging. Each bulk request line that failed will be logged separately with details. This is much more lightweight than having to turn on verbose to get error details. Verbose is not a recommended setting for production.","title":"monstache v3.6.1"},{"location":"about/#monstache-v360","text":"This release focuses on improvements with regards to handling dropped connections to either Elasticsearch or MongoDB and resuming gracefully when they come back online","title":"monstache v3.6.0"},{"location":"about/#monstache-v352","text":"The previous release safeguards the integrity of inserts and updates with a version number, but neglected deletes. This release adds versions to deletes such that an [insert, delete] sequence that gets sent to Elasticsearch in 2 different requests (due to elasticsearch-max-conns 1) cannot actually perform a [delete, insert] instead. In this case the insert would now carry a version number the delete version number and be rejected.","title":"monstache v3.5.2"},{"location":"about/#monstache-v351","text":"Fix for issue #37 - out of order indexing due to concurrent bulk indexing requests. With elasticsearch-max-conns set to greater than 1 you may get out of order index requests; however after this fix each document is versioned such that Elasticsearch will not replace a newer version with an older one. The version of the document is the timestamp from the MongoDB oplog of when the change (insert, update) occurred. Out of order indexing typically happens when both an insert and an update are queued for a bulk request at around the same time. In this case, do to the way the bulk processor multiplexes requests onto multiple connections, the document may be received out of order.","title":"monstache v3.5.1"},{"location":"about/#monstache-v350","text":"Support for sharded MongoDB cluster. See docs for details Performance optimizations Turn off bulk retries if configured to do so","title":"monstache v3.5.0"},{"location":"about/#monstache-v342","text":"Allow the stats index name format to be configurable. Continues to default to index per day.","title":"monstache v3.4.2"},{"location":"about/#monstache-v341","text":"Fix for the javascript mapping functions. An Otto Export does not appear to recurse into arrays. Need to do a recursive Export for this scenario.","title":"monstache v3.4.1"},{"location":"about/#monstache-v340","text":"Add ability to embed documents during the mapping phase. Javascript plugins get 3 new global functions: findId, findOne, and find. Golang plugins get access to the mgo.Session. See the docs for details.","title":"monstache v3.4.0"},{"location":"about/#monstache-v331","text":"Improve support for additional indexing metadata. Fix issue where indexing metadata was not honored","title":"monstache v3.3.1"},{"location":"about/#monstache-v330","text":"Added optional http server. Enable with --enable-http-server flag. Listens on :8080 by default. Configure address with --http-server-addr :8000. The server responds to the following endpoints (/started, /healthz, /config, and /stats). The stats endpoint is only enabled if stats are enabled. The /started and /healthz endpoints can be used to check for liveness. Upgraded the gtm library with performance improvements","title":"monstache v3.3.0"},{"location":"about/#monstache-v320","text":"Add systemd support","title":"monstache v3.2.0"},{"location":"about/#monstache-v312","text":"Built with go1.9 Fix golint warnings","title":"monstache v3.1.2"},{"location":"about/#monstache-v311","text":"timestamp stats indexes by day for easier cleanup using e.g. curator","title":"monstache v3.1.1"},{"location":"about/#monstache-v310","text":"add print-config argument to display the configuration and exit add index-stats option to write indexing statistics into Elasticsearch for analysis","title":"monstache v3.1.0"},{"location":"about/#monstache-v307","text":"fix elasticsearch client http scheme for secure connections","title":"monstache v3.0.7"},{"location":"about/#monstache-v306","text":"fix invalid struct field tag","title":"monstache v3.0.6"},{"location":"about/#monstache-v305","text":"add direct-read-batch-size option upgrade gtm to accept batch size and to ensure all direct read errors are logged","title":"monstache v3.0.5"},{"location":"about/#monstache-v304","text":"fix slowdown on direct reads for large mongodb collections","title":"monstache v3.0.4"},{"location":"about/#monstache-v303","text":"small changes to the settings for the exponential back off on retry. see the docs for details. only record timestamps originating from the oplog and not from direct reads apply the worker routing filter to direct reads in worker mode","title":"monstache v3.0.3"},{"location":"about/#monstache-v302","text":"add option to configure elasticsearch client http timout. up the default timeout to 60 seconds","title":"monstache v3.0.2"},{"location":"about/#monstache-v301","text":"upgrade gtm to fix an issue where a mongodb query error (such as CappedPositionLost) causes the tail go routine to exit (after which no more events will be processed)","title":"monstache v3.0.1"},{"location":"about/#monstache-v300","text":"new major release configuration changes with regards to Elasticsearch. see docs for details adds ability to write rolling logs to files adds ability to log indexing statistics changed go Elasticsearch client from elastigo to elastic which provides more API coverage upgrade gtm","title":"monstache v3.0.0"},{"location":"about/#monstache-v2140","text":"add support for golang plugins. you can now do in golang what you previously could do in javascript add more detail to bulk indexing errors upgrade gtm","title":"monstache v2.14.0"},{"location":"about/#monstache-v2130","text":"add direct-read-ns option. allows one to sync documents directly from a set of collections in addition to going through the oplog add exit-after-direct-reads option. tells monstache to exit after performing direct reads. useful for running monstache as a cron job. fix issue around custom routing where db name was being stored as an array upgrade gtm","title":"monstache v2.13.0"},{"location":"about/#monstache-v2120","text":"Fix order of operations surrounding db or collection drops in the oplog. Required the removal of some gtm-options introduced in 2.11. Built with latest version of gtm which includes some performance gains Add ssl option under mongo-dial-settings. Previously, in order to enable connections with TLS one had to provide a PEM file. Now, one can enable TLS without a PEM file by setting this new option to true. This was tested with MongoDB Atlas which requires SSL but does not provide a PEM file","title":"monstache v2.12.0"},{"location":"about/#monstache-v2112","text":"Built with Go 1.8 Added option fail-fast Added option index-oplog-time","title":"monstache v2.11.2"},{"location":"about/#monstache-v2111","text":"Built with Go 1.8 Performance improvements Support for rfc7386 JSON merge patches Support for overriding Elasticsearch index and type in JavaScript More configuration options surfaced","title":"monstache v2.11.1"},{"location":"about/#monstache-v2100","text":"add shard routing capability add Makefile","title":"monstache v2.10.0"},{"location":"about/#monstache-v293","text":"extend ttl for active in cluster to reduce process switching","title":"monstache v2.9.3"},{"location":"about/#monstache-v292","text":"fix potential collision on floating point _id closes #16","title":"monstache v2.9.2"},{"location":"about/#monstache-v291","text":"fix an edge case #18 where a process resuming for the cluster would remain paused","title":"monstache v2.9.1"},{"location":"about/#monstache-v29","text":"fix an issue with formatting of integer ids enable option for new clustering feature for high availability add TLS skip verify options for mongodb and elasticsearch add an option to specify a specific timestamp to start syncing from","title":"monstache v2.9"},{"location":"about/#monstache-v281","text":"fix an index out of bounds panic during error reporting surface gtm options for setting the oplog database and collection name as well as the cursor timeout report an error if unable to unzip a response when verbose is true","title":"monstache v2.8.1"},{"location":"about/#monstache-v28","text":"add a version flag -v document the elasticsearch-pem-file option add the elasticsearch-hosts option to configure pool of available nodes within a cluster","title":"monstache v2.8"},{"location":"about/#monstache-v27","text":"add a gzip configuration option to increase performance default resume-name to the worker name if defined decrease binary size by building with -ldflags \"-w\"","title":"monstache v2.7"},{"location":"about/#monstache-v26","text":"reuse allocations made for gridfs files add workers feature to distribute synching between multiple processes","title":"monstache v2.6"},{"location":"about/#monstache-v25","text":"add option to speed up writes when saving resume state remove extra buffering when adding file content","title":"monstache v2.5"},{"location":"about/#monstache-v24","text":"Fixed issue #10 Fixed issue #11","title":"monstache v2.4"},{"location":"about/#monstache-v23","text":"Added configuration option for max file size Added code to normalize index and type names based on restrictions in Elasticsearch Performance improvements for GridFs files","title":"monstache v2.3"},{"location":"about/#monstache-v22","text":"Added configuration option for dropped databases and dropped collections. See the README for more information.","title":"monstache v2.2"},{"location":"about/#monstache-v21","text":"Added support for dropped databases and collections. Now when you drop a database or collection from mongodb the corresponding indexes are deleted in elasticsearch.","title":"monstache v2.1"},{"location":"about/#monstache-v20","text":"Fixes an issue with the default mapping between mongodb and elasticsearch. Previously, each database in mongodb was mapped to an index of the same name in elasticsearch. This creates a problem because mongodb document ids are only guaranteed unique at the collection level. If there are 2 or more documents in a mongodb database with the same id those documents were previously written to the same elasticsearch index. This fix changes the default mapping such that the entire mongodb document namespace (database + collection) is mapped to the destination index in elasticsearch. This prevents the possibility of collisions within an index. Since this change requires reindexing of previously indexed data using monstache, the version number of monstache was bumped to 2. This change also means that by default you will have an index in elasticsearch for each mongodb collection instead of each mongod database. So more indexes by default. You still have control to override the default mapping. See the docs for how to explicitly control the index and type used for a particular mongodb namespace. Bumps the go version to 1.7.3","title":"monstache v2.0"},{"location":"about/#monstache-v131","text":"Version 1.3 rebuilt with go1.7.1","title":"monstache v1.3.1"},{"location":"about/#monstache-v13","text":"Improve log messages Add support for the ingest-attachment plugin in elasticsearch 5","title":"monstache v1.3"},{"location":"about/#monstache-v12","text":"Improve Error Reporting and Add Config Options","title":"monstache v1.2"},{"location":"about/#monstache-v11","text":"Fixes crash during replay (issue #2) Adds supports for indexing GridFS content (issue #3)","title":"monstache v1.1"},{"location":"about/#monstache-v10","text":"64-bit Linux binary built with go1.6.2","title":"monstache v1.0"},{"location":"about/#monstache-v08-beta2","text":"64-bit Linux binary built with go1.6.2","title":"monstache v0.8-beta.2"},{"location":"about/#monstache-v08-beta1","text":"64-bit Linux binary built with go1.6.2","title":"monstache v0.8-beta.1"},{"location":"about/#monstache-v08-beta","text":"64-bit Linux binary built with go1.6.2","title":"monstache v0.8-beta"},{"location":"about/#monstache-v08-alpha","text":"64-bit Linux binary built with go1.6.2","title":"monstache v0.8-alpha"},{"location":"advanced/","text":"Advanced Versions Monstache has 2 separate code streams to accomodate differences in the Elasticsearch API. Monstache versions 4.X are designed to work with Elasticsearch 6+. Monstache versions 3.X are designed to work with Elasticsearch 2 and 5. If you are working with Elasticsearch 6+ you should use the master branch at github.com/rwynn/monstache . If you are working with Elasticsearch 2 or 5 you should use the rel3 branch at github.com/rwynn/monstache . If you are working with Elasticsearch 6+ and coding golang plugins for monstache you should use the master branch and your plugin should import github.com/rwynn/monstache/monstachemap . If you are working with Elasticsearch 2 or 5 and coding golang plugins for monstache you should use the rel3 branch and your plugin should import gopkg.in/rwynn/monstache.v3/monstachemap . If you are working with Elasticsearch 6+ and using the monstache Docker images you should use the docker image rwynn/monstache:latest or a specific 4.X image such as rwynn/monstache:4.11.4 . If you are working with Elasticsearch 2 or 5 and using the monstache Docker images you should use the docker image rwynn/monstache:rel3 or a specific 3.X image such as rwynn/monstache:3.18.4 . GridFS Support Monstache supports indexing the raw content of files stored in GridFS into Elasticsearch for full text search. This feature requires that you install an Elasticsearch plugin which enables the field type attachment . For versions of Elasticsearch prior to version 5 you should install the mapper-attachments plugin. For version 5 or later of Elasticsearch you should instead install the ingest-attachment plugin. Once you have installed the appropriate plugin for Elasticsearch, getting file content from GridFS into Elasticsearch is as simple as configuring monstache. You will want to enable the index-files option and also tell monstache the namespace of all collections which will hold GridFS files. For example in your TOML config file, index-files = true direct-read-namespaces = [ users.fs.files , posts.fs.files ] file-namespaces = [ users.fs.files , posts.fs.files ] file-highlighting = true The above configuration tells monstache that you wish to index the raw content of GridFS files in the users and posts MongoDB databases. By default, MongoDB uses a bucket named fs , so if you just use the defaults your collection name will be fs.files . However, if you have customized the bucket name, then your file collection would be something like mybucket.files and the entire namespace would be users.mybucket.files . When you configure monstache this way it will perform an additional operation at startup to ensure the destination indexes in Elasticsearch have a field named file with a type mapping of attachment . For the example TOML configuration above, monstache would initialize 2 indices in preparation for indexing into Elasticsearch by issuing the following REST commands: For Elasticsearch versions prior to version 5... POST /users.fs.files { \"mappings\": { \"fs.files\": { \"properties\": { \"file\": { \"type\": \"attachment\" } }}}} POST /posts.fs.files { \"mappings\": { \"fs.files\": { \"properties\": { \"file\": { \"type\": \"attachment\" } }}}} For Elasticsearch version 5 and above... PUT /_ingest/pipeline/attachment { \"description\" : \"Extract file information\", \"processors\" : [ { \"attachment\" : { \"field\" : \"file\" } } ] } When a file is inserted into MongoDB via GridFS, monstache will detect the new file, use the MongoDB api to retrieve the raw content, and index a document into Elasticsearch with the raw content stored in a file field as a base64 encoded string. The Elasticsearch plugin will then extract text content from the raw content using Apache Tika , tokenize the text content, and allow you to query on the content of the file. To test this feature of monstache you can simply use the mongofiles command to quickly add a file to MongoDB via GridFS. Continuing the example above one could issue the following command to put a file named resume.docx into GridFS and after a short time this file should be searchable in Elasticsearch in the index users.fs.files . mongofiles -d users put resume.docx After a short time you should be able to query the contents of resume.docx in the users index in Elasticsearch curl -XGET \"http://localhost:9200/users.fs.files/_search?q=golang\" If you would like to see the text extracted by Apache Tika you can project the appropriate sub-field For Elasticsearch versions prior to version 5... curl -H \"Content-Type:application/json\" localhost:9200/users.fs.files/_search?pretty -d '{ \"fields\": [ \"file.content\" ], \"query\": { \"match\": { \"file.content\": \"golang\" } } }' For Elasticsearch version 5 and above... curl -H \"Content-Type:application/json\" localhost:9200/users.fs.files/_search?pretty -d '{ \"_source\": [ \"attachment.content\" ], \"query\": { \"match\": { \"attachment.content\": \"golang\" } } }' When file-highlighting is enabled you can add a highlight clause to your query For Elasticsearch versions prior to version 5... curl -H \"Content-Type:application/json\" localhost:9200/users.fs.files/_search?pretty -d '{ \"fields\": [\"file.content\"], \"query\": { \"match\": { \"file.content\": \"golang\" } }, \"highlight\": { \"fields\": { \"file.content\": { } } } }' For Elasticsearch version 5 and above... curl -H \"Content-Type:application/json\" localhost:9200/users.fs.files/_search?pretty -d '{ \"_source\": [\"attachment.content\"], \"query\": { \"match\": { \"attachment.content\": \"golang\" } }, \"highlight\": { \"fields\": { \"attachment.content\": { } } } }' The highlight response will contain emphasis on the matching terms For Elasticsearch versions prior to version 5... \"hits\" : [ { \"highlight\" : { \"file.content\" : [ \"I like to program in em golang /em .\\n\\n\" ] } } ] For Elasticsearch version 5 and above... \"hits\" : [{ \"highlight\" : { \"attachment.content\" : [ \"I like to program in em golang /em .\" ] } }] Workers You can run multiple monstache processes and distribute the work between them. First configure the names of all the workers in a shared config.toml file. workers = [ Tom , Dick , Harry ] In this case we have 3 workers. Now we can start 3 monstache processes and give each one of the worker names. monstache -f config.toml -worker Tom monstache -f config.toml -worker Dick monstache -f config.toml -worker Harry monstache will hash the id of each document using consistent hashing so that each id is handled by only one of the available workers. High Availability You can run monstache in high availability mode by starting multiple processes with the same value for cluster-name . Each process will join a cluster which works together to ensure that a monstache process is always syncing to Elasticsearch. High availability works by ensuring one active process in the monstache.cluster collection in MongoDB at any given time. Only the process in this collection will be syncing for the cluster. Processes not present in this collection will be paused. Documents in the monstache.cluster collection have a TTL assigned to them. When a document in this collection times out it will be removed from the collection by MongoDB and another process in the monstache cluster will have a chance to write to the collection and become the new active process. When cluster-name is supplied the resume feature is automatically turned on and the resume-name becomes the name of the cluster. This is to ensure that each of the processes is able to pick up syncing where the last one left off. You can combine the HA feature with the workers feature. For 3 cluster nodes with 3 workers per node you would have something like the following: // config.toml workers = [\"Tom\", \"Dick\", \"Harry\"] // on host A monstache -cluster-name HA -worker Tom -f config.toml monstache -cluster-name HA -worker Dick -f config.toml monstache -cluster-name HA -worker Harry -f config.toml // on host B monstache -cluster-name HA -worker Tom -f config.toml monstache -cluster-name HA -worker Dick -f config.toml monstache -cluster-name HA -worker Harry -f config.toml // on host C monstache -cluster-name HA -worker Tom -f config.toml monstache -cluster-name HA -worker Dick -f config.toml monstache -cluster-name HA -worker Harry -f config.toml When the clustering feature is combined with workers then the resume-name becomes the cluster name concatenated with the worker name. Index Mapping When indexing documents from MongoDB into Elasticsearch the default mapping is as follows: For Elasticsearch prior to 6.2 Elasticsearch index name = MongoDB database name . MongoDB collection name Elasticsearch type = MongoDB collection name Elasticsearch document _id = MongoDB document _id For Elasticsearch 6.2+ Elasticsearch index name = MongoDB database name . MongoDB collection name Elasticsearch type = _doc Elasticsearch document _id = MongoDB document _id If these default won't work for some reason you can override the index and type mapping on a per collection basis by adding the following to your TOML config file: [[mapping]] namespace = test.test index = index1 type = type1 [[mapping]] namespace = test.test2 index = index2 type = type2 With the configuration above documents in the test.test namespace in MongoDB are indexed into the index1 index in Elasticsearch with the type1 type. If you need your index and type mapping to be more dynamic, such as based on values inside the MongoDB document, then see the sections Middleware and Routing . Warning It is not recommended to override the default type of _doc if using Elasticsearch 6.2+ since this will be the supported path going forward. Also, using _doc as the type will not work with Elasticsearch prior to 6.2. Make sure that automatic index creation is not disabled in elasticsearch.yml or create your target indexes before using Monstache. If automatic index creation must be controlled, whitelist any indexes in elasticsearch.yml that monstache will create. Namespaces When a document is inserted, updated, or deleted in MongoDB a document is appended to the oplog representing the event. This document has a field ns which is the namespace. For inserts, updates, and deletes the namespace is the database name and collection name of the document changed joined by a dot. E.g. for use test; db.foo.insert({hello: \"world\"}); the namespace for the event in the oplog would be test.foo . In addition to inserts, updates, and deletes monstache also supports database and collection drops. When a database or collection is dropped in MongoDB an event is appended to the oplog. Like the other types of changes this event has a field ns representing the namespace. However, for drops the namespace is the database name and the string $cmd joined by a dot. E.g. for use test; db.foo.drop() the namespace for the event in the oplog would be test.$cmd . Middleware monstache supports embedding user defined middleware between MongoDB and Elasticsearch. middleware is able to transform documents, drop documents, or define indexing metadata. middleware may be written in either Javascript or in Golang as a plugin. Golang plugins require Go version 1.8 or greater on Linux. currently, you are able to use Javascript or Golang but not both (this may change in the future). Golang monstache supports Golang 1.8+ plugins on Linux. To implement a plugin for monstache you simply need to implement a specific function signature, use the go command to build a .so file for your plugin, and finally pass the path to your plugin .so file when running monstache. Note If you are working with Elasticsearch 6+ and coding golang plugins for monstache you should use the master branch and your plugin should import github.com/rwynn/monstache/monstachemap . If you are working with Elasticsearch 2 or 5 and coding golang plugins for monstache you should use the rel3 branch and your plugin should import gopkg.in/rwynn/monstache.v3/monstachemap . To create a golang plugin for monstache get the necessary dependencies for your version of Elasticsearch with go get -u github.com/rwynn/monstache/monstachemap or go get -u gopkg.in/rwynn/monstache.v3/monstachemap create a .go source file that belongs to the package main import github.com/rwynn/monstache/monstachemap for Elasticsearch 6+ or gopkg.in/rwynn/monstache.v3/monstachemap for Elasticsearch 2 or 5 implement a function named Map with the following signature func Map(input *monstachemap.MapperPluginInput) (output *monstachemap.MapperPluginOutput, err error) optionally implement a function named Filter with the following signature func Filter(input *monstachemap.MapperPluginInput) (keep bool, err error) optionally implement a function named Pipeline with the following signature func Pipeline(ns string, changeStream bool) (stages []interface, err error) optionally implement a function named Process with the following signature func Process(input*monstachemap.ProcessPluginInput) error plugins can be compiled using go build -buildmode=plugin -o myplugin.so myplugin.go to enable the plugin, start monstache with monstache -mapper-plugin-path /path/to/myplugin.so the following example plugin simply converts top level string values to uppercase package main import ( github.com/rwynn/monstache/monstachemap // or gopkg.in/rwynn/monstache.v3/monstachemap for Elasticsearch 2 or 5 strings ) // a plugin to convert document values to uppercase func Map(input *monstachemap.MapperPluginInput) (output *monstachemap.MapperPluginOutput, err error) { doc := input.Document for k, v := range doc { switch v.(type) { case string: doc[k] = strings.ToUpper(v.(string)) } } output = monstachemap.MapperPluginOutput{Document: doc} return } The input parameter will contain information about the document's origin database and collection: field meaning Document MongoDB document updated, inserted or deleted Namespace Operation namespace as described above Database MongoDB database from where the event came Collection MongoDB collection where the document was inserted, deleted or updated Operation Which kind of operation triggered this event, see gtm.mapOperation() . \"i\" for insert, \"u\" for update, \"d\" for delete and \"c\" for invalidate. The Map function will only receive inserts and updates. To handle deletes or invalidates implement the Process function described below. Session *mgo.Session . You need not Close the session as monstache will do this automatically when the function exits The output parameter will contain information about how the document should be treated by monstache: field meaning Document an updated document to index into Elasticsearch Index the name of the index to use Type the document type Routing the routing value to use Drop set to true to indicate that the document should not be indexed but removed Passthrough set to true to indicate the original document should be indexed unchanged Parent the parent id to use Version the version of the document VersionType the version type of the document (internal, external, external_gte) Pipeline the pipeline to index with RetryOnConflict how many times to retry updates before failing Skip set to true to indicate the the document should be ignored For detailed information see monstachemap/plugin.go Few examples are: To skip the document (direct monstache to ignore it) set output.Skip = true . To drop the document (direct monstache not to index it but remove it) set output.Drop = true . To simply pass the original document through to Elasticsearch, set output.Passthrough = true To set custom indexing metadata on the document use output.Index , output.Type , output.Parent and output.Routing . Note If you override output.Parent or output.Routing for any MongoDB namespaces in a golang plugin you should also add those namespaces to the routing-namespaces array in your config file. This instructs Monstache to save the routing information so that deletes of the document work correctly. If would like to embed other MongoDB documents (possibly from a different collection) within the current document before indexing, you can access the *mgo.Session pointer as input.Session . With the mgo session you can use the mgo API to find documents in MongoDB and embed them in the Document set on output. When you implement a Filter function the function is called immediately after reading inserts and updates from the oplog. You can return false from this function to completely ignore a document. This is different than setting output.Drop from the mapping function because when you set output.Drop to true, a delete request is issued to Elasticsearch in case the document had previously been indexed. By contrast, returning false from the Filter function causes the operation to be completely ignored and there is no corresponding delete request issued to Elasticsearch. When you implement a Pipeline function the function will be called to setup an aggregation pipeline for both direct reads and any change streams that you have configured. The aggregation pipeline stages that you return may be different depending if applied to a direct read or to a change stream. For direct reads the root document will be the document in the collection. For change streams the root document will be a change event with a fullDocument field inside it. Use the boolean parameter changeStream to alter the stages that you return from this function accordingly. When you implement a Process function the function will be called after monstache processes an event. This function has full access to the MongoDB and Elasticsearch clients (including the Elasticsearch bulk processor) in the input and allows you to handle complex event processing scenarios. The input parameter for the Process function will have all the same fields as the input to a Map function described above plus the following: field meaning ElasticClient A full featured Elasticsearch client ElasticBulkProcessor The same bulk processor monstache uses to index documents. You need only Add requests to the processor and they will be flushed in bulk automatically Timestamp The MongoDB timestamp of the change event from the oplog. In the case of direct reads the timestamp is the time at which the document was read from MongoDB. Note Under the docker/plugin folder there is a build.sh script to help you build a plugin. There is a README file in that directory with instructions. Javascript Transformation Monstache uses the amazing otto library to provide transformation at the document field level in Javascript. You can associate one javascript mapping function per MongoDB collection. You can also associate a function at the global level by not specifying a namespace. These javascript functions are added to your TOML config file, for example: [[script]] namespace = mydb.mycollection script = var counter = 1; module.exports = function(doc) { doc.foo += test + counter; counter++; return doc; } [[script]] namespace = anotherdb.anothercollection path = path/to/transform.js routing = true [[script]] # this script does not declare a namespace # it is global to all collections script = module.exports = function(doc, ns) { // the doc namespace e.g. test.test is passed as the 2nd arg return _.omit(doc, password , secret ); } The example TOML above configures 3 scripts. The first is applied to mycollection in mydb while the second is applied to anothercollection in anotherdb . The first script is inlined while the second is loaded from a file path. The path can be absolute or relative to the directory monstache is executed from. The last script does not specify a namespace, so documents from all collections pass through it. Global scripts are run before scripts which are linked to a specific namespace. You will notice that the multi-line string feature of TOML is used to assign a javascript snippet to the variable named script . The javascript assigned to script must assign a function to the exports property of the module object. This function will be passed the document from MongoDB just before it is indexed in Elasticsearch. Inside the function you can manipulate the document to drop fields, add fields, or augment the existing fields. The this reference in the mapping function is assigned to the document from MongoDB. When the return value from the mapping function is an object then that mapped object is what actually gets indexed in Elasticsearch. For these purposes an object is a javascript non-primitive, excluding Function , Array , String , Number , Boolean , Date , Error and RegExp . Filtering You can completely ignore documents by adding filter configurations to your TOML config file. The filter functions are executing immediately after inserts or updates are read from the oplog. The correspding document is passed into the function and you can return true or false to include or ignore the document. [[filter]] namespace = db.collection script = module.exports = function(doc) { return !!doc.interesting; } [[filter]] namespace = db2.collection2 path = path/to/script.js Aggregation Pipelines You can alter or filter direct reads and change streams by using a pipeline definition. Note, when building a pipeline for a change stream the root of the document will be the change event and the associated document will be under a field named fullDocument . For more information on the properties of the root document for change streams see Change Events . You can scope a pipeline to a particular namespace using the namespace attribute or leave it off to have the pipeline applied to all namespaces. [[pipeline]] script = module.exports = function(ns, changeStream) { if (changeStream) { return [ { $match: { fullDocument.foo : 1} } ]; } else { return [ { $match: { foo : 1} } ]; } } Warning You should not replace the root using $replaceRoot for a change stream since monstache needs this information. You should only make modifications to the fullDocument field in a pipeline. Dropping If the return value from the mapping function is not an object per the definition above then the result is converted into a boolean and if the boolean value is false then that indicates to monstache that you would not like to index the document. If the boolean value is true then the original document from MongoDB gets indexed in Elasticsearch. This allows you to return false or null if you have implemented soft deletes in MongoDB. [[script]] namespace = db.collection script = module.exports = function(doc) { if (!!doc.deletedAt) { return false; } return true; } In the above example monstache will index any document except the ones with a deletedAt property. If the document is first inserted without a deletedAt property, but later updated to include the deletedAt property then monstache will remove, or drop, the previously indexed document from the Elasticsearch index. Note Dropping a document is different that filtering a document. A filtered document is completely ignored. A dropped document results in a delete request being issued to Elasticsearch in case the document had previously been indexed. Scripting Features You may have noticed that in the first example above the exported mapping function closes over a var named counter . You can use closures to maintain state between invocations of your mapping function. Finally, since Otto makes it so easy, the venerable Underscore library is included for you at no extra charge. Feel free to abuse the power of the _ . Embedding Documents In your javascript function you have access to the following global functions to retreive documents from MongoDB for embedding in the current document before indexing. Using this approach you can pull in related data. function findId(documentId, [options]) { // convenience method for findOne({_id: documentId}) // returns 1 document or null } function findOne(query, [options]) { // returns 1 document or null } function find(query, [options]) { // returns an array of documents or null } function pipe(stages, [options]) { // returns an array of documents or null } Each function takes a query type object parameter and an optional options object parameter. The options object takes the following keys and values: var options = { database: test , collection: test , // to omit _id set the _id key to 0 in select select: { age: 1 }, // only applicable to find... sort: [ name ], limit: 2 } If the database or collection keys are omitted from the options object, the values for database and/or collection are set to the database and collection of the document being processed. Here are some examples: This example sorts the documents in the same collection as the document being processed by name and returns the first 2 documents projecting only the age field. The result is set on the current document before being indexed. [[script]] namespace = test.test script = module.exports = function(doc) { doc.twoAgesSortedByName = find({}, { sort: [ name ], limit: 2, select: { age: 1 } }); return doc; } This example grabs a reference id from a document and replaces it with the corresponding document with that id. [[script]] namespace = test.posts script = module.exports = function(post) { if (post.author) { // author is a an object id reference post.author = findId(post.author, { database: test , collection: users }); } return post; } This example runs an aggregation pipeline and stores the results in an extra field in the document [[script]] namespace = test.test script = module.exports = function(doc, ns) { doc.extra = pipe([ { $match: {foo: 1} }, { $limit: 1 }, { $project: { _id: 0, foo: 1}} ] // optional , { database: foo , collection: bar } // defaults to same namespace ); return doc; } Indexing Metadata You can override the indexing metadata for an individual document by setting a special field named _meta_monstache on the document you return from your Javascript function. Assume there is a collection in MongoDB named company in the test database. The documents in this collection look like either { _id : london , type : branch , name : London Westminster , city : London , country : UK } or { _id : alice , type : employee , name : Alice Smith , branch : london } Given the above the following snippet sets up a parent-child relationship in Elasticsearch based on the incoming documents from MongoDB and updates the ns (namespace) from test.company to company in Elasticsearch [[script]] namespace = test.company routing = true script = module.exports = function(doc, ns) { // var meta = { type: doc.type, index: 'company' }; var meta = { type: doc.type, index: ns.split( . )[1] }; if (doc.type === employee ) { meta.parent = doc.branch; } doc._meta_monstache = meta; return _.omit(doc, branch , type ); } The snippet above will route these documents to the company index in Elasticsearch instead of the default of test.company , if you didn't specify a namespace, it'll route all documents to indexes named as the collection only without the database db . collection (MongoDB) = collection (Elasticsearch). Also, instead of using company as the Elasticsearch type, the type attribute from the document will be used as the Elasticsearch type. Finally, if the type is employee then the document will be indexed as a child of the branch the person belongs to. We can throw away the type and branch information by deleting it from the document before returning since the type information will be stored in Elasticsearch under _type and the branch information will be stored under _parent . The example is based on the Elasticsearch docs for parent-child For more on updating the namespace name, check the Delete Strategy as there is a change of the default behviour in v4.4.0 v3.11.0 Routing Routing is the process by which Elasticsearch determines which shard a document will reside in. Monstache supports user defined, or custom, routing of your MongoDB documents into Elasticsearch. Consider an example where you have a comments collection in MongoDB which stores a comment and its associated post identifier. use blog; db.comments.insert({title: Did you read this? , post_id: 123 }); db.comments.insert({title: Yeah, it's good , post_id: 123 }); In this case monstache will index those 2 documents in an index named blog.comments under the id created by MongoDB. When Elasticsearch routes a document to a shard, by default, it does so by hashing the id of the document. This means that as the number of comments on post 123 grows, each of the comments will be distributed somewhat evenly between the available shards in the cluster. Thus, when a query is performed searching among the comments for post 123 Elasticsearch will need to query all of those shards just in case a comment happened to have been routed there. We can take advantage of the support in Elasticsearch and in monstache to do some intelligent routing such that all comments for post 123 reside in the same shard. First we need to tell monstache that we would like to do custom routing for this collection by setting routing equal to true on a custom script for the namespace. Then we need to add some metadata to the document telling monstache how to route the document when indexing. In this case we want to route by the post_id field. [[script]] namespace = blog.comments routing = true script = module.exports = function(doc) { doc._meta_monstache = { routing: doc.post_id }; return doc; } Now when monstache indexes document for the collection blog.comments it will set the special _routing attribute for the document on the index request such that Elasticsearch routes comments based on their corresponding post. The _meta_monstache field is used only to inform monstache about routing and is not included in the source document when indexing to Elasticsearch. Now when we are searching for comments and we know the post id that the comment belongs to we can include that post id in the request and make a search that normally queries all shards query only 1 shard. $ curl -H Content-Type:application/json -XGET 'http://localhost:9200/blog.comments/_search?routing=123' -d ' { query :{ match_all :{} } }' You will notice in the response that only 1 shard was queried instead of all your shards. Custom routing is great way to reduce broadcast searches and thus get better performance. The catch with custom routing is that you need to include the routing parameter on all insert, update, and delete operations. Insert and update is not a problem for monstache because the routing information will come from your MongoDB document. Deletes, however, pose a problem for monstache because when a delete occurs in MongoDB the information in the oplog is limited to the id of the document that was deleted. But monstache needs to know where the document was originally routed in order to tell Elasticsearch where to look for it. Monstache has 3 available strategies for handling deletes in this situation. The default strategy is stateless and uses a term query into Elasticsearch based on the ID of the document deleted in MongoDB. If the search into Elasticsearch returns exactly 1 document then monstache will schedule that document for deletion. The 2nd stategy monstache uses is stateful and requires giving monstache the ability to write to the collection monstache.meta . In this collection monstache stores information about documents that were given custom indexing metadata. This stategy slows down indexing and takes up space in MongoDB. However, it is precise because it records exactly how each document was indexed. The final stategy simply punts on deletes and leaves document deletion to the user. If you don't generally delete documents in MongoDB or don't care if Elasticsearch contains documents which have been deleted in MongoDB, this option is available. See Delete Strategy for more information. For more information see Customizing Document Routing In addition to letting your customize the shard routing for a specific document, you can also customize the Elasticsearch index and type using a script by putting the custom information in the meta attribute. [[script]] namespace = blog.comments routing = true script = module.exports = function(doc) { if (doc.score = 100) { // NOTE: prefix dynamic index with namespace for proper cleanup on drops doc._meta_monstache = { index: blog.comments.highscore , type: highScoreComment , routing: doc.post_id }; } else { doc._meta_monstache = { routing: doc.post_id }; } return doc; } Joins Elasticsearch 6 introduces an updated approach to parent-child called joins. The following example shows how you can accomplish joins with Monstache. The example is based on the Elasticsearch documentation . This example assumes Monstache is syncing the test.test collection in MongoDB with the test.test index in Elasticsearch. First we will want to setup an index mapping in Elasticsearch describing the join field. curl -XPUT 'localhost:9200/test.test?pretty' -H 'Content-Type: application/json' -d' { mappings : { _doc : { properties : { my_join_field : { type : join , relations : { question : answer } } } } } } ' Warning The above mapping uses _doc as the Elasticsearch type. _doc is the recommended type for new versions Elasticsearch but it only works with Elasticsearch versions 6.2 and greater. Monstache defaults to using _doc as the type when it detects Elasticsearch version 6.2 or greater. If you are using a previous version of Elasticsearch monstache defaults to using the MongoDB collection name as the Elasticsearch type. The type Monstache uses can be overriden but it is not recommended from Elasticsearch 6.2 on. Next will will configure Monstache with custom Javascript middleware that does transformation and routing. In a file called CONFIG.toml. [[script]] namespace = test.test routing = true script = module.exports = function(doc) { var routing; if (doc.type === question ) { routing = doc._id; doc.my_join_field = { name: question } } else if (doc.type === answer ) { routing = doc.question; doc.my_join_field = { name: answer , parent: routing }; } if (routing) { doc._meta_monstache = { routing: routing }; } return doc; } The mapping function adds a my_join_field field to each document. The contents of the field are based on the type attribute in the MongoDB document. Also, the function ensures that the routing is always based on the _id of the question document. Now with this config in place we can start Monstache. We will use verbose to see the requests. monstache -verbose -f CONFIG.toml With Monstache running we are now ready to insert into MongoDB rs:PRIMARY use test; switched to db test rs:PRIMARY db.test.insert({type: question , text: This is a question }); rs:PRIMARY db.test.find() { _id : ObjectId( 5a84a8b826993bde57c12893 ), type : question , text : This is a question } rs:PRIMARY db.test.insert({type: answer , text: This is an answer , question: ObjectId( 5a84a8b826993bde57c12893 ) }); When we insert these documents we should see Monstache generate the following requests to Elasticsearch { index :{ _id : 5a84a8b826993bde57c12893 , _index : test.test , _type : _doc , routing : 5a84a8b826993bde57c12893 , version :6522523668566769665, version_type : external }} { my_join_field :{ name : question }, text : This is a question , type : question } { index :{ _id : 5a84a92b26993bde57c12894 , _index : test.test , _type : _doc , routing : 5a84a8b826993bde57c12893 , version :6522524162488008705, version_type : external }} { my_join_field :{ name : answer , parent : 5a84a8b826993bde57c12893 }, question : 5a84a8b826993bde57c12893 , text : This is an answer , type : answer } This looks good. We should now have a parent/child relationship between these documents in Elasticsearch. If we do a search on the test.test index we see the following results: hits : { total : 2, max_score : 1.0, hits : [ { _index : test.test , _type : _doc , _id : 5a84a8b826993bde57c12893 , _score : 1.0, _routing : 5a84a8b826993bde57c12893 , _source : { my_join_field : { name : question }, text : This is a question , type : question } }, { _index : test.test , _type : _doc , _id : 5a84a92b26993bde57c12894 , _score : 1.0, _routing : 5a84a8b826993bde57c12893 , _source : { my_join_field : { name : answer , parent : 5a84a8b826993bde57c12893 }, question : 5a84a8b826993bde57c12893 , text : This is an answer , type : answer } } ] } To clean up our documents in Elasticsearch a bit we can omit the information that we don't really need in the source docs by updating our mapping function. This information needs not be at the top-level since it is duplicated in my_join_field . return _.omit(doc, type , question ); If your parent and child documents are in separate MongoDB collections then you would set up a script for each collection. You can tell if the doc is a parent or child by the collection it comes from. The only other difference would be that you would need to override the index dynamically in addition to the routing such that documents from both MongoDB collections target the same index. doc._meta_monstache = { routing: routing, index: parentsAndChildren }; Warning You must be careful when you route 2 or more MongoDB collections to the same Elasticsearch index that the document _ids across the MongoDB collections do not collide for any 2 docs because they will be used as the _id in the target index. Time Machines If you are not just interested in what the current value of a document in MongoDB is, but also would like to see how it has changed over time use time machine namespaces . For example, you've inserted and later updated a document with id 123 in the test.test collection in MongoDB. If test.test is a time machine namespace you will have 2 documents representing those changes in the log.test.test.2018-02-20 index (timestamp will change) in Elasticsearch. If you later want all the changes made to that document in MongoDB you can issue a query like this: $ curl -XGET 'http://localhost:9200/log.test.test.*/_search?routing=123' -d ' { query :{ sort : [ { _oplog_ts : { order : desc }} ], filtered :{ query :{ match_all :{} }, filter :{ term :{ _source_id : 123 } } } } }' That query will be very efficient because it only queries the shard that all the change docs went to for MongoDB document id 123. It filters the documents on that shard by _source_id , or id from MongoDB, to only give us the changes to that document. Finally, it sorts by the _oplog_ts which gives us the most recent change docs first. The index pattern in the query is a wildcard to pick up all the timestamped indexes that we've acculated for the test.test namespace. Merge Patches A unique feature of monstache is support for JSON Merge Patches rfc-7396 . If merge patches are enabled monstache will add an additional field to documents indexed into Elasticsearch. The name of this field is configurable but it defaults to json-merge-patches . Consider the following example with merge patches enabled... db.test.insert({name: Joe , age: 16, friends: [1, 2, 3]}) At this point you would have the following document source in Elasticsearch. \"_source\" : { \"age\" : 16, \"friends\" : [ 1, 2, 3 ], \"json-merge-patches\" : [ { \"p\" : \"{\\\"age\\\":16,\\\"friends\\\":[1,2,3],\\\"name\\\":\\\"Joe\\\"}\", \"ts\" : 1487263414, \"v\" : 1 } ], \"name\" : \"Joe\" } As you can see we have a single timestamped merge patch in the json-merge-patches array. Now let's update the document to remove a friend and update the age. db.test.update({name: Joe }, {$set: {age: 21, friends: [1, 3]}}) If we now look at the document in Elasticsearch we see the following: \"_source\" : { \"age\" : 21, \"friends\" : [ 1, 3 ], \"json-merge-patches\" : [ { \"p\" : \"{\\\"age\\\":16,\\\"friends\\\":[1,2,3],\\\"name\\\":\\\"Joe\\\"}\", \"ts\" : 1487263414, \"v\" : 1 }, { \"p\" : \"{\\\"age\\\":21,\\\"friends\\\":[1,3]}\", \"ts\" : 1487263746, \"v\" : 2 } ], \"name\" : \"Joe\" } You can see that the document was updated as expected and an additional merge patch was added. Each time the document is updated in MongoDB the corresponding document in Elasticsearch gains a timestamped merge patch. Using this information we can time travel is the document's history. There is a merge patch for each version of the document. To recreate a specific version we simply need to apply the merge patches in order up to the version that we want. To get version 1 of the document above we start with {} and apply the 1st merge patch. To get version 2 of the document above we start with {} apply the 1st merge patch to get v1 apply the 2nd merge patch to v1 to get v2 The timestamps associated with these merge patches are in seconds since the epoch, taken from the timestamp recorded in the oplog when the insert or update occured. To enable the merge patches feature in monstache you need to add the following to you TOML config: enable-patches = true patch-namespaces = [\"test.test\"] You need you add each namespace that you would like to see patches for in the patch-namespaces array. Optionally, you can change the key under which the patches are stored in the source document as follows: merge-patch-attribute = \"custom-merge-attr\" Merge patches will only be recorded for data read from the MongoDB oplog. Data read using the direct read feature will not be enhanced with merge patches. Most likely, you will want to turn off indexing for the merge patch attribute. You can do this by creating an index template for each patch namespace before running monstache... PUT /_template/test.test { \"template\" : \"test.test\", \"mappings\" : { \"test\" : { \"json-merge-patches\" : { \"index\" : false } } } } Systemd Monstache has support built in for integrating with systemd. The following monstache.service is an example systemd configuration. [Unit] Description=monstache sync service [Service] Type=notify ExecStart=/usr/local/bin/monstache -f /etc/monstache/config.toml WatchdogSec=30s Restart=always [Install] WantedBy=multi-user.target Systemd unit files are normally saved to /lib/systemd/system . Verify same with your OS documentation. After saving the monstache.service file you can run systemctl daemon-reload to tell systemd to reload all unit files. You can enable the service to start on boot with systemctl enable monstache.service and start the service with systemctl start monstache.service . With the configuration above monstache will notify systemd when it has started successfully and then notify systemd repeatedly at half the WatchDog interval to signal liveness. The configuration above causes systemd to restart monstache if it does not start or respond within the WatchdDog interval. Docker There are Docker images available for Monstache on Docker Hub You can pull and run the latest images with # for Elasticsearch = 6 docker run rwynn/monstache:latest -v # for Elasticsearch 6 docker run rwynn/monstache:rel3 -v You can pull and run release images with docker run rwynn/monstache:4.11.4 -v docker run rwynn/monstache:3.18.4 -v For example, to run monstache via Docker with a golang plugin that resides at ~/plugin/plugin.so on the host you can use a bind mount docker run --rm --net=host -v ~/plugin:/tmp/plugin rwynn/monstache:latest -mapper-plugin-path /tmp/plugin/plugin.so HTTP Server Monstache has a built in HTTP server that you can enable with --enable-http-server. It listens on :8080 by default but you can change this with --http-server-addr. When using monstache with kubernetes this server can be used to detect liveness and act accordingly The following GET endpoints are available /started Returns the uptime of the server /healthz Returns at 200 status code with the text \"ok\" when monstache is running /stats Returns the current indexing statistics in JSON format. Only available if stats are enabled /debug (if pprof is enabled) If the pprof setting is enabled the following endpoints are also made available: /debug/pprof/ /debug/pprof/cmdline /debug/pprof/profile /debug/pprof/symbol /debug/pprof/trace AWS Signature Version 4 Monstache has included AWS Signature Version 4 request signing for testing. This is in its initial stages and needs testing and feedback. To enable the experimental AWS Signature Version 4 support add the following to your config file: [aws-connect] access-key = XXX secret-key = YYY region = ZZZ You can read more about Signature Version 4 and Amazon Elasticsearch Service . MongoDB view replication You may have a situation where you want to replicate a MongoDB view in Elasticsearch. Or you have a collection that should trigger sync of another collection. You can use the relate config to do this. Consider you have a collections thing and state . A thing has an associated state and a thing is linked to a state via a field s which points to the _id of the associated state in the state collection. You can create a view in MongoDB that uses a $lookup to pull the state information in and present a view of things with the state information included. use thingdb; db.createView( thingview , thing , [ {$lookup: {from: state , localField: s , foreignField: _id , as: s }}]) Given this view you can use the following config to keep things up to date in a things index in Elasticsearch. direct-read-namespaces = [ thingdb.thingview ] # read direct from the view of the collection to seed index change-stream-namespaces = [ thingdb.thing , thingdb.state ] # change events happen on the underlying collections not views [[mapping]] namespace = thingdb.thing # map change events on the thing collection to the things index index = things [[mapping]] namespace = thingdb.thingview # map direct reads of the thingview to the same things index index = things [[relate]] namespace = thingdb.thing # when a thing changes look it up in the assoicated view by _id and index that with-namespace = thingdb.thingview keep-src = false # ignore the original thing that changed and instead just use the lookup of that thing via the view [[relate]] namespace = thingdb.states # when a state changes trigger a thing change event since thing is associated to a state with-namespace = thingdb.thing src-field = _id # use the _id field of the state that changed to lookup associated things match-field = s # only trigger change events for the things where thing.s (match-field) = state._id (src-field). keep-src = false Warning Be careful of the expense of using relate with a view. In the example above, if there were many things associated to a single state then a change to that state would trigger n+1 queries to MongoDB when n is the number of things related to the state. 1 query would be used to find all associated things and n queries would be used to lookup each thing in the view.","title":"Advanced"},{"location":"advanced/#advanced","text":"","title":"Advanced"},{"location":"advanced/#versions","text":"Monstache has 2 separate code streams to accomodate differences in the Elasticsearch API. Monstache versions 4.X are designed to work with Elasticsearch 6+. Monstache versions 3.X are designed to work with Elasticsearch 2 and 5. If you are working with Elasticsearch 6+ you should use the master branch at github.com/rwynn/monstache . If you are working with Elasticsearch 2 or 5 you should use the rel3 branch at github.com/rwynn/monstache . If you are working with Elasticsearch 6+ and coding golang plugins for monstache you should use the master branch and your plugin should import github.com/rwynn/monstache/monstachemap . If you are working with Elasticsearch 2 or 5 and coding golang plugins for monstache you should use the rel3 branch and your plugin should import gopkg.in/rwynn/monstache.v3/monstachemap . If you are working with Elasticsearch 6+ and using the monstache Docker images you should use the docker image rwynn/monstache:latest or a specific 4.X image such as rwynn/monstache:4.11.4 . If you are working with Elasticsearch 2 or 5 and using the monstache Docker images you should use the docker image rwynn/monstache:rel3 or a specific 3.X image such as rwynn/monstache:3.18.4 .","title":"Versions"},{"location":"advanced/#gridfs-support","text":"Monstache supports indexing the raw content of files stored in GridFS into Elasticsearch for full text search. This feature requires that you install an Elasticsearch plugin which enables the field type attachment . For versions of Elasticsearch prior to version 5 you should install the mapper-attachments plugin. For version 5 or later of Elasticsearch you should instead install the ingest-attachment plugin. Once you have installed the appropriate plugin for Elasticsearch, getting file content from GridFS into Elasticsearch is as simple as configuring monstache. You will want to enable the index-files option and also tell monstache the namespace of all collections which will hold GridFS files. For example in your TOML config file, index-files = true direct-read-namespaces = [ users.fs.files , posts.fs.files ] file-namespaces = [ users.fs.files , posts.fs.files ] file-highlighting = true The above configuration tells monstache that you wish to index the raw content of GridFS files in the users and posts MongoDB databases. By default, MongoDB uses a bucket named fs , so if you just use the defaults your collection name will be fs.files . However, if you have customized the bucket name, then your file collection would be something like mybucket.files and the entire namespace would be users.mybucket.files . When you configure monstache this way it will perform an additional operation at startup to ensure the destination indexes in Elasticsearch have a field named file with a type mapping of attachment . For the example TOML configuration above, monstache would initialize 2 indices in preparation for indexing into Elasticsearch by issuing the following REST commands: For Elasticsearch versions prior to version 5... POST /users.fs.files { \"mappings\": { \"fs.files\": { \"properties\": { \"file\": { \"type\": \"attachment\" } }}}} POST /posts.fs.files { \"mappings\": { \"fs.files\": { \"properties\": { \"file\": { \"type\": \"attachment\" } }}}} For Elasticsearch version 5 and above... PUT /_ingest/pipeline/attachment { \"description\" : \"Extract file information\", \"processors\" : [ { \"attachment\" : { \"field\" : \"file\" } } ] } When a file is inserted into MongoDB via GridFS, monstache will detect the new file, use the MongoDB api to retrieve the raw content, and index a document into Elasticsearch with the raw content stored in a file field as a base64 encoded string. The Elasticsearch plugin will then extract text content from the raw content using Apache Tika , tokenize the text content, and allow you to query on the content of the file. To test this feature of monstache you can simply use the mongofiles command to quickly add a file to MongoDB via GridFS. Continuing the example above one could issue the following command to put a file named resume.docx into GridFS and after a short time this file should be searchable in Elasticsearch in the index users.fs.files . mongofiles -d users put resume.docx After a short time you should be able to query the contents of resume.docx in the users index in Elasticsearch curl -XGET \"http://localhost:9200/users.fs.files/_search?q=golang\" If you would like to see the text extracted by Apache Tika you can project the appropriate sub-field For Elasticsearch versions prior to version 5... curl -H \"Content-Type:application/json\" localhost:9200/users.fs.files/_search?pretty -d '{ \"fields\": [ \"file.content\" ], \"query\": { \"match\": { \"file.content\": \"golang\" } } }' For Elasticsearch version 5 and above... curl -H \"Content-Type:application/json\" localhost:9200/users.fs.files/_search?pretty -d '{ \"_source\": [ \"attachment.content\" ], \"query\": { \"match\": { \"attachment.content\": \"golang\" } } }' When file-highlighting is enabled you can add a highlight clause to your query For Elasticsearch versions prior to version 5... curl -H \"Content-Type:application/json\" localhost:9200/users.fs.files/_search?pretty -d '{ \"fields\": [\"file.content\"], \"query\": { \"match\": { \"file.content\": \"golang\" } }, \"highlight\": { \"fields\": { \"file.content\": { } } } }' For Elasticsearch version 5 and above... curl -H \"Content-Type:application/json\" localhost:9200/users.fs.files/_search?pretty -d '{ \"_source\": [\"attachment.content\"], \"query\": { \"match\": { \"attachment.content\": \"golang\" } }, \"highlight\": { \"fields\": { \"attachment.content\": { } } } }' The highlight response will contain emphasis on the matching terms For Elasticsearch versions prior to version 5... \"hits\" : [ { \"highlight\" : { \"file.content\" : [ \"I like to program in em golang /em .\\n\\n\" ] } } ] For Elasticsearch version 5 and above... \"hits\" : [{ \"highlight\" : { \"attachment.content\" : [ \"I like to program in em golang /em .\" ] } }]","title":"GridFS Support"},{"location":"advanced/#workers","text":"You can run multiple monstache processes and distribute the work between them. First configure the names of all the workers in a shared config.toml file. workers = [ Tom , Dick , Harry ] In this case we have 3 workers. Now we can start 3 monstache processes and give each one of the worker names. monstache -f config.toml -worker Tom monstache -f config.toml -worker Dick monstache -f config.toml -worker Harry monstache will hash the id of each document using consistent hashing so that each id is handled by only one of the available workers.","title":"Workers"},{"location":"advanced/#high-availability","text":"You can run monstache in high availability mode by starting multiple processes with the same value for cluster-name . Each process will join a cluster which works together to ensure that a monstache process is always syncing to Elasticsearch. High availability works by ensuring one active process in the monstache.cluster collection in MongoDB at any given time. Only the process in this collection will be syncing for the cluster. Processes not present in this collection will be paused. Documents in the monstache.cluster collection have a TTL assigned to them. When a document in this collection times out it will be removed from the collection by MongoDB and another process in the monstache cluster will have a chance to write to the collection and become the new active process. When cluster-name is supplied the resume feature is automatically turned on and the resume-name becomes the name of the cluster. This is to ensure that each of the processes is able to pick up syncing where the last one left off. You can combine the HA feature with the workers feature. For 3 cluster nodes with 3 workers per node you would have something like the following: // config.toml workers = [\"Tom\", \"Dick\", \"Harry\"] // on host A monstache -cluster-name HA -worker Tom -f config.toml monstache -cluster-name HA -worker Dick -f config.toml monstache -cluster-name HA -worker Harry -f config.toml // on host B monstache -cluster-name HA -worker Tom -f config.toml monstache -cluster-name HA -worker Dick -f config.toml monstache -cluster-name HA -worker Harry -f config.toml // on host C monstache -cluster-name HA -worker Tom -f config.toml monstache -cluster-name HA -worker Dick -f config.toml monstache -cluster-name HA -worker Harry -f config.toml When the clustering feature is combined with workers then the resume-name becomes the cluster name concatenated with the worker name.","title":"High Availability"},{"location":"advanced/#index-mapping","text":"When indexing documents from MongoDB into Elasticsearch the default mapping is as follows: For Elasticsearch prior to 6.2 Elasticsearch index name = MongoDB database name . MongoDB collection name Elasticsearch type = MongoDB collection name Elasticsearch document _id = MongoDB document _id For Elasticsearch 6.2+ Elasticsearch index name = MongoDB database name . MongoDB collection name Elasticsearch type = _doc Elasticsearch document _id = MongoDB document _id If these default won't work for some reason you can override the index and type mapping on a per collection basis by adding the following to your TOML config file: [[mapping]] namespace = test.test index = index1 type = type1 [[mapping]] namespace = test.test2 index = index2 type = type2 With the configuration above documents in the test.test namespace in MongoDB are indexed into the index1 index in Elasticsearch with the type1 type. If you need your index and type mapping to be more dynamic, such as based on values inside the MongoDB document, then see the sections Middleware and Routing . Warning It is not recommended to override the default type of _doc if using Elasticsearch 6.2+ since this will be the supported path going forward. Also, using _doc as the type will not work with Elasticsearch prior to 6.2. Make sure that automatic index creation is not disabled in elasticsearch.yml or create your target indexes before using Monstache. If automatic index creation must be controlled, whitelist any indexes in elasticsearch.yml that monstache will create.","title":"Index Mapping"},{"location":"advanced/#namespaces","text":"When a document is inserted, updated, or deleted in MongoDB a document is appended to the oplog representing the event. This document has a field ns which is the namespace. For inserts, updates, and deletes the namespace is the database name and collection name of the document changed joined by a dot. E.g. for use test; db.foo.insert({hello: \"world\"}); the namespace for the event in the oplog would be test.foo . In addition to inserts, updates, and deletes monstache also supports database and collection drops. When a database or collection is dropped in MongoDB an event is appended to the oplog. Like the other types of changes this event has a field ns representing the namespace. However, for drops the namespace is the database name and the string $cmd joined by a dot. E.g. for use test; db.foo.drop() the namespace for the event in the oplog would be test.$cmd .","title":"Namespaces"},{"location":"advanced/#middleware","text":"monstache supports embedding user defined middleware between MongoDB and Elasticsearch. middleware is able to transform documents, drop documents, or define indexing metadata. middleware may be written in either Javascript or in Golang as a plugin. Golang plugins require Go version 1.8 or greater on Linux. currently, you are able to use Javascript or Golang but not both (this may change in the future).","title":"Middleware"},{"location":"advanced/#golang","text":"monstache supports Golang 1.8+ plugins on Linux. To implement a plugin for monstache you simply need to implement a specific function signature, use the go command to build a .so file for your plugin, and finally pass the path to your plugin .so file when running monstache. Note If you are working with Elasticsearch 6+ and coding golang plugins for monstache you should use the master branch and your plugin should import github.com/rwynn/monstache/monstachemap . If you are working with Elasticsearch 2 or 5 and coding golang plugins for monstache you should use the rel3 branch and your plugin should import gopkg.in/rwynn/monstache.v3/monstachemap . To create a golang plugin for monstache get the necessary dependencies for your version of Elasticsearch with go get -u github.com/rwynn/monstache/monstachemap or go get -u gopkg.in/rwynn/monstache.v3/monstachemap create a .go source file that belongs to the package main import github.com/rwynn/monstache/monstachemap for Elasticsearch 6+ or gopkg.in/rwynn/monstache.v3/monstachemap for Elasticsearch 2 or 5 implement a function named Map with the following signature func Map(input *monstachemap.MapperPluginInput) (output *monstachemap.MapperPluginOutput, err error) optionally implement a function named Filter with the following signature func Filter(input *monstachemap.MapperPluginInput) (keep bool, err error) optionally implement a function named Pipeline with the following signature func Pipeline(ns string, changeStream bool) (stages []interface, err error) optionally implement a function named Process with the following signature func Process(input*monstachemap.ProcessPluginInput) error plugins can be compiled using go build -buildmode=plugin -o myplugin.so myplugin.go to enable the plugin, start monstache with monstache -mapper-plugin-path /path/to/myplugin.so the following example plugin simply converts top level string values to uppercase package main import ( github.com/rwynn/monstache/monstachemap // or gopkg.in/rwynn/monstache.v3/monstachemap for Elasticsearch 2 or 5 strings ) // a plugin to convert document values to uppercase func Map(input *monstachemap.MapperPluginInput) (output *monstachemap.MapperPluginOutput, err error) { doc := input.Document for k, v := range doc { switch v.(type) { case string: doc[k] = strings.ToUpper(v.(string)) } } output = monstachemap.MapperPluginOutput{Document: doc} return } The input parameter will contain information about the document's origin database and collection: field meaning Document MongoDB document updated, inserted or deleted Namespace Operation namespace as described above Database MongoDB database from where the event came Collection MongoDB collection where the document was inserted, deleted or updated Operation Which kind of operation triggered this event, see gtm.mapOperation() . \"i\" for insert, \"u\" for update, \"d\" for delete and \"c\" for invalidate. The Map function will only receive inserts and updates. To handle deletes or invalidates implement the Process function described below. Session *mgo.Session . You need not Close the session as monstache will do this automatically when the function exits The output parameter will contain information about how the document should be treated by monstache: field meaning Document an updated document to index into Elasticsearch Index the name of the index to use Type the document type Routing the routing value to use Drop set to true to indicate that the document should not be indexed but removed Passthrough set to true to indicate the original document should be indexed unchanged Parent the parent id to use Version the version of the document VersionType the version type of the document (internal, external, external_gte) Pipeline the pipeline to index with RetryOnConflict how many times to retry updates before failing Skip set to true to indicate the the document should be ignored For detailed information see monstachemap/plugin.go Few examples are: To skip the document (direct monstache to ignore it) set output.Skip = true . To drop the document (direct monstache not to index it but remove it) set output.Drop = true . To simply pass the original document through to Elasticsearch, set output.Passthrough = true To set custom indexing metadata on the document use output.Index , output.Type , output.Parent and output.Routing . Note If you override output.Parent or output.Routing for any MongoDB namespaces in a golang plugin you should also add those namespaces to the routing-namespaces array in your config file. This instructs Monstache to save the routing information so that deletes of the document work correctly. If would like to embed other MongoDB documents (possibly from a different collection) within the current document before indexing, you can access the *mgo.Session pointer as input.Session . With the mgo session you can use the mgo API to find documents in MongoDB and embed them in the Document set on output. When you implement a Filter function the function is called immediately after reading inserts and updates from the oplog. You can return false from this function to completely ignore a document. This is different than setting output.Drop from the mapping function because when you set output.Drop to true, a delete request is issued to Elasticsearch in case the document had previously been indexed. By contrast, returning false from the Filter function causes the operation to be completely ignored and there is no corresponding delete request issued to Elasticsearch. When you implement a Pipeline function the function will be called to setup an aggregation pipeline for both direct reads and any change streams that you have configured. The aggregation pipeline stages that you return may be different depending if applied to a direct read or to a change stream. For direct reads the root document will be the document in the collection. For change streams the root document will be a change event with a fullDocument field inside it. Use the boolean parameter changeStream to alter the stages that you return from this function accordingly. When you implement a Process function the function will be called after monstache processes an event. This function has full access to the MongoDB and Elasticsearch clients (including the Elasticsearch bulk processor) in the input and allows you to handle complex event processing scenarios. The input parameter for the Process function will have all the same fields as the input to a Map function described above plus the following: field meaning ElasticClient A full featured Elasticsearch client ElasticBulkProcessor The same bulk processor monstache uses to index documents. You need only Add requests to the processor and they will be flushed in bulk automatically Timestamp The MongoDB timestamp of the change event from the oplog. In the case of direct reads the timestamp is the time at which the document was read from MongoDB. Note Under the docker/plugin folder there is a build.sh script to help you build a plugin. There is a README file in that directory with instructions.","title":"Golang"},{"location":"advanced/#javascript","text":"","title":"Javascript"},{"location":"advanced/#transformation","text":"Monstache uses the amazing otto library to provide transformation at the document field level in Javascript. You can associate one javascript mapping function per MongoDB collection. You can also associate a function at the global level by not specifying a namespace. These javascript functions are added to your TOML config file, for example: [[script]] namespace = mydb.mycollection script = var counter = 1; module.exports = function(doc) { doc.foo += test + counter; counter++; return doc; } [[script]] namespace = anotherdb.anothercollection path = path/to/transform.js routing = true [[script]] # this script does not declare a namespace # it is global to all collections script = module.exports = function(doc, ns) { // the doc namespace e.g. test.test is passed as the 2nd arg return _.omit(doc, password , secret ); } The example TOML above configures 3 scripts. The first is applied to mycollection in mydb while the second is applied to anothercollection in anotherdb . The first script is inlined while the second is loaded from a file path. The path can be absolute or relative to the directory monstache is executed from. The last script does not specify a namespace, so documents from all collections pass through it. Global scripts are run before scripts which are linked to a specific namespace. You will notice that the multi-line string feature of TOML is used to assign a javascript snippet to the variable named script . The javascript assigned to script must assign a function to the exports property of the module object. This function will be passed the document from MongoDB just before it is indexed in Elasticsearch. Inside the function you can manipulate the document to drop fields, add fields, or augment the existing fields. The this reference in the mapping function is assigned to the document from MongoDB. When the return value from the mapping function is an object then that mapped object is what actually gets indexed in Elasticsearch. For these purposes an object is a javascript non-primitive, excluding Function , Array , String , Number , Boolean , Date , Error and RegExp .","title":"Transformation"},{"location":"advanced/#filtering","text":"You can completely ignore documents by adding filter configurations to your TOML config file. The filter functions are executing immediately after inserts or updates are read from the oplog. The correspding document is passed into the function and you can return true or false to include or ignore the document. [[filter]] namespace = db.collection script = module.exports = function(doc) { return !!doc.interesting; } [[filter]] namespace = db2.collection2 path = path/to/script.js","title":"Filtering"},{"location":"advanced/#aggregation-pipelines","text":"You can alter or filter direct reads and change streams by using a pipeline definition. Note, when building a pipeline for a change stream the root of the document will be the change event and the associated document will be under a field named fullDocument . For more information on the properties of the root document for change streams see Change Events . You can scope a pipeline to a particular namespace using the namespace attribute or leave it off to have the pipeline applied to all namespaces. [[pipeline]] script = module.exports = function(ns, changeStream) { if (changeStream) { return [ { $match: { fullDocument.foo : 1} } ]; } else { return [ { $match: { foo : 1} } ]; } } Warning You should not replace the root using $replaceRoot for a change stream since monstache needs this information. You should only make modifications to the fullDocument field in a pipeline.","title":"Aggregation Pipelines"},{"location":"advanced/#dropping","text":"If the return value from the mapping function is not an object per the definition above then the result is converted into a boolean and if the boolean value is false then that indicates to monstache that you would not like to index the document. If the boolean value is true then the original document from MongoDB gets indexed in Elasticsearch. This allows you to return false or null if you have implemented soft deletes in MongoDB. [[script]] namespace = db.collection script = module.exports = function(doc) { if (!!doc.deletedAt) { return false; } return true; } In the above example monstache will index any document except the ones with a deletedAt property. If the document is first inserted without a deletedAt property, but later updated to include the deletedAt property then monstache will remove, or drop, the previously indexed document from the Elasticsearch index. Note Dropping a document is different that filtering a document. A filtered document is completely ignored. A dropped document results in a delete request being issued to Elasticsearch in case the document had previously been indexed.","title":"Dropping"},{"location":"advanced/#scripting-features","text":"You may have noticed that in the first example above the exported mapping function closes over a var named counter . You can use closures to maintain state between invocations of your mapping function. Finally, since Otto makes it so easy, the venerable Underscore library is included for you at no extra charge. Feel free to abuse the power of the _ .","title":"Scripting Features"},{"location":"advanced/#embedding-documents","text":"In your javascript function you have access to the following global functions to retreive documents from MongoDB for embedding in the current document before indexing. Using this approach you can pull in related data. function findId(documentId, [options]) { // convenience method for findOne({_id: documentId}) // returns 1 document or null } function findOne(query, [options]) { // returns 1 document or null } function find(query, [options]) { // returns an array of documents or null } function pipe(stages, [options]) { // returns an array of documents or null } Each function takes a query type object parameter and an optional options object parameter. The options object takes the following keys and values: var options = { database: test , collection: test , // to omit _id set the _id key to 0 in select select: { age: 1 }, // only applicable to find... sort: [ name ], limit: 2 } If the database or collection keys are omitted from the options object, the values for database and/or collection are set to the database and collection of the document being processed. Here are some examples: This example sorts the documents in the same collection as the document being processed by name and returns the first 2 documents projecting only the age field. The result is set on the current document before being indexed. [[script]] namespace = test.test script = module.exports = function(doc) { doc.twoAgesSortedByName = find({}, { sort: [ name ], limit: 2, select: { age: 1 } }); return doc; } This example grabs a reference id from a document and replaces it with the corresponding document with that id. [[script]] namespace = test.posts script = module.exports = function(post) { if (post.author) { // author is a an object id reference post.author = findId(post.author, { database: test , collection: users }); } return post; } This example runs an aggregation pipeline and stores the results in an extra field in the document [[script]] namespace = test.test script = module.exports = function(doc, ns) { doc.extra = pipe([ { $match: {foo: 1} }, { $limit: 1 }, { $project: { _id: 0, foo: 1}} ] // optional , { database: foo , collection: bar } // defaults to same namespace ); return doc; }","title":"Embedding Documents"},{"location":"advanced/#indexing-metadata","text":"You can override the indexing metadata for an individual document by setting a special field named _meta_monstache on the document you return from your Javascript function. Assume there is a collection in MongoDB named company in the test database. The documents in this collection look like either { _id : london , type : branch , name : London Westminster , city : London , country : UK } or { _id : alice , type : employee , name : Alice Smith , branch : london } Given the above the following snippet sets up a parent-child relationship in Elasticsearch based on the incoming documents from MongoDB and updates the ns (namespace) from test.company to company in Elasticsearch [[script]] namespace = test.company routing = true script = module.exports = function(doc, ns) { // var meta = { type: doc.type, index: 'company' }; var meta = { type: doc.type, index: ns.split( . )[1] }; if (doc.type === employee ) { meta.parent = doc.branch; } doc._meta_monstache = meta; return _.omit(doc, branch , type ); } The snippet above will route these documents to the company index in Elasticsearch instead of the default of test.company , if you didn't specify a namespace, it'll route all documents to indexes named as the collection only without the database db . collection (MongoDB) = collection (Elasticsearch). Also, instead of using company as the Elasticsearch type, the type attribute from the document will be used as the Elasticsearch type. Finally, if the type is employee then the document will be indexed as a child of the branch the person belongs to. We can throw away the type and branch information by deleting it from the document before returning since the type information will be stored in Elasticsearch under _type and the branch information will be stored under _parent . The example is based on the Elasticsearch docs for parent-child For more on updating the namespace name, check the Delete Strategy as there is a change of the default behviour in v4.4.0 v3.11.0","title":"Indexing Metadata"},{"location":"advanced/#routing","text":"Routing is the process by which Elasticsearch determines which shard a document will reside in. Monstache supports user defined, or custom, routing of your MongoDB documents into Elasticsearch. Consider an example where you have a comments collection in MongoDB which stores a comment and its associated post identifier. use blog; db.comments.insert({title: Did you read this? , post_id: 123 }); db.comments.insert({title: Yeah, it's good , post_id: 123 }); In this case monstache will index those 2 documents in an index named blog.comments under the id created by MongoDB. When Elasticsearch routes a document to a shard, by default, it does so by hashing the id of the document. This means that as the number of comments on post 123 grows, each of the comments will be distributed somewhat evenly between the available shards in the cluster. Thus, when a query is performed searching among the comments for post 123 Elasticsearch will need to query all of those shards just in case a comment happened to have been routed there. We can take advantage of the support in Elasticsearch and in monstache to do some intelligent routing such that all comments for post 123 reside in the same shard. First we need to tell monstache that we would like to do custom routing for this collection by setting routing equal to true on a custom script for the namespace. Then we need to add some metadata to the document telling monstache how to route the document when indexing. In this case we want to route by the post_id field. [[script]] namespace = blog.comments routing = true script = module.exports = function(doc) { doc._meta_monstache = { routing: doc.post_id }; return doc; } Now when monstache indexes document for the collection blog.comments it will set the special _routing attribute for the document on the index request such that Elasticsearch routes comments based on their corresponding post. The _meta_monstache field is used only to inform monstache about routing and is not included in the source document when indexing to Elasticsearch. Now when we are searching for comments and we know the post id that the comment belongs to we can include that post id in the request and make a search that normally queries all shards query only 1 shard. $ curl -H Content-Type:application/json -XGET 'http://localhost:9200/blog.comments/_search?routing=123' -d ' { query :{ match_all :{} } }' You will notice in the response that only 1 shard was queried instead of all your shards. Custom routing is great way to reduce broadcast searches and thus get better performance. The catch with custom routing is that you need to include the routing parameter on all insert, update, and delete operations. Insert and update is not a problem for monstache because the routing information will come from your MongoDB document. Deletes, however, pose a problem for monstache because when a delete occurs in MongoDB the information in the oplog is limited to the id of the document that was deleted. But monstache needs to know where the document was originally routed in order to tell Elasticsearch where to look for it. Monstache has 3 available strategies for handling deletes in this situation. The default strategy is stateless and uses a term query into Elasticsearch based on the ID of the document deleted in MongoDB. If the search into Elasticsearch returns exactly 1 document then monstache will schedule that document for deletion. The 2nd stategy monstache uses is stateful and requires giving monstache the ability to write to the collection monstache.meta . In this collection monstache stores information about documents that were given custom indexing metadata. This stategy slows down indexing and takes up space in MongoDB. However, it is precise because it records exactly how each document was indexed. The final stategy simply punts on deletes and leaves document deletion to the user. If you don't generally delete documents in MongoDB or don't care if Elasticsearch contains documents which have been deleted in MongoDB, this option is available. See Delete Strategy for more information. For more information see Customizing Document Routing In addition to letting your customize the shard routing for a specific document, you can also customize the Elasticsearch index and type using a script by putting the custom information in the meta attribute. [[script]] namespace = blog.comments routing = true script = module.exports = function(doc) { if (doc.score = 100) { // NOTE: prefix dynamic index with namespace for proper cleanup on drops doc._meta_monstache = { index: blog.comments.highscore , type: highScoreComment , routing: doc.post_id }; } else { doc._meta_monstache = { routing: doc.post_id }; } return doc; }","title":"Routing"},{"location":"advanced/#joins","text":"Elasticsearch 6 introduces an updated approach to parent-child called joins. The following example shows how you can accomplish joins with Monstache. The example is based on the Elasticsearch documentation . This example assumes Monstache is syncing the test.test collection in MongoDB with the test.test index in Elasticsearch. First we will want to setup an index mapping in Elasticsearch describing the join field. curl -XPUT 'localhost:9200/test.test?pretty' -H 'Content-Type: application/json' -d' { mappings : { _doc : { properties : { my_join_field : { type : join , relations : { question : answer } } } } } } ' Warning The above mapping uses _doc as the Elasticsearch type. _doc is the recommended type for new versions Elasticsearch but it only works with Elasticsearch versions 6.2 and greater. Monstache defaults to using _doc as the type when it detects Elasticsearch version 6.2 or greater. If you are using a previous version of Elasticsearch monstache defaults to using the MongoDB collection name as the Elasticsearch type. The type Monstache uses can be overriden but it is not recommended from Elasticsearch 6.2 on. Next will will configure Monstache with custom Javascript middleware that does transformation and routing. In a file called CONFIG.toml. [[script]] namespace = test.test routing = true script = module.exports = function(doc) { var routing; if (doc.type === question ) { routing = doc._id; doc.my_join_field = { name: question } } else if (doc.type === answer ) { routing = doc.question; doc.my_join_field = { name: answer , parent: routing }; } if (routing) { doc._meta_monstache = { routing: routing }; } return doc; } The mapping function adds a my_join_field field to each document. The contents of the field are based on the type attribute in the MongoDB document. Also, the function ensures that the routing is always based on the _id of the question document. Now with this config in place we can start Monstache. We will use verbose to see the requests. monstache -verbose -f CONFIG.toml With Monstache running we are now ready to insert into MongoDB rs:PRIMARY use test; switched to db test rs:PRIMARY db.test.insert({type: question , text: This is a question }); rs:PRIMARY db.test.find() { _id : ObjectId( 5a84a8b826993bde57c12893 ), type : question , text : This is a question } rs:PRIMARY db.test.insert({type: answer , text: This is an answer , question: ObjectId( 5a84a8b826993bde57c12893 ) }); When we insert these documents we should see Monstache generate the following requests to Elasticsearch { index :{ _id : 5a84a8b826993bde57c12893 , _index : test.test , _type : _doc , routing : 5a84a8b826993bde57c12893 , version :6522523668566769665, version_type : external }} { my_join_field :{ name : question }, text : This is a question , type : question } { index :{ _id : 5a84a92b26993bde57c12894 , _index : test.test , _type : _doc , routing : 5a84a8b826993bde57c12893 , version :6522524162488008705, version_type : external }} { my_join_field :{ name : answer , parent : 5a84a8b826993bde57c12893 }, question : 5a84a8b826993bde57c12893 , text : This is an answer , type : answer } This looks good. We should now have a parent/child relationship between these documents in Elasticsearch. If we do a search on the test.test index we see the following results: hits : { total : 2, max_score : 1.0, hits : [ { _index : test.test , _type : _doc , _id : 5a84a8b826993bde57c12893 , _score : 1.0, _routing : 5a84a8b826993bde57c12893 , _source : { my_join_field : { name : question }, text : This is a question , type : question } }, { _index : test.test , _type : _doc , _id : 5a84a92b26993bde57c12894 , _score : 1.0, _routing : 5a84a8b826993bde57c12893 , _source : { my_join_field : { name : answer , parent : 5a84a8b826993bde57c12893 }, question : 5a84a8b826993bde57c12893 , text : This is an answer , type : answer } } ] } To clean up our documents in Elasticsearch a bit we can omit the information that we don't really need in the source docs by updating our mapping function. This information needs not be at the top-level since it is duplicated in my_join_field . return _.omit(doc, type , question ); If your parent and child documents are in separate MongoDB collections then you would set up a script for each collection. You can tell if the doc is a parent or child by the collection it comes from. The only other difference would be that you would need to override the index dynamically in addition to the routing such that documents from both MongoDB collections target the same index. doc._meta_monstache = { routing: routing, index: parentsAndChildren }; Warning You must be careful when you route 2 or more MongoDB collections to the same Elasticsearch index that the document _ids across the MongoDB collections do not collide for any 2 docs because they will be used as the _id in the target index.","title":"Joins"},{"location":"advanced/#time-machines","text":"If you are not just interested in what the current value of a document in MongoDB is, but also would like to see how it has changed over time use time machine namespaces . For example, you've inserted and later updated a document with id 123 in the test.test collection in MongoDB. If test.test is a time machine namespace you will have 2 documents representing those changes in the log.test.test.2018-02-20 index (timestamp will change) in Elasticsearch. If you later want all the changes made to that document in MongoDB you can issue a query like this: $ curl -XGET 'http://localhost:9200/log.test.test.*/_search?routing=123' -d ' { query :{ sort : [ { _oplog_ts : { order : desc }} ], filtered :{ query :{ match_all :{} }, filter :{ term :{ _source_id : 123 } } } } }' That query will be very efficient because it only queries the shard that all the change docs went to for MongoDB document id 123. It filters the documents on that shard by _source_id , or id from MongoDB, to only give us the changes to that document. Finally, it sorts by the _oplog_ts which gives us the most recent change docs first. The index pattern in the query is a wildcard to pick up all the timestamped indexes that we've acculated for the test.test namespace.","title":"Time Machines"},{"location":"advanced/#merge-patches","text":"A unique feature of monstache is support for JSON Merge Patches rfc-7396 . If merge patches are enabled monstache will add an additional field to documents indexed into Elasticsearch. The name of this field is configurable but it defaults to json-merge-patches . Consider the following example with merge patches enabled... db.test.insert({name: Joe , age: 16, friends: [1, 2, 3]}) At this point you would have the following document source in Elasticsearch. \"_source\" : { \"age\" : 16, \"friends\" : [ 1, 2, 3 ], \"json-merge-patches\" : [ { \"p\" : \"{\\\"age\\\":16,\\\"friends\\\":[1,2,3],\\\"name\\\":\\\"Joe\\\"}\", \"ts\" : 1487263414, \"v\" : 1 } ], \"name\" : \"Joe\" } As you can see we have a single timestamped merge patch in the json-merge-patches array. Now let's update the document to remove a friend and update the age. db.test.update({name: Joe }, {$set: {age: 21, friends: [1, 3]}}) If we now look at the document in Elasticsearch we see the following: \"_source\" : { \"age\" : 21, \"friends\" : [ 1, 3 ], \"json-merge-patches\" : [ { \"p\" : \"{\\\"age\\\":16,\\\"friends\\\":[1,2,3],\\\"name\\\":\\\"Joe\\\"}\", \"ts\" : 1487263414, \"v\" : 1 }, { \"p\" : \"{\\\"age\\\":21,\\\"friends\\\":[1,3]}\", \"ts\" : 1487263746, \"v\" : 2 } ], \"name\" : \"Joe\" } You can see that the document was updated as expected and an additional merge patch was added. Each time the document is updated in MongoDB the corresponding document in Elasticsearch gains a timestamped merge patch. Using this information we can time travel is the document's history. There is a merge patch for each version of the document. To recreate a specific version we simply need to apply the merge patches in order up to the version that we want. To get version 1 of the document above we start with {} and apply the 1st merge patch. To get version 2 of the document above we start with {} apply the 1st merge patch to get v1 apply the 2nd merge patch to v1 to get v2 The timestamps associated with these merge patches are in seconds since the epoch, taken from the timestamp recorded in the oplog when the insert or update occured. To enable the merge patches feature in monstache you need to add the following to you TOML config: enable-patches = true patch-namespaces = [\"test.test\"] You need you add each namespace that you would like to see patches for in the patch-namespaces array. Optionally, you can change the key under which the patches are stored in the source document as follows: merge-patch-attribute = \"custom-merge-attr\" Merge patches will only be recorded for data read from the MongoDB oplog. Data read using the direct read feature will not be enhanced with merge patches. Most likely, you will want to turn off indexing for the merge patch attribute. You can do this by creating an index template for each patch namespace before running monstache... PUT /_template/test.test { \"template\" : \"test.test\", \"mappings\" : { \"test\" : { \"json-merge-patches\" : { \"index\" : false } } } }","title":"Merge Patches"},{"location":"advanced/#systemd","text":"Monstache has support built in for integrating with systemd. The following monstache.service is an example systemd configuration. [Unit] Description=monstache sync service [Service] Type=notify ExecStart=/usr/local/bin/monstache -f /etc/monstache/config.toml WatchdogSec=30s Restart=always [Install] WantedBy=multi-user.target Systemd unit files are normally saved to /lib/systemd/system . Verify same with your OS documentation. After saving the monstache.service file you can run systemctl daemon-reload to tell systemd to reload all unit files. You can enable the service to start on boot with systemctl enable monstache.service and start the service with systemctl start monstache.service . With the configuration above monstache will notify systemd when it has started successfully and then notify systemd repeatedly at half the WatchDog interval to signal liveness. The configuration above causes systemd to restart monstache if it does not start or respond within the WatchdDog interval.","title":"Systemd"},{"location":"advanced/#docker","text":"There are Docker images available for Monstache on Docker Hub You can pull and run the latest images with # for Elasticsearch = 6 docker run rwynn/monstache:latest -v # for Elasticsearch 6 docker run rwynn/monstache:rel3 -v You can pull and run release images with docker run rwynn/monstache:4.11.4 -v docker run rwynn/monstache:3.18.4 -v For example, to run monstache via Docker with a golang plugin that resides at ~/plugin/plugin.so on the host you can use a bind mount docker run --rm --net=host -v ~/plugin:/tmp/plugin rwynn/monstache:latest -mapper-plugin-path /tmp/plugin/plugin.so","title":"Docker"},{"location":"advanced/#http-server","text":"Monstache has a built in HTTP server that you can enable with --enable-http-server. It listens on :8080 by default but you can change this with --http-server-addr. When using monstache with kubernetes this server can be used to detect liveness and act accordingly The following GET endpoints are available","title":"HTTP Server"},{"location":"advanced/#started","text":"Returns the uptime of the server","title":"/started"},{"location":"advanced/#healthz","text":"Returns at 200 status code with the text \"ok\" when monstache is running","title":"/healthz"},{"location":"advanced/#stats","text":"Returns the current indexing statistics in JSON format. Only available if stats are enabled","title":"/stats"},{"location":"advanced/#debug-if-pprof-is-enabled","text":"If the pprof setting is enabled the following endpoints are also made available: /debug/pprof/ /debug/pprof/cmdline /debug/pprof/profile /debug/pprof/symbol /debug/pprof/trace","title":"/debug (if pprof is enabled)"},{"location":"advanced/#aws-signature-version-4","text":"Monstache has included AWS Signature Version 4 request signing for testing. This is in its initial stages and needs testing and feedback. To enable the experimental AWS Signature Version 4 support add the following to your config file: [aws-connect] access-key = XXX secret-key = YYY region = ZZZ You can read more about Signature Version 4 and Amazon Elasticsearch Service .","title":"AWS Signature Version 4"},{"location":"advanced/#mongodb-view-replication","text":"You may have a situation where you want to replicate a MongoDB view in Elasticsearch. Or you have a collection that should trigger sync of another collection. You can use the relate config to do this. Consider you have a collections thing and state . A thing has an associated state and a thing is linked to a state via a field s which points to the _id of the associated state in the state collection. You can create a view in MongoDB that uses a $lookup to pull the state information in and present a view of things with the state information included. use thingdb; db.createView( thingview , thing , [ {$lookup: {from: state , localField: s , foreignField: _id , as: s }}]) Given this view you can use the following config to keep things up to date in a things index in Elasticsearch. direct-read-namespaces = [ thingdb.thingview ] # read direct from the view of the collection to seed index change-stream-namespaces = [ thingdb.thing , thingdb.state ] # change events happen on the underlying collections not views [[mapping]] namespace = thingdb.thing # map change events on the thing collection to the things index index = things [[mapping]] namespace = thingdb.thingview # map direct reads of the thingview to the same things index index = things [[relate]] namespace = thingdb.thing # when a thing changes look it up in the assoicated view by _id and index that with-namespace = thingdb.thingview keep-src = false # ignore the original thing that changed and instead just use the lookup of that thing via the view [[relate]] namespace = thingdb.states # when a state changes trigger a thing change event since thing is associated to a state with-namespace = thingdb.thing src-field = _id # use the _id field of the state that changed to lookup associated things match-field = s # only trigger change events for the things where thing.s (match-field) = state._id (src-field). keep-src = false Warning Be careful of the expense of using relate with a view. In the example above, if there were many things associated to a single state then a change to that state would trigger n+1 queries to MongoDB when n is the number of things related to the state. 1 query would be used to find all associated things and n queries would be used to lookup each thing in the view.","title":"MongoDB view replication"},{"location":"config/","text":"Configuration Configuration can be specified in your TOML config file or passed into monstache as Go program arguments on the command line. Program arguments take precedance over configuration specified in the TOML config file. Warning Please keep any simple -one line config- above any [[script]] or toml table configs, as there is a bug in the toml parser where if you have definitions below a TOML table, e.g. a [[script]] then the parser thinks that lines below that belong to the table instead of at the global level print-config boolean (default false ) When print-config is true monstache will print its configuration and then exit stats boolean (default false ) When stats is true monstache will periodically print statistics accumulated by the indexer enable-easy-json boolean (default false ) When enable-easy-json is true monstache will the easy-json library to serialize requests to Elasticsearch stats-duration string (default 30s ) Sets the duration after which statistics are printed if stats is enabled index-stats boolean (default false ) When both stats and index-stats are true monstache will write statistics about its indexing progress in Elasticsearch instead of standard out. The indexes used to store the statistics are time stamped by day and prefixed monstache.stats. . E.g. monstache.stats.2017-07-01 and so on. As these indexes will accrue over time your can use a tool like curator to prune them with a Delete Indices action and an age filter. index-as-update boolean (default false ) When index-as-update is set to true monstache will sync create and update operations in MongoDB as updates to Elasticsearch. By default, monstache will overwrite the entire document in Elasticsearch. This setting may be useful if you make updates to Elasticsearch to the documents monstache has previously synced and would like to retain these updates when the document changes in MongoDB. You will only be able to retain fields in Elasticsearch that do not overlap with fields in MongoDB. stats-index-format string (default monstache.stats.2006-01-02 ) The time.Time supported index name format for stats indices. By default, stats indexes are partitioned by day. To use less indices for stats you can shorten this format string (e.g monstache.stats.2006-01) or remove the time component completely to use a single index. gzip boolean (default false ) When gzip is true, monstache will compress requests to Elasticsearch. If you enable gzip in monstache and are using Elasticsearch prior to version 5 you will also need to update the Elasticsearch config file to set http.compression: true. In Elasticsearch version 5 and above http.compression is enabled by default. Enabling gzip compression is recommended if you enable the index-files setting. fail-fast boolean (default false ) When fail-fast is true, if monstache receives a failed bulk indexing response from Elasticsearch, monstache will log the request that produced the response as an ERROR and then exit immediately with an error status. Normally, monstache just logs the error and continues processing events. If monstache has been configured with elasticsearch-retry true, a failed request will be retried before being considered a failure. prune-invalid-json boolean (default false ) If your MongoDB data contains values like +Infinity, -Infinity, NaN, or invalid dates you will want to set this option to true. The Golang json serializer is not able to handle these values and the indexer will get stuck in an infinite loop. When prune-invalid-json is set to true Monstache will drop those fields so that indexing errors do not occur. index-oplog-time boolean (default false ) If this option is set to true monstache will include 2 automatic fields in the source document indexed into Elasticsearch. The first is oplog_ts which is the timestamp for the event copied directly from the MongoDB oplog. The second is oplog_date which is an Elasticsearch date field corresponding to the time of the same event. This information is generally useful in Elasticsearch giving the notion of last updated. However, it's also valuable information to have for failed indexing requests since it gives one the information to replay from a failure point. See the option resume-from-timestamp for information on how to replay oplog events since a given event occurred. For data read via the direct read feature the oplog time will only be available if the id of the MongoDB document is an ObjectID. If the id of the MongoDB document is not an ObjectID and the document source is a direct read query then the oplog time will not be available. oplog-ts-field-name string (default oplog_ts ) Use this option to override the name of the field used to store the oplog timestamp oplog-date-field-name string (default oplog_date ) Use this option to override the name of the field used to store the oplog date string oplog-date-field-format string (default 2006/01/02 15:04:05 ) Use this option to override the layout for formatting the oplog_date field. Refer to the Format function for the reference time values to use in the layout. resume boolean (default false ) When resume is true, monstache writes the timestamp of MongoDB operations it has successfully synced to Elasticsearch to the collection monstache.monstache. It also reads that timestamp from that collection when it starts in order to replay events which it might have missed because monstache was stopped. If monstache is started with the cluster-name option set then resume is automatically turned on. resume-name string (default default ) monstache uses the value of resume-name as an id when storing and retrieving timestamps to and from the MongoDB collection monstache.monstache. The default value for this option is the string default . However, there are some exceptions. If monstache is started with the cluster-name option set then the name of the cluster becomes the resume-name. This is to ensure that any process in the cluster is able to resume from the last timestamp successfully processed. The other exception occurs when resume-name is not given but worker-name is. In that case the worker name becomes the resume-name. resume-from-timestamp int64 (default 0 ) When resume-from-timestamp (a 64 bit timestamp where the high 32 bytes represent the number of seconds since epoch and the low 32 bits represent an offset within a second) is given, monstache will sync events starting immediately after the timestamp. This is useful if you have a specific timestamp from the oplog and would like to start syncing from after this event. replay boolean (default false ) When replay is true, monstache replays all events from the beginning of the MongoDB oplog and syncs them to Elasticsearch. If you've previously synced Monstache to Elasticsearch you may see many WARN statments in the log indicating that there was a version conflict. This is normal during a replay and it just means that you already have data in Elasticsearch that is newer than the point in time data from the oplog. When resume and replay are both true, monstache replays all events from the beginning of the MongoDB oplog, syncs them to Elasticsearch and also writes the timestamps of processed events to monstache.monstache. When neither resume nor replay are true, monstache reads the last timestamp in the oplog and starts listening for events occurring after this timestamp (tails starting at the end). Timestamps are not written to monstache.monstache. This is the default behavior. resume-write-unsafe boolean (default false ) When resume-write-unsafe is true monstache sets the safety mode of the MongoDB session such that writes are fire and forget. This speeds up writing of timestamps used to resume synching in a subsequent run of monstache. This speed up comes at the cost of no error checking on the write of the timestamp. Since errors writing the last synched timestamp are only logged by monstache and do not stop execution it's not unreasonable to set this to true to get a speedup. time-machine-namespaces []string (default nil ) Monstache is good at keeping your MongoDB collections and Elasticsearch indexes in sync. When a document is updated in MongoDB the corresponding document in Elasticsearch is updated too. Same goes for deleting documents in MongoDB. But what if you also wanted to keep a log of all the changes to a MongoDB document over its lifespan. That's what time-machine-namespaces are for. When you configure a list of namespaces in MongoDB to add to the time machine, in addition to keeping documents in sync, Monstache will index of copy of your MongoDB document at the time it changes in a separate timestamped index. Say for example, you insert a document into the test.test collection in MongoDB. Monstache will index by default into the test.test index in Elasticsearch, but with time machines it will also index it into log.test.test.2018-02-19 . When it indexes it into the time machine index it does so without the id from MongoDB and lets Elasticsearch generate a unique id. But, it stores the id from MongoDB in the source field _source_id . Also, it adds _oplog_ts and _oplog_date fields on the source document. These correspond to the timestamp from the oplog when the data changed in MongoDB. Finally, it routes the document by the MongoDB id so that you can speed up queries later to find changes to a doc. This lets you do some cool things but mostly you'll want to sort by _oplog_date and filter by _source_id to see how documents have changed over time. Because the indexes are timestamped you can drop then after a period of time so they don't take up space. If you just want the last couple of days of changes, delete the indexes with the old timestamps. Elastic curator is your friend here. This option may be passed on the command line as ./monstache --time-machine-namespace test.foo --time-machine-namespace test.bar time-machine-index-prefix string (default log ) If you have enabled time machine namespaces and want to change the prefix assigned to the index names use this setting. time-machine-index-suffix string (default 2006-01-02 ) If you have enabled time machine namespaces and want to suffix the index names using a different date format use this setting. Consult the golang docs for how date formats work. By default this suffixes the index name with the year, month, and day. time-machine-direct-reads boolean (default false ) This setting controls whether or not direct reads are added to the time machine log index. This is false by default so only changes read from the oplog are added. routing-namespaces []string (default nil ) You only need to set this configuration option if you use golang and javascript plugins are do custom routing: override parent or routing attributes. This array should be set to a list of all the namespaces that custom routing is done on. This ensures that deletes in MongoDB are routed correctly to Elasticsearch. delete-strategy int (default 0 ) The strategy to use for handling document deletes when custom indexing is done in scripts. Warning Breaking change in versions v4.4.0 v3.11.0 . Monstache was saving routing foreach document in MongoDB in a db called monstache collection meta using the same MongoDB URL and credentials provided in config by default. Monstache was only saving this information if the document metadata was being altered via _monstache_meta in a script or via the API in a golang plugin. Monstache needed to save this information in order to locate and perform deletes in Elasticsearch if the corresponding document was deleted in MongoDB. If you want to maintain this stategy use value 1. Otherwise, you can drop the monstache.meta collection as this is no longer used by default. But now the default has changed to be stateless, you can read more: discussion commit Strategy 0 -default- will do a term query by document id across all Elasticsearch indexes. Will only perform the delete if one single document is returned by the query. Stategy 1 will store indexing metadata in MongoDB in the monstache.meta collection and use this metadata to locate and delete the document. Stategy 2 will completely ignore document deletes in MongoDB. delete-index-pattern string (default * ) When using a stateless delete strategy, set this to a valid Elasticsearch index pattern to restrict the scope of possible indexes that a stateless delete will consider. If monstache only indexes to index a, b, and c then you can set this to a,b,c . If monstache only indexes to indexes starting with mydb then you can set this to mydb* . change-stream-namespaces []string (default `nil') This option allows you to opt in to using MongoDB change streams. The namespaces included here will be tailed using $watch function. This options requires MongoDB version 3.6 and above. When this option is enabled the legacy direct tailing of the oplog is disabled, therefore you do not need to specify additional regular expressions to filter the set of collections to watch. direct-read-namespaces []string (default nil ) This option allows you to directly copy collections from MongoDB to Elasticsearch. Monstache allows filtering the data that is actually indexed to Elasticsearch, so you need not necessarily copy the entire collection. Since the oplog is a capped collection it may only contain a subset of all your data. In this case you can perform a direct sync of Mongodb to Elasticsearch. To do this, set direct-read-namespaces to an array of namespaces that you would like to copy. Monstache will perform reads directly from the given set of db.collection and sync them to Elasticsearch. This option may be passed on the command line as ./monstache --direct-read-namespace test.foo --direct-read-namespace test.bar By default, Monstache maps a MongoDB collection named foo in a database named test to the test.foo index in Elasticsearch. For maximum indexing performance when doing alot of a direct reads you will want to adjust the refresh interval during indexing on the destination Elasticsearch indices. The refresh interval can be set at a global level in elasticsearch.yml or on a per index basis by using the Index Settings or Index Template APIs. For more information see Update Indices Settings . By default, Elasticsearch refreshes every second. You will want to increase this value or turn off refresh completely during the indexing phase by setting the refresh_interval to -1. Remember to reset the refresh_interval to a positive value and do a force merge after the indexing phase has completed if you decide to temporarily turn off refresh, otherwise you will not be able to see the new documents in queries. direct-read-split-max int (default 9 ) The maximum number of times to split a collection for direct reads. This setting greatly impacts the memory consumption of Monstache. When direct reads are performed, the collection is first broken up into ranges which are then read concurrently is separate go routines. If you increase this value you will notice the connection count increase in mongostat when direct reads are performed. You will also notice the memory consumption of Monstache grow. Increasing this value can increase the throughput for reading large collections, but you need to have enough memory available to Monstache to do so. You can decrease this value for a memory constrained Monstache process. exit-after-direct-reads boolean (default false ) The direct-read-namespaces option gives you a way to do a full sync on multiple collections. At times you may want to perform a full sync via the direct-read-namespaces option and then quit monstache. Set this option to true and monstache will exit after syncing the direct read collections instead of continuing to tail the oplog. This is useful if you would like to run monstache to run a full sync on a set of collections via a cron job. namespace-regex regexp (default \"\" ) When namespace-regex is given this regex is tested against the namespace, database.collection, of any insert, update, delete in MongoDB. If the regex matches monstache continues processing event filters, otherwise it drops the event. By default monstache processes events in all databases and all collections with the exception of the reserved database monstache, any collections suffixed with .chunks, and the system collections. For more information see the section Namespaces . namespace-exclude-regex regex (default \"\" ) When namespace-exclude-regex is given this regex is tested against the namespace, database.collection, of any insert, update, delete in MongoDB. If the regex matches monstache ignores the event, otherwise it continues processing event filters. By default monstache processes events in all databases and all collections with the exception of the reserved database monstache, any collections suffixed with .chunks, and the system collections. For more information see the section Namespaces . namespace-drop-regex regexp (default \"\" ) When namespace-drop-regex is given this regex is tested against the namespace, database.collection, of drops in MongoDB. For database drops the namespace will be database-name.$cmd. For collections drops the namespace will be database-name.collection-name. If the regex matches the namespace then the operation will by synced. namespace-drop-exclude-regex regex (default \"\" ) When namespace-drop-exclude-regex is given this regex is tested against the namespace, database.collection, of drops in MongoDB. For database drops the namespace will be database-name.$cmd. For collections drops the namespace will be database-name.collection-name. If the regex does not match the namespace then the operation will by synced. mongo-url string (default localhost ) The URL to connect to MongoDB which must follow the Standard Connection String Format For sharded clusters this URL should point to the mongos router server and the mongo-config-url option must be set to point to the config server. mongo-config-url string (default \"\" ) This config must only be set for sharded MongoDB clusters. Has the same syntax as mongo-url. This URL must point to the MongoDB config server. Monstache will read the list of shards using this connection and then setup a listener to react to new shards being added to the cluster at a later time. It will then setup a new direct connection to each shard to listen for events. Setting the mongo-config-url is not necessary if you are using change-stream-namespaces . mongo-pem-file string (default \"\" ) When mongo-pem-file is given monstache will use the given file path to add a local certificate to x509 cert pool when connecting to MongoDB. This should only be used when MongoDB is configured with SSL enabled. mongo-validate-pem boolean (default true ) When mongo-validate-pem-file is false TLS will be configured to skip verification mongo-oplog-database-name string (default local ) When mongo-oplog-database-name is given monstache will look for the MongoDB oplog in the supplied database mongo-oplog-collection-name string (default $oplog.main ) When mongo-oplog-collection-name is given monstache will look for the MongoDB oplog in the supplied collection mongo-dial-settings TOML table (default nil ) The following MongoDB dial properties are available. Timeout values of 0 disable the timeout. ssl bool (default false) Set to true to establish a connection using TLS. timeout int (default 15) Seconds to wait when establishing an initial connection to MongoDB before giving up read-timeout int (default 0) Seconds to wait when reading data from MongoDB before giving up write-timeout int (default 0) Seconds to wait when writing data to MongoDB before giving up mongo-session-settings TOML table (default nil ) The following MongoDB session properties are available. Timeout values of 0 disable the timeout. socket-timeout int (default 0) Seconds to wait for a non-responding socket before it is forcefully closed sync-timeout int (default 0) Amount of time in seconds an operation will wait before returning an error in case a connection to a usable server can't be established. Set it to zero to wait forever. gtm-settings TOML table (default nil ) The following gtm configuration properties are available. See gtm for details channel-size int (default 512) Controls the size of the go channels created for processing events. When many events are processed at once a larger channel size may prevent blocking in gtm. buffer-size int (default 32) Determines how many documents are buffered by a gtm worker go routine before they are batch fetched from MongoDB. When many documents are inserted or updated at once it is better to fetch them together. buffer-duration string (default 750ms) A string representation of a golang duration. Determines the maximum time a buffer is held before it is fetched in batch from MongoDB and flushed for indexing. index-files boolean (default false ) When index-files is true monstache will index the raw content of files stored in GridFS into Elasticsearch as an attachment type. By default index-files is false meaning that monstache will only index metadata associated with files stored in GridFS. In order for index-files to index the raw content of files stored in GridFS you must install a plugin for Elasticsearch. For versions of Elasticsearch prior to version 5, you should install the mapper-attachments plugin. In version 5 or greater of Elasticsearch the mapper-attachment plugin is deprecated and you should install the ingest-attachment plugin instead. For further information on how to configure monstache to index content from GridFS, see the section GridFS support . max-file-size int (default 0 ) When max-file-size is greater than 0 monstache will not index the content of GridFS files that exceed this limit in bytes. file-namespaces []string (default nil ) The file-namespaces config must be set when index-files is enabled. file-namespaces must be set to an array of MongoDB namespace strings. Files uploaded through gridfs to any of the namespaces in file-namespaces will be retrieved and their raw content indexed into Elasticsearch via either the mapper-attachments or ingest-attachment plugin. This option may be passed on the command line as ./monstache --file-namespace test.foo --file-namespace test.bar file-highlighting boolean (default false ) When file-highlighting is true monstache will enable the ability to return highlighted keywords in the extracted text of files for queries on files which were indexed in Elasticsearch from gridfs. verbose boolean (default false ) When verbose is true monstache with enable debug logging including a trace of requests to Elasticsearch elasticsearch-user string (default \"\" ) Optional Elasticsearch username for basic auth elasticsearch-password string (default \"\" ) Optional Elasticsearch password for basic auth elasticsearch-urls []string (default [ \"http://localhost:9200\" ] ) An array of URLs to connect to the Elasticsearch REST Interface This option may be passed on the command line as ./monstache --elasticsearch-url URL1 --elasticsearch-url URL2 elasticsearch-version string (by default determined by connecting to the server ) When elasticsearch-version is provided monstache will parse the given server version to determine how to interact with the Elasticsearch API. This is normally not recommended because monstache will connect to Elasticsearch to find out which version is being used. This option is provided for cases where connecting to the base URL of the Elasticsearch REST API to get the version is not possible or desired. elasticsearch-max-conns int (default 4 ) The size of the Elasticsearch HTTP connection pool. This determines the concurrency of bulk indexing requests to Elasticsearch. If you increase this value too high you may begin to see bulk indexing failures if the bulk index queue gets overloaded. To increase the size of the bulk indexing queue you can update the Elasticsearch config file: thread_pool: bulk: queue_size: 200 For more information see Thread Pool . You will want to tune this variable in sync with the elasticsearch-max-bytes option. elasticsearch-retry boolean (default false ) When elasticseach-retry is true a failed request to Elasticsearch will be retried with an exponential backoff policy. The policy is set with an initial timeout of 50 ms, an exponential factor of 2, and a max wait of 20 seconds. For more information on how this works see Back Off Strategy elasticsearch-client-timeout int (default 0 ) The number of seconds before a request to Elasticsearch times out. A setting of 0, the default, disables the timeout. elasticsearch-max-docs int (default -1 ) When elasticsearch-max-docs is given a bulk index request to Elasticsearch will be forced when the buffer reaches the given number of documents. Warning It is not recommended to change this option but rather use elasticsearch-max-bytes instead since the document count is not a good gauge of when to flush. The default value of -1 means to not use the number of docs as a flush indicator. elasticsearch-max-bytes int (default 8MB as bytes) When elasticsearch-max-bytes is given a bulk index request to Elasticsearch will be forced when a connection buffer reaches the given number of bytes. This setting greatly impacts performance. A high value for this setting will cause high memory monstache memory usage as the documents are buffered in memory. Each connection in elasticsearch-max-conns will flush when its queue gets filled to this size. elasticsearch-max-seconds int (default 1 ) When elasticsearch-max-seconds is given a bulk index request to Elasticsearch will be forced when a request has not been made in the given number of seconds. The default value is automatically increased to 5 when direct read namespaces are detected. This is to ensure that flushes do not happen too often in this case which would cut performance. elasticsearch-pem-file string (default \"\" ) When elasticsearch-pem-file is given monstache will use the given file path to add a local certificate to x509 cert pool when connecting to Elasticsearch. This should only be used when Elasticsearch is configured with SSL enabled. elasticsearch-validate-pem boolean (default true ) When elasticsearch-validate-pem-file is false TLS will be configured to skip verification dropped-databases boolean (default true ) When dropped-databases is false monstache will not delete the mapped indexes in Elasticsearch if a MongoDB database is dropped dropped-collections boolean (default true ) When dropped-collections is false monstache will not delete the mapped index in Elasticsearch if a MongoDB collection is dropped worker string (default \"\" ) When worker is given monstache will enter multi-worker mode and will require you to also provide the config option workers. Use this mode to run multiple monstache processes and distribute the work between them. In this mode monstache will ensure that each MongoDB document id always goes to the same worker and none of the other workers. See the Workers section for more information. workers []string (default nil ) An array of worker names to be used in conjunction with the worker option. This option may be passed on the command line as ./monstache --workers w1 --workers w2 enable-patches boolean (default false ) Set to true to enable storing rfc7396 patches in your Elasticsearch documents patch-namespaces []string (default nil ) An array of MongoDB namespaces that you would like to enable rfc7396 patches on This option may be passed on the command line as ./monstache --patch-namespace test.foo --patch-namespace test.bar merge-patch-attribute string (default json-merge-patches ) Customize the name of the property under which merge patches are stored cluster-name string (default \"\" ) When cluster-name is given monstache will enter a high availablity mode. Processes with cluster name set to the same value will coordinate. Only one of the processes in a cluster will sync changes. The other processes will be in a paused state. If the process which is syncing changes goes down for some reason one of the processes in paused state will take control and start syncing. See the section high availability for more information. mapping [] array of TOML table (default nil ) When mapping is given monstache will be directed to override the default index and type assigned to documents in Elasticsearch. See the section Index Mapping for more information. namespace string (default \"\") The MongoDB namespace, db.collection, to apply the mapping to. index string (default \"same as namespace including the dot. e.g. test.test\") Allows you to override the default index that monstache will send documents to. By default, the index is the same as the MongoDB namespace. type string (default \"_doc for ES 6.2+ and the name of the MongoDB collection otherwise\") Allows you to override the default type that monstache will index documents with. Overriding the type is not recommended for Elasticsearch version 6.2+. filter [] array of TOML table (default nil ) When filter is given monstache will pass the MongoDB document from an insert or update operation into the filter function immediately after it is read from the oplog. Return true from the function to continue processing the document or false to completely ignore the document. See the section Middleware for more information. See the section Middleware for more information. namespace string (default \"\") The MongoDB namespace, db.collection, to apply the script to. If you omit namespace the filter function will be applied to all documents. script string (default \"\") An inline script. You can use TOML multiline syntax here. The function should take 2 arguments, a doc and a namespace, and return true/false to include or filter the document. path string (default \"\") The file path to load a script from. Use this or an inline script but not both. Can be a path relative to the directory monstache is executed from or an absolute path. script [] array of TOML table (default nil ) When script is given monstache will pass the MongoDB document into the script before indexing into Elasticsearch. See the section Middleware for more information. namespace string (default \"\") The MongoDB namespace, db.collection, to apply the script to. If you omit the namespace the mapping function with be applied to all documents. routing boolean (default false) Set routing to true if you override the index, routing or parent metadata via _meta_monstache script string (default \"\") An inline script. You can use TOML multiline syntax here. The function should take 2 arguments, a doc and a namespace, and return a modified doc. You can also return true to index the original document or false to ignore the document and schedule any previous documents with the same id for deletion. path string (default \"\") The file path to load a script from. Use this or an inline script but not both. Can be a path relative to the directory monstache is executed from or an absolute path. pipeline [] array of TOML table (default nil ) When pipeline is given monstache will call the function specified to determine an array of aggregation pipeline stages to run. See the section Middleware for more information. namespace string (default \"\") The MongoDB namespace, db.collection, to apply the script to. If you omit the namespace the pipeline function with be applied to all namespaces. script string (default \"\") An inline script. You can use TOML multiline syntax here. The function should take 2 arguments, a namespace and a boolean indicating whether or not the data is a change stream. The function should return an array of aggregation pipeline stages. Note, for change streams the root of the pipeline will be the change event with a field fullDocument representing the changed doc. You should alter your pipeline stages according to this boolean. Monstache needs the change event data so do not replace the root of the document in your pipeline for change streams. path string (default \"\") The file path to load a script from. Use this or an inline script but not both. Can be a path relative to the directory monstache is executed from or an absolute path. pipe-allow-disk boolean (default false ) Add this flag to allow MongoDB to use the disk as a temporary store for data during aggregation pipelines graylog-addr string (default \"\") The address of a graylog server to redirect logs to in GELF relate [] array of TOML table (default nil ) Allows one to relate 2 namespaces together such that a change to one causes a sync of the associated namespace namespace string (default \"\") The namespace of the collection that, when modified, triggers a sync of the with-namespace with-namespace string (default \"\") The namespace of the collection or view that will be synced when namespace changes src-field string (default \"_id\") The name of the field in namespace that will be extracted from the change doc and used as the value side of the query into with-namespace match-field string (default \"_id\") The name of the field in with-namespace that will be used as the field name to match side of the query into with-namespace keep-src bool (default \"false\") Whether or not to sync the original change event in addition to the one looked up in with-namespace. By default the original change is ignored and only the document from with-namespace is synced. aws-connect TOML table (default nil ) Enable experimental support for using a connection to Elasticsearch that uses AWS Signature Version 4 access-key string (default \"\") AWS Access Key secret-key string (default \"\") AWS Secrete Key region string (default \"\") AWS Region logs TOML table (default nil ) Allows writing logs to a file using a rolling appender instead of stdout. Supply a file path for each type of log you would like to send to a file. info string (default \"\") The file path to write info level logs to warn string (default \"\") The file path to write warning level logs to error string (default \"\") The file path to write error level logs to trace string (default \"\") The file path to write trace level logs to. Trace logs are enabled via the verbose option. stats string (default \"\") The file path to write indexing statistics to. Stats logs are enabled via the stats option. enable-http-server boolean (default false ) Add this flag to enable an embedded HTTP server at localhost:8080 http-server-addr string (default :8080 ) The address to bind the embedded HTTP server on if enabled pprof boolean (default false ) When pprof is true and the http server is enabled, monstache will make profiling information available. See Profiling for Go for more information.","title":"Configuration"},{"location":"config/#configuration","text":"Configuration can be specified in your TOML config file or passed into monstache as Go program arguments on the command line. Program arguments take precedance over configuration specified in the TOML config file. Warning Please keep any simple -one line config- above any [[script]] or toml table configs, as there is a bug in the toml parser where if you have definitions below a TOML table, e.g. a [[script]] then the parser thinks that lines below that belong to the table instead of at the global level","title":"Configuration"},{"location":"config/#print-config","text":"boolean (default false ) When print-config is true monstache will print its configuration and then exit","title":"print-config"},{"location":"config/#stats","text":"boolean (default false ) When stats is true monstache will periodically print statistics accumulated by the indexer","title":"stats"},{"location":"config/#enable-easy-json","text":"boolean (default false ) When enable-easy-json is true monstache will the easy-json library to serialize requests to Elasticsearch","title":"enable-easy-json"},{"location":"config/#stats-duration","text":"string (default 30s ) Sets the duration after which statistics are printed if stats is enabled","title":"stats-duration"},{"location":"config/#index-stats","text":"boolean (default false ) When both stats and index-stats are true monstache will write statistics about its indexing progress in Elasticsearch instead of standard out. The indexes used to store the statistics are time stamped by day and prefixed monstache.stats. . E.g. monstache.stats.2017-07-01 and so on. As these indexes will accrue over time your can use a tool like curator to prune them with a Delete Indices action and an age filter.","title":"index-stats"},{"location":"config/#index-as-update","text":"boolean (default false ) When index-as-update is set to true monstache will sync create and update operations in MongoDB as updates to Elasticsearch. By default, monstache will overwrite the entire document in Elasticsearch. This setting may be useful if you make updates to Elasticsearch to the documents monstache has previously synced and would like to retain these updates when the document changes in MongoDB. You will only be able to retain fields in Elasticsearch that do not overlap with fields in MongoDB.","title":"index-as-update"},{"location":"config/#stats-index-format","text":"string (default monstache.stats.2006-01-02 ) The time.Time supported index name format for stats indices. By default, stats indexes are partitioned by day. To use less indices for stats you can shorten this format string (e.g monstache.stats.2006-01) or remove the time component completely to use a single index.","title":"stats-index-format"},{"location":"config/#gzip","text":"boolean (default false ) When gzip is true, monstache will compress requests to Elasticsearch. If you enable gzip in monstache and are using Elasticsearch prior to version 5 you will also need to update the Elasticsearch config file to set http.compression: true. In Elasticsearch version 5 and above http.compression is enabled by default. Enabling gzip compression is recommended if you enable the index-files setting.","title":"gzip"},{"location":"config/#fail-fast","text":"boolean (default false ) When fail-fast is true, if monstache receives a failed bulk indexing response from Elasticsearch, monstache will log the request that produced the response as an ERROR and then exit immediately with an error status. Normally, monstache just logs the error and continues processing events. If monstache has been configured with elasticsearch-retry true, a failed request will be retried before being considered a failure.","title":"fail-fast"},{"location":"config/#prune-invalid-json","text":"boolean (default false ) If your MongoDB data contains values like +Infinity, -Infinity, NaN, or invalid dates you will want to set this option to true. The Golang json serializer is not able to handle these values and the indexer will get stuck in an infinite loop. When prune-invalid-json is set to true Monstache will drop those fields so that indexing errors do not occur.","title":"prune-invalid-json"},{"location":"config/#index-oplog-time","text":"boolean (default false ) If this option is set to true monstache will include 2 automatic fields in the source document indexed into Elasticsearch. The first is oplog_ts which is the timestamp for the event copied directly from the MongoDB oplog. The second is oplog_date which is an Elasticsearch date field corresponding to the time of the same event. This information is generally useful in Elasticsearch giving the notion of last updated. However, it's also valuable information to have for failed indexing requests since it gives one the information to replay from a failure point. See the option resume-from-timestamp for information on how to replay oplog events since a given event occurred. For data read via the direct read feature the oplog time will only be available if the id of the MongoDB document is an ObjectID. If the id of the MongoDB document is not an ObjectID and the document source is a direct read query then the oplog time will not be available.","title":"index-oplog-time"},{"location":"config/#oplog-ts-field-name","text":"string (default oplog_ts ) Use this option to override the name of the field used to store the oplog timestamp","title":"oplog-ts-field-name"},{"location":"config/#oplog-date-field-name","text":"string (default oplog_date ) Use this option to override the name of the field used to store the oplog date string","title":"oplog-date-field-name"},{"location":"config/#oplog-date-field-format","text":"string (default 2006/01/02 15:04:05 ) Use this option to override the layout for formatting the oplog_date field. Refer to the Format function for the reference time values to use in the layout.","title":"oplog-date-field-format"},{"location":"config/#resume","text":"boolean (default false ) When resume is true, monstache writes the timestamp of MongoDB operations it has successfully synced to Elasticsearch to the collection monstache.monstache. It also reads that timestamp from that collection when it starts in order to replay events which it might have missed because monstache was stopped. If monstache is started with the cluster-name option set then resume is automatically turned on.","title":"resume"},{"location":"config/#resume-name","text":"string (default default ) monstache uses the value of resume-name as an id when storing and retrieving timestamps to and from the MongoDB collection monstache.monstache. The default value for this option is the string default . However, there are some exceptions. If monstache is started with the cluster-name option set then the name of the cluster becomes the resume-name. This is to ensure that any process in the cluster is able to resume from the last timestamp successfully processed. The other exception occurs when resume-name is not given but worker-name is. In that case the worker name becomes the resume-name.","title":"resume-name"},{"location":"config/#resume-from-timestamp","text":"int64 (default 0 ) When resume-from-timestamp (a 64 bit timestamp where the high 32 bytes represent the number of seconds since epoch and the low 32 bits represent an offset within a second) is given, monstache will sync events starting immediately after the timestamp. This is useful if you have a specific timestamp from the oplog and would like to start syncing from after this event.","title":"resume-from-timestamp"},{"location":"config/#replay","text":"boolean (default false ) When replay is true, monstache replays all events from the beginning of the MongoDB oplog and syncs them to Elasticsearch. If you've previously synced Monstache to Elasticsearch you may see many WARN statments in the log indicating that there was a version conflict. This is normal during a replay and it just means that you already have data in Elasticsearch that is newer than the point in time data from the oplog. When resume and replay are both true, monstache replays all events from the beginning of the MongoDB oplog, syncs them to Elasticsearch and also writes the timestamps of processed events to monstache.monstache. When neither resume nor replay are true, monstache reads the last timestamp in the oplog and starts listening for events occurring after this timestamp (tails starting at the end). Timestamps are not written to monstache.monstache. This is the default behavior.","title":"replay"},{"location":"config/#resume-write-unsafe","text":"boolean (default false ) When resume-write-unsafe is true monstache sets the safety mode of the MongoDB session such that writes are fire and forget. This speeds up writing of timestamps used to resume synching in a subsequent run of monstache. This speed up comes at the cost of no error checking on the write of the timestamp. Since errors writing the last synched timestamp are only logged by monstache and do not stop execution it's not unreasonable to set this to true to get a speedup.","title":"resume-write-unsafe"},{"location":"config/#time-machine-namespaces","text":"[]string (default nil ) Monstache is good at keeping your MongoDB collections and Elasticsearch indexes in sync. When a document is updated in MongoDB the corresponding document in Elasticsearch is updated too. Same goes for deleting documents in MongoDB. But what if you also wanted to keep a log of all the changes to a MongoDB document over its lifespan. That's what time-machine-namespaces are for. When you configure a list of namespaces in MongoDB to add to the time machine, in addition to keeping documents in sync, Monstache will index of copy of your MongoDB document at the time it changes in a separate timestamped index. Say for example, you insert a document into the test.test collection in MongoDB. Monstache will index by default into the test.test index in Elasticsearch, but with time machines it will also index it into log.test.test.2018-02-19 . When it indexes it into the time machine index it does so without the id from MongoDB and lets Elasticsearch generate a unique id. But, it stores the id from MongoDB in the source field _source_id . Also, it adds _oplog_ts and _oplog_date fields on the source document. These correspond to the timestamp from the oplog when the data changed in MongoDB. Finally, it routes the document by the MongoDB id so that you can speed up queries later to find changes to a doc. This lets you do some cool things but mostly you'll want to sort by _oplog_date and filter by _source_id to see how documents have changed over time. Because the indexes are timestamped you can drop then after a period of time so they don't take up space. If you just want the last couple of days of changes, delete the indexes with the old timestamps. Elastic curator is your friend here. This option may be passed on the command line as ./monstache --time-machine-namespace test.foo --time-machine-namespace test.bar","title":"time-machine-namespaces"},{"location":"config/#time-machine-index-prefix","text":"string (default log ) If you have enabled time machine namespaces and want to change the prefix assigned to the index names use this setting.","title":"time-machine-index-prefix"},{"location":"config/#time-machine-index-suffix","text":"string (default 2006-01-02 ) If you have enabled time machine namespaces and want to suffix the index names using a different date format use this setting. Consult the golang docs for how date formats work. By default this suffixes the index name with the year, month, and day.","title":"time-machine-index-suffix"},{"location":"config/#time-machine-direct-reads","text":"boolean (default false ) This setting controls whether or not direct reads are added to the time machine log index. This is false by default so only changes read from the oplog are added.","title":"time-machine-direct-reads"},{"location":"config/#routing-namespaces","text":"[]string (default nil ) You only need to set this configuration option if you use golang and javascript plugins are do custom routing: override parent or routing attributes. This array should be set to a list of all the namespaces that custom routing is done on. This ensures that deletes in MongoDB are routed correctly to Elasticsearch.","title":"routing-namespaces"},{"location":"config/#delete-strategy","text":"int (default 0 ) The strategy to use for handling document deletes when custom indexing is done in scripts. Warning Breaking change in versions v4.4.0 v3.11.0 . Monstache was saving routing foreach document in MongoDB in a db called monstache collection meta using the same MongoDB URL and credentials provided in config by default. Monstache was only saving this information if the document metadata was being altered via _monstache_meta in a script or via the API in a golang plugin. Monstache needed to save this information in order to locate and perform deletes in Elasticsearch if the corresponding document was deleted in MongoDB. If you want to maintain this stategy use value 1. Otherwise, you can drop the monstache.meta collection as this is no longer used by default. But now the default has changed to be stateless, you can read more: discussion commit Strategy 0 -default- will do a term query by document id across all Elasticsearch indexes. Will only perform the delete if one single document is returned by the query. Stategy 1 will store indexing metadata in MongoDB in the monstache.meta collection and use this metadata to locate and delete the document. Stategy 2 will completely ignore document deletes in MongoDB.","title":"delete-strategy"},{"location":"config/#delete-index-pattern","text":"string (default * ) When using a stateless delete strategy, set this to a valid Elasticsearch index pattern to restrict the scope of possible indexes that a stateless delete will consider. If monstache only indexes to index a, b, and c then you can set this to a,b,c . If monstache only indexes to indexes starting with mydb then you can set this to mydb* .","title":"delete-index-pattern"},{"location":"config/#change-stream-namespaces","text":"[]string (default `nil') This option allows you to opt in to using MongoDB change streams. The namespaces included here will be tailed using $watch function. This options requires MongoDB version 3.6 and above. When this option is enabled the legacy direct tailing of the oplog is disabled, therefore you do not need to specify additional regular expressions to filter the set of collections to watch.","title":"change-stream-namespaces"},{"location":"config/#direct-read-namespaces","text":"[]string (default nil ) This option allows you to directly copy collections from MongoDB to Elasticsearch. Monstache allows filtering the data that is actually indexed to Elasticsearch, so you need not necessarily copy the entire collection. Since the oplog is a capped collection it may only contain a subset of all your data. In this case you can perform a direct sync of Mongodb to Elasticsearch. To do this, set direct-read-namespaces to an array of namespaces that you would like to copy. Monstache will perform reads directly from the given set of db.collection and sync them to Elasticsearch. This option may be passed on the command line as ./monstache --direct-read-namespace test.foo --direct-read-namespace test.bar By default, Monstache maps a MongoDB collection named foo in a database named test to the test.foo index in Elasticsearch. For maximum indexing performance when doing alot of a direct reads you will want to adjust the refresh interval during indexing on the destination Elasticsearch indices. The refresh interval can be set at a global level in elasticsearch.yml or on a per index basis by using the Index Settings or Index Template APIs. For more information see Update Indices Settings . By default, Elasticsearch refreshes every second. You will want to increase this value or turn off refresh completely during the indexing phase by setting the refresh_interval to -1. Remember to reset the refresh_interval to a positive value and do a force merge after the indexing phase has completed if you decide to temporarily turn off refresh, otherwise you will not be able to see the new documents in queries.","title":"direct-read-namespaces"},{"location":"config/#direct-read-split-max","text":"int (default 9 ) The maximum number of times to split a collection for direct reads. This setting greatly impacts the memory consumption of Monstache. When direct reads are performed, the collection is first broken up into ranges which are then read concurrently is separate go routines. If you increase this value you will notice the connection count increase in mongostat when direct reads are performed. You will also notice the memory consumption of Monstache grow. Increasing this value can increase the throughput for reading large collections, but you need to have enough memory available to Monstache to do so. You can decrease this value for a memory constrained Monstache process.","title":"direct-read-split-max"},{"location":"config/#exit-after-direct-reads","text":"boolean (default false ) The direct-read-namespaces option gives you a way to do a full sync on multiple collections. At times you may want to perform a full sync via the direct-read-namespaces option and then quit monstache. Set this option to true and monstache will exit after syncing the direct read collections instead of continuing to tail the oplog. This is useful if you would like to run monstache to run a full sync on a set of collections via a cron job.","title":"exit-after-direct-reads"},{"location":"config/#namespace-regex","text":"regexp (default \"\" ) When namespace-regex is given this regex is tested against the namespace, database.collection, of any insert, update, delete in MongoDB. If the regex matches monstache continues processing event filters, otherwise it drops the event. By default monstache processes events in all databases and all collections with the exception of the reserved database monstache, any collections suffixed with .chunks, and the system collections. For more information see the section Namespaces .","title":"namespace-regex"},{"location":"config/#namespace-exclude-regex","text":"regex (default \"\" ) When namespace-exclude-regex is given this regex is tested against the namespace, database.collection, of any insert, update, delete in MongoDB. If the regex matches monstache ignores the event, otherwise it continues processing event filters. By default monstache processes events in all databases and all collections with the exception of the reserved database monstache, any collections suffixed with .chunks, and the system collections. For more information see the section Namespaces .","title":"namespace-exclude-regex"},{"location":"config/#namespace-drop-regex","text":"regexp (default \"\" ) When namespace-drop-regex is given this regex is tested against the namespace, database.collection, of drops in MongoDB. For database drops the namespace will be database-name.$cmd. For collections drops the namespace will be database-name.collection-name. If the regex matches the namespace then the operation will by synced.","title":"namespace-drop-regex"},{"location":"config/#namespace-drop-exclude-regex","text":"regex (default \"\" ) When namespace-drop-exclude-regex is given this regex is tested against the namespace, database.collection, of drops in MongoDB. For database drops the namespace will be database-name.$cmd. For collections drops the namespace will be database-name.collection-name. If the regex does not match the namespace then the operation will by synced.","title":"namespace-drop-exclude-regex"},{"location":"config/#mongo-url","text":"string (default localhost ) The URL to connect to MongoDB which must follow the Standard Connection String Format For sharded clusters this URL should point to the mongos router server and the mongo-config-url option must be set to point to the config server.","title":"mongo-url"},{"location":"config/#mongo-config-url","text":"string (default \"\" ) This config must only be set for sharded MongoDB clusters. Has the same syntax as mongo-url. This URL must point to the MongoDB config server. Monstache will read the list of shards using this connection and then setup a listener to react to new shards being added to the cluster at a later time. It will then setup a new direct connection to each shard to listen for events. Setting the mongo-config-url is not necessary if you are using change-stream-namespaces .","title":"mongo-config-url"},{"location":"config/#mongo-pem-file","text":"string (default \"\" ) When mongo-pem-file is given monstache will use the given file path to add a local certificate to x509 cert pool when connecting to MongoDB. This should only be used when MongoDB is configured with SSL enabled.","title":"mongo-pem-file"},{"location":"config/#mongo-validate-pem","text":"boolean (default true ) When mongo-validate-pem-file is false TLS will be configured to skip verification","title":"mongo-validate-pem"},{"location":"config/#mongo-oplog-database-name","text":"string (default local ) When mongo-oplog-database-name is given monstache will look for the MongoDB oplog in the supplied database","title":"mongo-oplog-database-name"},{"location":"config/#mongo-oplog-collection-name","text":"string (default $oplog.main ) When mongo-oplog-collection-name is given monstache will look for the MongoDB oplog in the supplied collection","title":"mongo-oplog-collection-name"},{"location":"config/#mongo-dial-settings","text":"TOML table (default nil ) The following MongoDB dial properties are available. Timeout values of 0 disable the timeout.","title":"mongo-dial-settings"},{"location":"config/#ssl","text":"","title":"ssl"},{"location":"config/#bool-default-false","text":"Set to true to establish a connection using TLS.","title":"bool (default false)"},{"location":"config/#timeout","text":"","title":"timeout"},{"location":"config/#int-default-15","text":"Seconds to wait when establishing an initial connection to MongoDB before giving up","title":"int (default 15)"},{"location":"config/#read-timeout","text":"","title":"read-timeout"},{"location":"config/#int-default-0","text":"Seconds to wait when reading data from MongoDB before giving up","title":"int (default 0)"},{"location":"config/#write-timeout","text":"","title":"write-timeout"},{"location":"config/#int-default-0_1","text":"Seconds to wait when writing data to MongoDB before giving up","title":"int (default 0)"},{"location":"config/#mongo-session-settings","text":"TOML table (default nil ) The following MongoDB session properties are available. Timeout values of 0 disable the timeout.","title":"mongo-session-settings"},{"location":"config/#socket-timeout","text":"int (default 0) Seconds to wait for a non-responding socket before it is forcefully closed","title":"socket-timeout"},{"location":"config/#sync-timeout","text":"int (default 0) Amount of time in seconds an operation will wait before returning an error in case a connection to a usable server can't be established. Set it to zero to wait forever.","title":"sync-timeout"},{"location":"config/#gtm-settings","text":"TOML table (default nil ) The following gtm configuration properties are available. See gtm for details","title":"gtm-settings"},{"location":"config/#channel-size","text":"int (default 512) Controls the size of the go channels created for processing events. When many events are processed at once a larger channel size may prevent blocking in gtm.","title":"channel-size"},{"location":"config/#buffer-size","text":"int (default 32) Determines how many documents are buffered by a gtm worker go routine before they are batch fetched from MongoDB. When many documents are inserted or updated at once it is better to fetch them together.","title":"buffer-size"},{"location":"config/#buffer-duration","text":"string (default 750ms) A string representation of a golang duration. Determines the maximum time a buffer is held before it is fetched in batch from MongoDB and flushed for indexing.","title":"buffer-duration"},{"location":"config/#index-files","text":"boolean (default false ) When index-files is true monstache will index the raw content of files stored in GridFS into Elasticsearch as an attachment type. By default index-files is false meaning that monstache will only index metadata associated with files stored in GridFS. In order for index-files to index the raw content of files stored in GridFS you must install a plugin for Elasticsearch. For versions of Elasticsearch prior to version 5, you should install the mapper-attachments plugin. In version 5 or greater of Elasticsearch the mapper-attachment plugin is deprecated and you should install the ingest-attachment plugin instead. For further information on how to configure monstache to index content from GridFS, see the section GridFS support .","title":"index-files"},{"location":"config/#max-file-size","text":"int (default 0 ) When max-file-size is greater than 0 monstache will not index the content of GridFS files that exceed this limit in bytes.","title":"max-file-size"},{"location":"config/#file-namespaces","text":"[]string (default nil ) The file-namespaces config must be set when index-files is enabled. file-namespaces must be set to an array of MongoDB namespace strings. Files uploaded through gridfs to any of the namespaces in file-namespaces will be retrieved and their raw content indexed into Elasticsearch via either the mapper-attachments or ingest-attachment plugin. This option may be passed on the command line as ./monstache --file-namespace test.foo --file-namespace test.bar","title":"file-namespaces"},{"location":"config/#file-highlighting","text":"boolean (default false ) When file-highlighting is true monstache will enable the ability to return highlighted keywords in the extracted text of files for queries on files which were indexed in Elasticsearch from gridfs.","title":"file-highlighting"},{"location":"config/#verbose","text":"boolean (default false ) When verbose is true monstache with enable debug logging including a trace of requests to Elasticsearch","title":"verbose"},{"location":"config/#elasticsearch-user","text":"string (default \"\" ) Optional Elasticsearch username for basic auth","title":"elasticsearch-user"},{"location":"config/#elasticsearch-password","text":"string (default \"\" ) Optional Elasticsearch password for basic auth","title":"elasticsearch-password"},{"location":"config/#elasticsearch-urls","text":"[]string (default [ \"http://localhost:9200\" ] ) An array of URLs to connect to the Elasticsearch REST Interface This option may be passed on the command line as ./monstache --elasticsearch-url URL1 --elasticsearch-url URL2","title":"elasticsearch-urls"},{"location":"config/#elasticsearch-version","text":"string (by default determined by connecting to the server ) When elasticsearch-version is provided monstache will parse the given server version to determine how to interact with the Elasticsearch API. This is normally not recommended because monstache will connect to Elasticsearch to find out which version is being used. This option is provided for cases where connecting to the base URL of the Elasticsearch REST API to get the version is not possible or desired.","title":"elasticsearch-version"},{"location":"config/#elasticsearch-max-conns","text":"int (default 4 ) The size of the Elasticsearch HTTP connection pool. This determines the concurrency of bulk indexing requests to Elasticsearch. If you increase this value too high you may begin to see bulk indexing failures if the bulk index queue gets overloaded. To increase the size of the bulk indexing queue you can update the Elasticsearch config file: thread_pool: bulk: queue_size: 200 For more information see Thread Pool . You will want to tune this variable in sync with the elasticsearch-max-bytes option.","title":"elasticsearch-max-conns"},{"location":"config/#elasticsearch-retry","text":"boolean (default false ) When elasticseach-retry is true a failed request to Elasticsearch will be retried with an exponential backoff policy. The policy is set with an initial timeout of 50 ms, an exponential factor of 2, and a max wait of 20 seconds. For more information on how this works see Back Off Strategy","title":"elasticsearch-retry"},{"location":"config/#elasticsearch-client-timeout","text":"int (default 0 ) The number of seconds before a request to Elasticsearch times out. A setting of 0, the default, disables the timeout.","title":"elasticsearch-client-timeout"},{"location":"config/#elasticsearch-max-docs","text":"int (default -1 ) When elasticsearch-max-docs is given a bulk index request to Elasticsearch will be forced when the buffer reaches the given number of documents. Warning It is not recommended to change this option but rather use elasticsearch-max-bytes instead since the document count is not a good gauge of when to flush. The default value of -1 means to not use the number of docs as a flush indicator.","title":"elasticsearch-max-docs"},{"location":"config/#elasticsearch-max-bytes","text":"int (default 8MB as bytes) When elasticsearch-max-bytes is given a bulk index request to Elasticsearch will be forced when a connection buffer reaches the given number of bytes. This setting greatly impacts performance. A high value for this setting will cause high memory monstache memory usage as the documents are buffered in memory. Each connection in elasticsearch-max-conns will flush when its queue gets filled to this size.","title":"elasticsearch-max-bytes"},{"location":"config/#elasticsearch-max-seconds","text":"int (default 1 ) When elasticsearch-max-seconds is given a bulk index request to Elasticsearch will be forced when a request has not been made in the given number of seconds. The default value is automatically increased to 5 when direct read namespaces are detected. This is to ensure that flushes do not happen too often in this case which would cut performance.","title":"elasticsearch-max-seconds"},{"location":"config/#elasticsearch-pem-file","text":"string (default \"\" ) When elasticsearch-pem-file is given monstache will use the given file path to add a local certificate to x509 cert pool when connecting to Elasticsearch. This should only be used when Elasticsearch is configured with SSL enabled.","title":"elasticsearch-pem-file"},{"location":"config/#elasticsearch-validate-pem","text":"boolean (default true ) When elasticsearch-validate-pem-file is false TLS will be configured to skip verification","title":"elasticsearch-validate-pem"},{"location":"config/#dropped-databases","text":"boolean (default true ) When dropped-databases is false monstache will not delete the mapped indexes in Elasticsearch if a MongoDB database is dropped","title":"dropped-databases"},{"location":"config/#dropped-collections","text":"boolean (default true ) When dropped-collections is false monstache will not delete the mapped index in Elasticsearch if a MongoDB collection is dropped","title":"dropped-collections"},{"location":"config/#worker","text":"string (default \"\" ) When worker is given monstache will enter multi-worker mode and will require you to also provide the config option workers. Use this mode to run multiple monstache processes and distribute the work between them. In this mode monstache will ensure that each MongoDB document id always goes to the same worker and none of the other workers. See the Workers section for more information.","title":"worker"},{"location":"config/#workers","text":"[]string (default nil ) An array of worker names to be used in conjunction with the worker option. This option may be passed on the command line as ./monstache --workers w1 --workers w2","title":"workers"},{"location":"config/#enable-patches","text":"boolean (default false ) Set to true to enable storing rfc7396 patches in your Elasticsearch documents","title":"enable-patches"},{"location":"config/#patch-namespaces","text":"[]string (default nil ) An array of MongoDB namespaces that you would like to enable rfc7396 patches on This option may be passed on the command line as ./monstache --patch-namespace test.foo --patch-namespace test.bar","title":"patch-namespaces"},{"location":"config/#merge-patch-attribute","text":"string (default json-merge-patches ) Customize the name of the property under which merge patches are stored","title":"merge-patch-attribute"},{"location":"config/#cluster-name","text":"string (default \"\" ) When cluster-name is given monstache will enter a high availablity mode. Processes with cluster name set to the same value will coordinate. Only one of the processes in a cluster will sync changes. The other processes will be in a paused state. If the process which is syncing changes goes down for some reason one of the processes in paused state will take control and start syncing. See the section high availability for more information.","title":"cluster-name"},{"location":"config/#mapping","text":"[] array of TOML table (default nil ) When mapping is given monstache will be directed to override the default index and type assigned to documents in Elasticsearch. See the section Index Mapping for more information.","title":"mapping"},{"location":"config/#namespace","text":"string (default \"\") The MongoDB namespace, db.collection, to apply the mapping to.","title":"namespace"},{"location":"config/#index","text":"string (default \"same as namespace including the dot. e.g. test.test\") Allows you to override the default index that monstache will send documents to. By default, the index is the same as the MongoDB namespace.","title":"index"},{"location":"config/#type","text":"string (default \"_doc for ES 6.2+ and the name of the MongoDB collection otherwise\") Allows you to override the default type that monstache will index documents with. Overriding the type is not recommended for Elasticsearch version 6.2+.","title":"type"},{"location":"config/#filter","text":"[] array of TOML table (default nil ) When filter is given monstache will pass the MongoDB document from an insert or update operation into the filter function immediately after it is read from the oplog. Return true from the function to continue processing the document or false to completely ignore the document. See the section Middleware for more information. See the section Middleware for more information.","title":"filter"},{"location":"config/#namespace_1","text":"string (default \"\") The MongoDB namespace, db.collection, to apply the script to. If you omit namespace the filter function will be applied to all documents.","title":"namespace"},{"location":"config/#script","text":"string (default \"\") An inline script. You can use TOML multiline syntax here. The function should take 2 arguments, a doc and a namespace, and return true/false to include or filter the document.","title":"script"},{"location":"config/#path","text":"string (default \"\") The file path to load a script from. Use this or an inline script but not both. Can be a path relative to the directory monstache is executed from or an absolute path.","title":"path"},{"location":"config/#script_1","text":"[] array of TOML table (default nil ) When script is given monstache will pass the MongoDB document into the script before indexing into Elasticsearch. See the section Middleware for more information.","title":"script"},{"location":"config/#namespace_2","text":"string (default \"\") The MongoDB namespace, db.collection, to apply the script to. If you omit the namespace the mapping function with be applied to all documents.","title":"namespace"},{"location":"config/#routing","text":"boolean (default false) Set routing to true if you override the index, routing or parent metadata via _meta_monstache","title":"routing"},{"location":"config/#script_2","text":"string (default \"\") An inline script. You can use TOML multiline syntax here. The function should take 2 arguments, a doc and a namespace, and return a modified doc. You can also return true to index the original document or false to ignore the document and schedule any previous documents with the same id for deletion.","title":"script"},{"location":"config/#path_1","text":"string (default \"\") The file path to load a script from. Use this or an inline script but not both. Can be a path relative to the directory monstache is executed from or an absolute path.","title":"path"},{"location":"config/#pipeline","text":"[] array of TOML table (default nil ) When pipeline is given monstache will call the function specified to determine an array of aggregation pipeline stages to run. See the section Middleware for more information.","title":"pipeline"},{"location":"config/#namespace_3","text":"string (default \"\") The MongoDB namespace, db.collection, to apply the script to. If you omit the namespace the pipeline function with be applied to all namespaces.","title":"namespace"},{"location":"config/#script_3","text":"string (default \"\") An inline script. You can use TOML multiline syntax here. The function should take 2 arguments, a namespace and a boolean indicating whether or not the data is a change stream. The function should return an array of aggregation pipeline stages. Note, for change streams the root of the pipeline will be the change event with a field fullDocument representing the changed doc. You should alter your pipeline stages according to this boolean. Monstache needs the change event data so do not replace the root of the document in your pipeline for change streams.","title":"script"},{"location":"config/#path_2","text":"string (default \"\") The file path to load a script from. Use this or an inline script but not both. Can be a path relative to the directory monstache is executed from or an absolute path.","title":"path"},{"location":"config/#pipe-allow-disk","text":"boolean (default false ) Add this flag to allow MongoDB to use the disk as a temporary store for data during aggregation pipelines","title":"pipe-allow-disk"},{"location":"config/#graylog-addr","text":"string (default \"\") The address of a graylog server to redirect logs to in GELF","title":"graylog-addr"},{"location":"config/#relate","text":"[] array of TOML table (default nil ) Allows one to relate 2 namespaces together such that a change to one causes a sync of the associated namespace","title":"relate"},{"location":"config/#namespace_4","text":"string (default \"\") The namespace of the collection that, when modified, triggers a sync of the with-namespace","title":"namespace"},{"location":"config/#with-namespace","text":"string (default \"\") The namespace of the collection or view that will be synced when namespace changes","title":"with-namespace"},{"location":"config/#src-field","text":"string (default \"_id\") The name of the field in namespace that will be extracted from the change doc and used as the value side of the query into with-namespace","title":"src-field"},{"location":"config/#match-field","text":"string (default \"_id\") The name of the field in with-namespace that will be used as the field name to match side of the query into with-namespace","title":"match-field"},{"location":"config/#keep-src","text":"bool (default \"false\") Whether or not to sync the original change event in addition to the one looked up in with-namespace. By default the original change is ignored and only the document from with-namespace is synced.","title":"keep-src"},{"location":"config/#aws-connect","text":"TOML table (default nil ) Enable experimental support for using a connection to Elasticsearch that uses AWS Signature Version 4","title":"aws-connect"},{"location":"config/#access-key","text":"string (default \"\") AWS Access Key","title":"access-key"},{"location":"config/#secret-key","text":"string (default \"\") AWS Secrete Key","title":"secret-key"},{"location":"config/#region","text":"string (default \"\") AWS Region","title":"region"},{"location":"config/#logs","text":"TOML table (default nil ) Allows writing logs to a file using a rolling appender instead of stdout. Supply a file path for each type of log you would like to send to a file.","title":"logs"},{"location":"config/#info","text":"string (default \"\") The file path to write info level logs to","title":"info"},{"location":"config/#warn","text":"string (default \"\") The file path to write warning level logs to","title":"warn"},{"location":"config/#error","text":"string (default \"\") The file path to write error level logs to","title":"error"},{"location":"config/#trace","text":"string (default \"\") The file path to write trace level logs to. Trace logs are enabled via the verbose option.","title":"trace"},{"location":"config/#stats_1","text":"string (default \"\") The file path to write indexing statistics to. Stats logs are enabled via the stats option.","title":"stats"},{"location":"config/#enable-http-server","text":"boolean (default false ) Add this flag to enable an embedded HTTP server at localhost:8080","title":"enable-http-server"},{"location":"config/#http-server-addr","text":"string (default :8080 ) The address to bind the embedded HTTP server on if enabled","title":"http-server-addr"},{"location":"config/#pprof","text":"boolean (default false ) When pprof is true and the http server is enabled, monstache will make profiling information available. See Profiling for Go for more information.","title":"pprof"},{"location":"start/","text":"Getting Started Installation Monstache is just a single binary without dependencies on runtimes like Ruby, Python or PHP. Monstache is written in Go but you don't need to install the Go language unless you decide to write your own Go plugins. If you simply want to run Monstache you just need to download the latest version . You will want to use 4.x releases for ES6+ and 3.x releases for ES2-5. Unzip the download and adjust your PATH variable to include the path to the folder for your platform. Let's make sure Monstache is set up as expected. You should see a similar version number in your terminal: monstache -v # 4.11.4 The version number should start with 3.x if you are using Elasticsearch prior to version 6. You can also build monstache from source. For Elasticsearch 6 and up use go get -u github.com/rwynn/monstache For Elasticsearch before version 6 use go get -u gopkg.in/rwynn/monstache.v3 Usage Monstache uses the MongoDB oplog as an event source. You will need to make sure that MongoDB is configured to produce an oplog. The oplog can be enabled by using one of the following options: Setting up replica sets Passing --master to the mongod process Setting the following in /etc/mongod.conf master = true Note If you have enabled security in MongoDB you will need to give the user in your connection string certain privileges. Specifically, the user will need to be able read the local database (to read from the oplog) and any user databases that you wish to synch data from. Additionally, when using the resume or clustering features the user will need to be able to write to and create indexes for the monstache database. Monstache makes concurrent bulk indexing requests to Elasticsearch. It is recommended to increase the pool of bulk request handlers configured for Elasticsearch to ensure that requests do not begin to time out due to an overloaded queue. The queue size can be increased by making changes to your elasticsearch.yml configuration. thread_pool: bulk: queue_size: 200 Without any explicit configuration monstache will connect to Elasticsearch and MongoDB on localhost on the default ports and begin tailing the MongoDB oplog. Any changes to MongoDB while Monstache is running will be reflected in Elasticsearch. To see the indexes created by Monstache you may want to issue the following command which will show the indices in Elasticsearch. By default, the index names will match the db.collection name in MongoDB. curl localhost:9200/_cat/indices?v Monstache uses the TOML format for its configuration. You can run monstache with an explicit configuration by passing the -f flag. monstache -f /path/to/config.toml The following shows how to specify options in a TOML config file. It is recommended that you start with only your MongoDB and Elasticsearch connection settings and only specify additional options as needed. # connection settings # connect to MongoDB using the following URL mongo-url = mongodb://someuser:password@localhost:40001 # connect to the Elasticsearch REST API at the following node URLs elasticsearch-urls = [ https://es1:9200 , https://es2:9200 ] # frequently required settings # if you don't want to listen for changes to all collections in MongoDB but only a few # e.g. only listen for inserts, updates, deletes, and drops from mydb.mycollection # this setting does not initiate a copy, it is a filter on the oplog change listener only namespace-regex = '^mydb\\.mycollection$' # additionally, if you need to seed an index from a collection and not just listen for changes from the oplog # you can copy entire collections or views from MongoDB to Elasticsearch direct-read-namespaces = [ mydb.mycollection , db.collection , test.test ] # if you want to use MongoDB change streams instead of legacy oplog tailing add the following # in this case you don't need regexes to filter collections. # change streams require MongoDB version 3.6+ # change streams cannot be combined yet with resume, replay, or cluster options. # change streams start listening for new changes since the monstache process is started change-stream-namespaces = [ mydb.mycollection , db.collection , test.test ] # additional settings # compress requests to Elasticsearch gzip = true # generate indexing statistics stats = true # index statistics into Elasticsearch index-stats = true # use the following PEM file for connections to MongoDB mongo-pem-file = /path/to/mongoCert.pem # disable PEM validation mongo-validate-pem-file = false # use the following user name for Elasticsearch basic auth elasticsearch-user = someuser # use the following password for Elasticsearch basic auth elasticsearch-password = somepassword # use 4 go routines concurrently pushing documents to Elasticsearch elasticsearch-max-conns = 4 # use the following PEM file to connections to Elasticsearch elasticsearch-pem-file = /path/to/elasticCert.pem # validate connections to Elasticsearch elastic-validate-pem-file = true # propogate dropped collections in MongoDB as index deletes in Elasticsearch dropped-collections = true # propogate dropped databases in MongoDB as index deletes in Elasticsearch dropped-databases = true # do not start processing at the beginning of the MongoDB oplog # if you set the replay to true you may see version conflict messages # in the log if you had synced previously. This just means that you are replaying old docs which are already # in Elasticsearch with a newer version. Elasticsearch is preventing the old docs from overwriting new ones. replay = false # resume processing from a timestamp saved in a previous run resume = true # do not validate that progress timestamps have been saved resume-write-unsafe = false # override the name under which resume state is saved resume-name = default # exclude documents whose namespace matches the following pattern namespace-exclude-regex = '^mydb\\.ignorecollection$' # turn on indexing of GridFS file content index-files = true # turn on search result highlighting of GridFS content file-highlighting = true # index GridFS files inserted into the following collections file-namespaces = [ users.fs.files ] # print detailed information including request traces verbose = true # enable clustering mode cluster-name = 'apollo' # do not exit after full-sync, rather continue tailing the oplog exit-after-direct-reads = false See Configuration for details about each configuration option.","title":"Getting Started"},{"location":"start/#getting-started","text":"","title":"Getting Started"},{"location":"start/#installation","text":"Monstache is just a single binary without dependencies on runtimes like Ruby, Python or PHP. Monstache is written in Go but you don't need to install the Go language unless you decide to write your own Go plugins. If you simply want to run Monstache you just need to download the latest version . You will want to use 4.x releases for ES6+ and 3.x releases for ES2-5. Unzip the download and adjust your PATH variable to include the path to the folder for your platform. Let's make sure Monstache is set up as expected. You should see a similar version number in your terminal: monstache -v # 4.11.4 The version number should start with 3.x if you are using Elasticsearch prior to version 6. You can also build monstache from source. For Elasticsearch 6 and up use go get -u github.com/rwynn/monstache For Elasticsearch before version 6 use go get -u gopkg.in/rwynn/monstache.v3","title":"Installation"},{"location":"start/#usage","text":"Monstache uses the MongoDB oplog as an event source. You will need to make sure that MongoDB is configured to produce an oplog. The oplog can be enabled by using one of the following options: Setting up replica sets Passing --master to the mongod process Setting the following in /etc/mongod.conf master = true Note If you have enabled security in MongoDB you will need to give the user in your connection string certain privileges. Specifically, the user will need to be able read the local database (to read from the oplog) and any user databases that you wish to synch data from. Additionally, when using the resume or clustering features the user will need to be able to write to and create indexes for the monstache database. Monstache makes concurrent bulk indexing requests to Elasticsearch. It is recommended to increase the pool of bulk request handlers configured for Elasticsearch to ensure that requests do not begin to time out due to an overloaded queue. The queue size can be increased by making changes to your elasticsearch.yml configuration. thread_pool: bulk: queue_size: 200 Without any explicit configuration monstache will connect to Elasticsearch and MongoDB on localhost on the default ports and begin tailing the MongoDB oplog. Any changes to MongoDB while Monstache is running will be reflected in Elasticsearch. To see the indexes created by Monstache you may want to issue the following command which will show the indices in Elasticsearch. By default, the index names will match the db.collection name in MongoDB. curl localhost:9200/_cat/indices?v Monstache uses the TOML format for its configuration. You can run monstache with an explicit configuration by passing the -f flag. monstache -f /path/to/config.toml The following shows how to specify options in a TOML config file. It is recommended that you start with only your MongoDB and Elasticsearch connection settings and only specify additional options as needed. # connection settings # connect to MongoDB using the following URL mongo-url = mongodb://someuser:password@localhost:40001 # connect to the Elasticsearch REST API at the following node URLs elasticsearch-urls = [ https://es1:9200 , https://es2:9200 ] # frequently required settings # if you don't want to listen for changes to all collections in MongoDB but only a few # e.g. only listen for inserts, updates, deletes, and drops from mydb.mycollection # this setting does not initiate a copy, it is a filter on the oplog change listener only namespace-regex = '^mydb\\.mycollection$' # additionally, if you need to seed an index from a collection and not just listen for changes from the oplog # you can copy entire collections or views from MongoDB to Elasticsearch direct-read-namespaces = [ mydb.mycollection , db.collection , test.test ] # if you want to use MongoDB change streams instead of legacy oplog tailing add the following # in this case you don't need regexes to filter collections. # change streams require MongoDB version 3.6+ # change streams cannot be combined yet with resume, replay, or cluster options. # change streams start listening for new changes since the monstache process is started change-stream-namespaces = [ mydb.mycollection , db.collection , test.test ] # additional settings # compress requests to Elasticsearch gzip = true # generate indexing statistics stats = true # index statistics into Elasticsearch index-stats = true # use the following PEM file for connections to MongoDB mongo-pem-file = /path/to/mongoCert.pem # disable PEM validation mongo-validate-pem-file = false # use the following user name for Elasticsearch basic auth elasticsearch-user = someuser # use the following password for Elasticsearch basic auth elasticsearch-password = somepassword # use 4 go routines concurrently pushing documents to Elasticsearch elasticsearch-max-conns = 4 # use the following PEM file to connections to Elasticsearch elasticsearch-pem-file = /path/to/elasticCert.pem # validate connections to Elasticsearch elastic-validate-pem-file = true # propogate dropped collections in MongoDB as index deletes in Elasticsearch dropped-collections = true # propogate dropped databases in MongoDB as index deletes in Elasticsearch dropped-databases = true # do not start processing at the beginning of the MongoDB oplog # if you set the replay to true you may see version conflict messages # in the log if you had synced previously. This just means that you are replaying old docs which are already # in Elasticsearch with a newer version. Elasticsearch is preventing the old docs from overwriting new ones. replay = false # resume processing from a timestamp saved in a previous run resume = true # do not validate that progress timestamps have been saved resume-write-unsafe = false # override the name under which resume state is saved resume-name = default # exclude documents whose namespace matches the following pattern namespace-exclude-regex = '^mydb\\.ignorecollection$' # turn on indexing of GridFS file content index-files = true # turn on search result highlighting of GridFS content file-highlighting = true # index GridFS files inserted into the following collections file-namespaces = [ users.fs.files ] # print detailed information including request traces verbose = true # enable clustering mode cluster-name = 'apollo' # do not exit after full-sync, rather continue tailing the oplog exit-after-direct-reads = false See Configuration for details about each configuration option.","title":"Usage"}]}